# Introduction to GIS in R

```{r, include=FALSE}

# load packages
library(sf) 
library(ggplot2)
library(dplyr)
library(readr)
library(stringr)
library(ggspatial)
library(plotly)
library(countdown)
library(tidyr)
```

## Aims

-   Know how to load spatial data into R using the `sf` library.
-   Be familiar with using GSS codes to join statistics to geographies.
-   Understand how spatial objects can be manipulated using `dplyr`.
-   Understand how to use spatial joins.
-   Be aware of map projections and Coordinate Reference Systems (CRS) and be able to modify them.
-   Know how to make static and interactive maps in `ggplot2`.
-   Be able to export your maps and shapefiles.

## GIS and R

R is commonly used for statistical analysis and programming, however it also has a range of geospatial libraries developed by a community of researchers and programmers. In the last few years, working with spatial data became much easier in R, with the development of the `sf` package. `sf` keeps all the spatial information for each observation in a geometry column which means that we can treat it like a normal data frame and also perform spatial operations on the data.

```{r, echo=FALSE}
poi <- sf::st_read("/Users/christophersteinberg/Downloads/hotosm_ken_points_of_interest_points_shp/hotosm_ken_points_of_interest_points_shp.shp", quiet=TRUE, as_tibble = TRUE)
head(poi)
```

## Working with spatial data in R

### Open Street Map Data

Throughout this tutorial you will be using data about Kenyan rainfall - [Kenyan Rainfall Data](https://data.humdata.org/dataset/ken-rainfall-subnational). It covers total rainfall at the subcounty level in the last 5 years, from 2021-01-01. The data is updated weekly and includes a range of variables capturing amount of rain.

We want to visualise, and better understand how much rain has fallen in the last 5 years at the sub-county and county level, and what the distribution is at the MSOA level of geography. To achieve this we will have to import spatial data, manipulate it, create summary statistics, and then plot it.

## Loading spatial and non-spatial data

Rainfall data has been tidied up and saved as a Comma Separated Value file (.csv). We can use `read_csv` to open it in R.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Create a new object called `poi` by using `read_csv()`. Load data located in "<https://data.humdata.org/dataset/ken-rainfall-subnational>".
2.  Use `glimpse()` or `head()` to view `health` structure.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>\

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r, eval = TRUE}
poi <- readr::read_csv("/Users/christophersteinberg/Documents/GitHub/intro_R-main/data/poi.csv")

poi
```

</details>

<br>

`poi` is currently just a data frame - it has not got an explicit geometry column which links observations to their geographic location. It does however contain several columns which can be used to convert it into a spatial data format.

**Ward_code** column references the GSS codes of wards within which the observations fall. GSS codes can be used to join `lfb` data to boundaries from the Open Geography Portal. One issue with this particular column is that it does not indicate the currency of GSS codes. Wards are subject to frequent change, and as such it is best practice to be clear about the dates of any boundaries used by stating the exact code used, e.g. **wd19cd**. Because LFB data does not include this information we have no guarantee that the boundaries and GSS codes we join will match.

Fortunately we have also been provided with columns recording the **easting**, and **northing** of each incident. We can use those to convert `lfb` into an `sf` object. To achieve this we will use the `st_as_sf()` function which takes the following arguments:

```{r, eval = FALSE}
new_object <- st_as_sf(x = input_data_frame, coords = c("x_coordinate_column", "y_coordinate_column"), crs = 27700)
```

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Create a new object called `poi_sf` by converting `poi` using the `st_as_sf()` function.
2.  Use `glimpse()` or `head()` to view `poi_sf` structure.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
poi_sf <- sf::st_as_sf(x = poi, coords = c("easting", "northing"), crs = 27700)
head(poi_sf)
```

</details>

<br>

`st_as_sf()` converted the **easting** and **northing** columns to simple feature geometries and created a new column called **geometry** which holds spatial information for each row. Now that `poi` is a spatial object we can plot it using the `ggplot2` package. For now we will use the `geom_sf()` function which creates a quick map, using `ggplot2's` default settings. `geom_sf()` only needs to be supplied with a simple feature object and is very useful for quickly inspecting your data. To quickly plot multiple layers on the same map use `geom_sf() + geom_sf()`.

### Exercise

`r countdown(minutes = 3, seconds = 0, style = "position: relative; width: min-content;")`

1.  Plot `poi_sf` using the `geom_sf()` function.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
ggplot2::ggplot(poi_sf) + 
  ggplot2::geom_sf() 
```

</details>

<br>

We can also create interactive maps using `plotly` or `leaflet`. For `plotly` we ca write the map using ggplot2, before executing `ggplotly()`.

### Exercise

`r countdown(minutes = 3, seconds = 0, style = "position: relative; width: min-content;")`

1.  Make an interactive map of `poi_sf` using the `ggplotly()` function.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
ggplot_graph <- ggplot2::ggplot(poi_sf) + 
  ggplot2::geom_sf() 

plotly::ggplotly(ggplot_graph)
  
```

</details>

<br>

## Filtering by Administration Code

This dataset covers all of Kenya at subnational administrative boundaries. Let's look at solely locations in Nairobi to narrow down our exploration more. To remove all points outside of Nairobi we will have to import a shapefile with the right county level and then use it to spatially filter our `poi_df` data.

So far we have created our own `sf` objects by adding a geometry column. The kenya data set is already a spatial one and as such we can use the `st_read()` function from the `sf` package to import it. `st_read` isextremely versatile and able to import most spatial data formats into R. The only argument that needs to be supplied to `st_read` is the fullpath to the UTLA file

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Use `st_read()` to load the kenya sub-county boundaries you downloaded at the beginning of the tutorial, as`kenya_2017`.
2.  UTLA path - `https://data.humdata.org/dataset/cod-ab-ken`
3.  Make a static map of the object you have just created using `geom_sf()`.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}

kenya_2017 <- sf::st_read("/Users/christophersteinberg/Documents/GitHub/admn2/ken_admbnda_adm2_iebc_20191031.shp", quiet = TRUE)
ggplot2::ggplot(kenya_2017) + 
  ggplot2::geom_sf() 

```

</details>

<br>

Sub-county level boundaries have loaded correctly but they currently cover all of Kenya. Because simple feature objects are data frames with a geometry column attached, any operations that we would perform on a normal data frame can also be performed on an object of class `sf`. We will use `dplyr::filter` and `stringr::str_detect()` to only keep UTLAs where their `ADMN_PCODE` code starts with "KE47". "KE47" denotes that an sub-county is part of Nairobi county.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Inspect kenya_2017 using `head()` or `glimpse()`, and identify which column holds the GSS codes - it should end in "cd".
2.  Create a new object called `nairobi_subcounty`. Use `dplyr::filter` alongside `stringr::str_detect()` to only keep observations which have a Administrative code starting with "KE47".
3.  Plot `nairobi_subcounty` to see if the results look correct.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
head(kenya_2017)
```

```{r}
nairobi_subcounty <- dplyr::filter(kenya_2017, stringr::str_detect(ADM2_PCODE, "KE047"))
ggplot2::ggplot(nairobi_subcounty) + 
  ggplot2::geom_sf() 
```

</details>

<br>

Finally, for the next step, we only need the outer boundary of Nairobi - all the internal subcounty boundaries have to be removed and only the outer edges kept. `sf` has a function exactly for this purpose called `st_union()`. It only takes one argument, which is the `sf` object we want to merge.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Create a new object called `nairobi_boundary` using the `st_union` function.
2.  Plot it to check the results.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
nairobi_boundary <- sf::st_union(nairobi_subcounty)
ggplot2::ggplot(nairobi_boundary) + 
  ggplot2::geom_sf() 
```

</details>

<br>

## Spatial subsetting and CRS

In addition to subsetting by value, as we did with the UTLA boundaries earlier, we can also subset observations by evaluating their spatial relationship with another data set. We can for example select all UTLAs which are fully within Wales, every Output Area intersected by a river, or all households outside of city boundaries. There are a number of different spatial relationships which can be tested and used to subset observations.

`sf` has an inbuilt function called `st_filter()` which we can use to spatially subset observations. The function takes several arguments:

-   x - `sf` data frame we want to subset - `lfb_sf`
-   y - `sf` object used to evaluate the spatial relationship - `london_boundary`

Before running any spatial operations on two spatial objects it is always worth checking if their coordinate reference systems (CRS) match. `sf` will throw an error if that's not the case. Try it for yourself below.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Use `st_filter()` to spatially subset `lfb_sf` by testing its relationship with `london_boundary`.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r, eval = FALSE}
poi_sf_nairobi <- st_filter(x = poi_sf, y = nairobi_boundary)
```

</details>

<br>

You should have got an error here saying `x st_crs(x) == st_crs(y) is not TRUE`. It means that objects x and y have different coordinate reference systems. Spatial operations require all objects to have the same CRS. We can see this for ourselves by running the `st_crs()` function, which returns the coordinate reference system of an object.

### Exercise

`r countdown(minutes = 3, seconds = 0, style = "position: relative; width: min-content;")`

1.  Run `st_crs()` on both and `lfb_sf` and `london_boundary` and compare the results.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
sf::st_crs(lfb_sf)[[1]]
```

```{r}
sf::st_crs(london_boundary)[[1]]
```

</details>

<br>

`st_crs()` provides detailed information about the CRS and projection of data, but all we need to check is its first element denoted by `[[1]]`. We can see that `lfb_sf` uses `EPSG:27700`, while `london_boundary` is set to `WGS 84`. This problem can be solved by transforming `london_boundary`'s CRS to match that of `lfb_sf`, simply by using the correct EPSG code. To do so we will use the `st_transform()` function which takes two arguments:

-   x - `sf` object to be transformed
-   crs - EPSG code that we want to transform our data to - BNG is
    27700. 

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Run `st_transform()` to transform and overwrite `london_boundary`. Remember to set the correct CRS.
2.  Run `st_crs()` on `lfb_sf` and newly transformed `london_boundary` and compare the results.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
london_boundary <- sf::st_transform(london_boundary, crs = 27700)

sf::st_crs(lfb_sf)[[1]]
```

```{r}
sf::st_crs(london_boundary)[[1]]
```

```         
[1] "EPSG:27700"
```

Now that the CRS are matching we should be able to spatially subset `lfb_sf`.

</details>

<br>

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Use `st_filter` to spatially subset `lfb_sf` by testing its relationship with `london_boundary`. Overwrite `lfb_sf` with the subset data.
2.  Plot it to check if the results are correct - all points should be within London.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.2.2. Solution</strong>

</summary>

<br>

```{r}
lfb_sf_filter <- sf::st_filter(x = lfb_sf, y = london_boundary)
ggplot2::ggplot() + 
  ggplot2::geom_sf(london_boundary)  +
  ggplot2::geom_sf(lfb_sf_filter)
```

</details>

<br>

## Spatial and non-spatial joins

Simple features data can be joined to other data sets in two ways. We can either use a traditional, SQL like join, based on a value shared across the data sets or, since we have a geometry column, on the spatial relationship between the data sets. This is known as a spatial join, where variables from one data set are joined to another only on the basis of their spatial relationship. The most commonly used operation is known as a Point-in-Polygon join where data from a polygon is joined to the points within them.

In `sf` spatial joins are handled using the `st_join(x, y)` function with arguments:

-   x - `sf` object to which we are joining data (LHS in SQL)
-   y - `sf` object whose variables are being joined (RHS in SQL)

We will be joining the Middle Super Output Areas to LFB locations, which will then allow us to group and plot data at MSOA level. It's important to note that we are using two different sets of MSOA boundaries - one to perform a spatial join, and another to visualise the data. Full Extent (BFE) boundaries are used for the former. This ensures that all points are correctly joined to MSOA boundaries, and that we don't lose any data around bodies of water. To plot the data we use Super Generalised Clipped (BSC) boundaries. They are smaller in size, load faster, and, because they're clipped to the coastline, appear in a way that's familiar to most users.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Read in BFE boundaries`data/shp/MSOA_2011_london/MSOA_2011_BFE_London.gpkg` as `msoa_london_BFE` - use `st_read()`
2.  Overwrite `lfb_sf` by running `st_join()` between `lfb_sf` and `msoa_london_BFE`
3.  Inspect it using `head()` or `glimpse()` to see what columns have been added.

```{r}
msoa_london_BFE <- sf::st_read("data/shp/MSOA_2011_london/MSOA_2011_BFE_London.gpkg", quiet = TRUE)

lfb_sf <- sf::st_join(lfb_sf, msoa_london_BFE) 

head(lfb_sf)
```

```         
Simple feature collection with 6 features and 12 fields
geometry type:  POINT
dimension:      XY
bbox:           xmin: 504650 ymin: 164950 xmax: 554650 ymax: 192350
projected CRS:  OSGB 1936 / British National Grid
```

::: {pagedtable="false"}
:::

Now that `MSOA11CD` is attached to our observations we can create summary statistics for each MSOA. As mentioned before, we can use `tidyverse` functions on `sf` objects. Here, we will use `dplyr` to calculate the total number of incidents and their cost, and then use a non spatial join to attach those results to MSOA boundaries. At this stage we no longer need the geometry column for each LFB incident as a) we're not performing any spatial operations on our points, and b) the geometry column can slow down/interrupt the `dplyr::group_by` function which we will be using. To remove the geometry column we use the `st_drop_geometry()` function directly in the `dplyr` pipe.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Read in BSC boundaries`data/shp/MSOA_2011_london/MSOA_2011_BSC_London.shp` as `msoa_london_BSC` - use `st_read()`
2.  Use `st_drop_geometry()` on `lfb_sf` to remove geometry data.
3.  Create summary statistics per MSOA - sum of `cost_gbp` as `total_cost` (use `na.rm = TRUE`), and the total number of incidents as `n_cases`. You will need to use `group_by()` and `summarise()`
4.  Use `mutate` to create a new column called `cost_per_incident` which is equal to `total_cost` divided by `n_cases`.
5.  Join `lfb_msoa_stats` to `msoa_london_BSC`, using `left_join()` and create a new object `msoa_lfb`

```{r}
msoa_london_BSC <- sf::st_read("data/shp/MSOA_2011_london/MSOA_2011_BSC_London.shp", quiet = TRUE) 

lfb_msoa_stats <- lfb_sf %>% 
                  sf::st_drop_geometry() %>% 
                  dplyr::group_by(MSOA11CD) %>% 
                  dplyr::summarise(total_cost = sum(cost_gbp, na.rm=TRUE), n_cases = n()) %>% 
                  dplyr::mutate(cost_per_incident = total_cost/n_cases)
                         
msoa_lfb <- dplyr::left_join(msoa_london_BSC, lfb_msoa_stats)
head(msoa_lfb)
```

At this stage it is a good idea to save our data. We can do this using the `st_write()` function. It needs an `sf` object and the path and name of the output.

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Copy and execute the following code to save your data: `st_write(msoa_lfb,"output/msoa_lfb.gpkg)`

## Making better maps

Now that we have processed our data we can start mapping it. So far we have only used the `geom_sf()` function from the `ggplot2` package. This creates a default map and is great when all we want to do is quickly visualise our data. The full range of `ggplot2` (and `ggspatial`) functions gives us control over all elements of the final plot and allows us to create high quality maps.

`ggplot2` follows the grammar of graphics, where we first specify the data source - `tm_shape`, then the aesthetics of the plot - `tm_polygons`, `tm_dots`, etc., and then we make any final adjustments to the overall look - `tm_layout`. All functions need to be connected using the `+` symbol.

-   `tm_shape()` - `sf` object which you want to plot
-   `tm_fill()`, `tm_borders()`, `tm_polygons()`, `tm_dots()` - types of output
-   `tm_layout()` - controls layout of the map, titles, labels, etc.

`tmap` syntax: `tm_shape(sf_object) + tm_borders(col = either "colour" or name of column which we want to plot) + tm_layout(main.title = "title of your map")`

### Exercise

`r countdown(minutes = 10, seconds = 0, style = "position: relative; width: min-content;")`

1.  Start by specifying which `sf` object is being mapped in `tm_shape()` and what column holds the values to be visualised. We will also change the legend's title.

```{r}
ggplot(msoa_lfb) + 
  geom_sf(aes(fill = cost_per_incident)) +
  labs(fill = "Cost per Incident (£)")
```

2.  Now let's add `london_boundary` to have a thicker line around London.

```{r}
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  labs(fill = "Cost per Incident (£)")
```

3.  Next we will add a scale bar and position it in the bottom left corner.

```{r}
# First, install and load ggspatial if not already available
# install.packages("ggspatial")
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  labs(fill = "Cost per Incident (£)")
```

4.  We can now remove the black frame from the map and add a title to our map.

```{r}
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(panel.grid = element_blank())
```

5.  All of the map elements are now visible but they're not in the right place. We can solve this by increasing the margins around our map. This will allow the title and the legend to move outwards.

```{r}
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  )
```

6.  `tmap` breaks up numerical data into evenly sized categories by default, but you can provide it with custom breaks as well.

```{r}
seq(0, 900, 300)
```

```{r}
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  scale_fill_gradientn(
    colors = blues9,  # From RColorBrewer palette
    breaks = seq(0, 900, 300),
    labels = c("0 - 300", ">300 - 600", ">600 - 900", ">900 - 1200")
  ) +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  )
```

You can also use other methods of automatic data categorisation. `Jenks` is a popular method for clustering data into classes. You can set the number of desired classes using the `n` argument, and the exact method with the `style` argument.

```{r}
# For Jenks natural breaks
# install.packages("classInt")
library(classInt)
breaks <- classIntervals(msoa_lfb$cost_per_incident, n = 4, style = "jenks")$brks

ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  scale_fill_gradientn(
    colors = blues9,
    breaks = breaks,
    labels = paste(round(breaks[-length(breaks)]), "-", round(breaks[-1]))
  ) +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  )
```

We will use the manually set breaks to ensure consistent results. It's also important to change the legend labels to ensure there are no overlapping values.

```{r}
ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  scale_fill_gradientn(
    colors = blues9,  # From RColorBrewer palette
    breaks = seq(0, 900, 300),
    labels = c("0 - 300", ">300 - 600", ">600 - 900", ">900 - 1200")
  ) +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  )
```

Finally, save your map as an R object and export it.

```{r}
average_cost <- ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident)) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  annotation_scale(location = "bl") +
  scale_fill_brewer(
    palette = "Blues", 
    breaks = seq(0, 900, 300),
    labels = c("0 - 300", ">300 - 600", ">600 - 900", ">900 - 1200")
  ) +
  labs(
    title = "Average cost of animal related incidents between 2009 and 2020",
    fill = "Cost per Incident (£)"
  ) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    plot.margin = margin(0.5, 0.5, 0.5, 0.5, "cm")
  )


```

```{r}
# Save the map
ggsave("output/maps/average_cost_msoa.png", average_cost, width = 8, height = 5)
```

You can also view your choropleth as an interactive map.

```{r}

# Create a basic ggplot map
p <- ggplot() + 
  geom_sf(data = msoa_lfb, aes(fill = cost_per_incident, text = paste("MSOA:", MSOA11CD, "<br>Cost:", round(cost_per_incident, 2)))) +
  geom_sf(data = london_boundary, fill = NA, color = "black", size = 1) +
  scale_fill_gradientn(
    colors = blues9,
    breaks = seq(0, 900, 300),
    labels = c("0 - 300", ">300 - 600", ">600 - 900", ">900 - 1200")
  ) +
  labs(fill = "Cost per Incident (£)") +
  theme_minimal() +
  theme(panel.grid = element_blank())

# Convert to interactive plot
interactive_map <- ggplotly(p, tooltip = "text")
interactive_map
```

## Recommended resources

[Geocomputation with R](https://geocompr.robinlovelace.net/index.html)

[Simple Features for R](https://r-spatial.github.io/sf/index.html)

[Spatial Data Science with R](https://www.rspatial.org/)

### Exercise

`r countdown(minutes = 15, seconds = 0, style = "position: relative; width: min-content;")`

1.  Using the inbuilt dataset `relig_income`, pivot the income columns into a long dataset.
2.  Assign this to an object called `income_long`
3.  Using the dataset income_long you created, plot a bar chart in ggplot. Use religion as the x axis and the count of people as the y axis.
4.  Assign the income groupings to the colour aesthetic.

<details>

<summary style="font-size: 20px; font-weight: bold;">

5.4.1. Solution</strong>

</summary>

<br>

```{r, eval = FALSE}
#Tidy the data using pivot_longer()
income_long <- relig_income |>
  tidyr::pivot_longer(cols = -religion,
               names_to = "income_group",
               values_to = "count")

#Plot your data 
income_chart <- 
  ggplot(income_long, aes(x = religion, y = count, fill = income_group)) +
  #stat = identity creates a stacked bar chart
  geom_bar(stat = "identity") +
  labs(x = "Religion", y = "Number of People", fill = "Income Group") +
  theme_minimal()

#print() allows you to view the chart in the 'Plots' tab
print(income_chart)
```

</details>

<br>
