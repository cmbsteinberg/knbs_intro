[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KNBS: Introduction to R",
    "section": "",
    "text": "1 R Curriculum\nThis comprehensive curriculum is designed to take KNBS employees from novice R users to advanced practitioners, equipping them with the skills necessary to perform sophisticated data analysis and programming tasks. The curriculum is structured into three distinct levels - Beginner, Intermediate, and Advanced - each building upon the knowledge gained in the previous level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#session-aims",
    "href": "index.html#session-aims",
    "title": "Introduction to R - KNBS",
    "section": "1.1 Session aims",
    "text": "1.1 Session aims\n\nnavigate the R and R Studio environment\nunderstand and use the common R functions for data manipulation\nunderstand the basics of data visualisation using the ggplot2 package\nunderstand the term tidy data and why it is important for writing efficient code",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-r",
    "href": "index.html#what-is-r",
    "title": "Introduction to R - KNBS",
    "section": "1.2 What is R?",
    "text": "1.2 What is R?\nR is an open-source programming language and software environment, designed primarily for statistical computing. It has a long history - it is based on the S language, which was developed in 1976 in Bell Labs, where the UNIX operating system and the C and C++ languages were developed. The R language itself was developed in the 1990s, with the first stable version release in 2000.\nR has grown rapidly in popularity particularly in the last five years, due to the increased interest in the data science field. It is now a key tool used by analysts in governments globally.\nSome of the advantages:\n\nIt is popular - there is a large, active and rapidly growing community of R programmers, which has resulted in a plethora of resources and extensions.\nIt is powerful - the history as a statistical language means it is well suited for data analysis and manipulation.\nIt is extensible - there are a vast array of packages that can be added to extend the functionality of R, produced by statisticians and programmers around the world. These can range from obscure statistical techniques to tools for making interactive charts.\nIt’s free and open source - a large part of its popularity can be owed to its low cost, particularly relative to proprietary software such as SAS or STATA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#introducing-rstudio",
    "href": "index.html#introducing-rstudio",
    "title": "Introduction to R - KNBS",
    "section": "1.3 Introducing RStudio",
    "text": "1.3 Introducing RStudio\nRStudio is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with. RStudio has four main panels;\n\nScript Editor (top left) - used to write and save your code, which is only run when you explicitly tell RStudio to do so.\nConsole (bottom left) - all code is run through the console, even the code you write in the script editor is sent to the console to be run. It’s perfect for quickly viewing data structures and help for functions but should not be used to write code you want to save (that should be done in the script editor).\nEnvironment (top right) - all data, objects and functions that you have read in/created will appear here.\nFiles/Plots/Help (bottom right) - this pane groups a few miscellaneous areas of RStudio.\n\nFiles acts like the windows folder to navigate between files and folders.\nPlots shows any graphs that you generate.\nPackages let’s you install and manage packages currently in use.\nHelp provides information about packages or functions, including how to use them.\nViewer is essentially RStudio’s built-in browser, which can be used for web app development.\n\n\nYou may have noticed that your Script Editor is bigger than the Console or your Environment has suddenly disappeared. In RStudio, you can adjust the size of different panes by clicking and dragging the dividers between them. If you want to maximize a specific pane, such as the Script Editor, use the shortcut Ctrl + Shift + 1 (Windows/Linux) or Cmd + Shift + 1 (Mac) to focus on it. To restore the default layout, press Ctrl + Shift + 0 (Windows/Linux) or Cmd + Shift + 0 (Mac). You can also use the View menu to toggle different panes on and off, ensuring your workspace suits your needs.\nIf you find the text difficult to read or prefer a different appearance, you can customise the theme, font, and text size in RStudio. Go to Tools &gt; Global Options &gt; Appearance, where you can choose from different editor themes (e.g., light or dark mode), adjust the font type, and increase or decrease the text size for better readability. These changes can help make coding more comfortable, especially during long sessions.\n\n1.3.1 Recommended Changes\nWhile not necessary, certain changes are almost always recommended for visibility reasons. These include: - Choosing a different theme, as Textmate can be hard on the eyes. This can be done in Tools &gt; Global Options &gt; Appearance &gt; Editor theme:. - Highlight R function calls. This makes functions look a different colour than normal text, which can make reading your code much easier. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Highlight R function calls. - Use Rainbow Parenthesis. This makes each pair of () in a line a different colour, which can help you catch if you’re missing one and it’s breaking your code. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Use rainbow parenthesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#basic-syntax",
    "href": "index.html#basic-syntax",
    "title": "Introduction to R - KNBS",
    "section": "1.4 Basic Syntax",
    "text": "1.4 Basic Syntax\n\n1.4.1 Exercise\n −+ 03:00 \nAs a quick exercise, try out some arithmetic in your console:\n\n25 * 15\n(45 + 3) ^ 2\n78 / 4\n\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n\nSolution\n\n\n\n25 * 15\n\n[1] 375\n\n(45 + 3) ^ 2\n\n[1] 2304\n\n78 / 4\n\n[1] 19.5\n\n\n\n\n\n\n1.4.2 The assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\n1.4.3 Exercise\n −+ 05:00 \n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\nSolution\n\n\n\nx1 &lt;- 14\nx1\n\n[1] 14\n\n\n\nx2 &lt;- x1 + 7\nx2\n\n[1] 21\n\n\n\nx3 &lt;- x2 / 3\nx3\n\n[1] 7\n\n\n\n\n\n\n1.4.4 Combining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\n1.4.5 Exercise\n −+ 05:00 \n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n\n\n\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n[1] 1 2 3\n\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x10a8e78b8&gt;\n&lt;environment: namespace:lubridate&gt;\n\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "Introduction to R - KNBS",
    "section": "1.5 Functions",
    "text": "1.5 Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\n1.5.1 Exercise\n −+ 05:00 \n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n\n\n\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n[1] 36",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Introduction to R - KNBS",
    "section": "1.6 Packages",
    "text": "1.6 Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#the-tidyverse",
    "href": "index.html#the-tidyverse",
    "title": "Introduction to R - KNBS",
    "section": "1.7 The Tidyverse",
    "text": "1.7 The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\n1.7.1 Exercise\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\n\n\n\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html",
    "href": "data_manipulation.html",
    "title": "3  Manipulating Data",
    "section": "",
    "text": "3.1 Select\nIn this chapter, we will cover dplyr, one of the most essential packages in an R user’s toolkit. As a key part of the tidyverse, dplyr offers easy-to-use functions for manipulating data frames, which is a vital step in the data analysis process.\nTo illustrate the key functions of dplyr, we’ll be using the gapminder dataset. You can view this dataset by installing and loading the gapminder package, just as you did with tidyverse.\nThe exercises in this chapter will use an inbuilt R dataset. However, if you’d like to follow along with the examples, you’re welcome to load the gapminder dataset, although please note it will not be required for the exercises themselves.\nThere are six key dplyr functions that allow you to solve the vast majority of data-manipulation challenges;\nThese functions look similar to SQL statements and are designed to replace the need for any data manipulation in SQL.\nAll dplyr functions allow you to specify the column names without “quotations”. However, if there are spaces in the column name, you’ll need to use `back ticks`.\nSelect allows you to choose the columns that you’d like to keep from a dataset.\n?select\nLooking at the gapminder dataset, if we want to create a new dataset which only included the year, country and life expectancy, we could do this by selecting those columns:\ngapminder_life_exp &lt;- select(gapminder, year, country, lifeExp)\nThe first argument within the select command specifies use of the gapminder dataset. Following this we list the variables we want to keep.\nIt is also possible to select to exclude specific columns. This is ideal if you want to keep all columns except for one or two, and can be done by using a - minus sign in front of column names.\n#Removes the continent column but keeps all others\ngapminder_no_continent &lt;- select(gapminder, -continent)\nYou can also use the select function to reorder columns as it will select columns in the order specified.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#select",
    "href": "data_manipulation.html#select",
    "title": "3  Manipulating Data",
    "section": "",
    "text": "3.1.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUsing the inbuilt dataset airquality, select to keep the columns Ozone, Temp, Month and Day.\n\n\n\nSolution\nairquality_exercise &lt;-\n  select(airquality, Ozone, Temp, Month, Day)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#pipes",
    "href": "data_manipulation.html#pipes",
    "title": "3  Manipulating Data",
    "section": "3.2 Pipes",
    "text": "3.2 Pipes\nBefore we continue, let’s visit one of the most important (and cool) operators in R… the pipe |&gt;. You may have seen the old pipe %&gt;% before. This has been replaced with a native pipe in R 4.1 onwards. It’s likely that you’ll want to use multiple functions consecutively, especially when using dplyr. Currently, we may do something like this:\n\ngapminder_new &lt;- select(gapminder, -continent)\ngapminder_new &lt;- select(gapminder_new, year, country, lifeExp)\n\nThis code is a little frustrating to write because we have to name each intermediate data frame, even though we don’t care about it. Naming things is hard, and having separate names for each step makes it difficult to read. Let’s see how we can rewrite this code using the pipe:\n\ngapminder_new &lt;- gapminder |&gt;\n select(-continent) |&gt; \n select(year, country, lifeExp)\n\nThe pipe means we can read this code as a series of statements separated by the pipe representing “and then”; e.g. take the gapminder data and then remove the continent column and then select the year, country and lifeExp columns.\nYou may notice that we don’t need to specify the data argument in each function when using |&gt;. By piping, the subsequent function recognises we want to use the result of our previous statement as our data.\nYou can quickly insert the pipe operator in RStudio by pressing Ctrl + Shift + M (Windows) or Cmd + Shift + M (Mac). This will still insert the old pipe %&gt;%. This can be changed in Tools &gt; Global Options &gt; Code &gt; Editing &gt; Use native pipe operator, |&gt;",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#grouping-and-summarising-data",
    "href": "data_manipulation.html#grouping-and-summarising-data",
    "title": "3  Manipulating Data",
    "section": "3.3 Grouping and summarising data",
    "text": "3.3 Grouping and summarising data\nWe can produce breakdowns of statistics using the group_by and summarise commands from the dplyr package:\n\ngroup_by() identifies which variables we want to produce breakdowns by.\nsummarise() is used to indicate which values we want to calculate.\n\nUsing these functions together we can produce summary statistics in a similar way to pivot tables in Excel. We can use the pipe (|&gt;) operator to chain these functions together.\nSo if we want the mean life expectancy by continent and year:\n\nmean_life_exp &lt;- gapminder |&gt;\n  group_by(year, continent) |&gt;\n  summarise(life_exp = mean(lifeExp))\n\nHere R takes the dataset, then groups it first by year and then by continent and then outputs the mean life expectancy. The mean life expentancy variable is created as a new column called life_exp. The results are saved into a new dataset called mean_life_exp.\nThere are other functions that could be used here instead of mean e.g. n, n_distinct, min, max, mean, median, var and sd.\nIf we want to add a new variable that we decide to call country_count that provides the counts by year and continent we can rerun as follows using the pipe operator:\n\nmean_life_exp &lt;- gapminder |&gt;\n  group_by(year, continent) |&gt;\n  summarise(life_exp = mean(lifeExp), country_count = n())\n\n\n3.3.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUsing the pipe function, group the airquality dataset by month.\nSummarise the grouped dataset to produce an average of Ozone and Temp by month.\nAssign this to an object called airquality_summarised.\n\n\n\nSolution\nairquality_summarised &lt;- airquality |&gt;\n  group_by(Month) |&gt;\n  summarise(avg_ozone = mean(Ozone, na.rm = TRUE),\n            avg_temp = mean(Temp, na.rm = TRUE))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#filter",
    "href": "data_manipulation.html#filter",
    "title": "3  Manipulating Data",
    "section": "3.4 Filter",
    "text": "3.4 Filter\nIf you would like to produce statistics for a subset of rows or observations, a good function to use is filter() from the dplyr package.\nLet’s first take a look at the different possible values of the continent variable. We can do that quickly using the group_by/summarise combination.\n\ngapminder |&gt;\n  group_by(continent) |&gt;\n  summarise(count = n())\n\nTo filter we just specify the data that we want to filter (gapminder) and the value that we want to filter on. In this case lets filter where continent is “Asia” and year is after 1992 then recalculate the mean life expectancy by country:\n\nmean_life_exp &lt;- gapminder |&gt; \nfilter(continent == \"Asia\" & year &gt; 1992) |&gt;\ngroup_by(country) |&gt;\nsummarise(life_exp = mean(lifeExp))\n\nR provides the standard suite of comparison operators which can be used to filter:\n\n\n\nComparison\nOperator\n\n\n\n\nGreater than\n&gt;\n\n\nGreater than or equal to\n&gt;=\n\n\nLess than\n&lt;\n\n\nLess than or equal to\n&lt;=\n\n\nEqual to\n==\n\n\nNot equal to\n!=\n\n\nAnd\n&\n\n\nOr\n|\n\n\nNot\n!\n\n\nGroup membership\n%in%\n\n\n\nThe %in% operator allows you to compare a column against a vector of values to see if it matches any one of them; this is much more convenient than comparing against each value individually.\n\n##This does work to filter the data for the three given years but is clunky to read and edit\ngapminder |&gt; \nfilter(year == 1992 | year == 1998 | year == 2002) \n\n##Using the %in% operator is simple and clean to read, and gives exactly the same result\ngapminder |&gt; \nfilter(year %in% c(1992, 1998, 2002)) \n\n\n3.4.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nFilter the original airquality dataset to only include data for May and June. Try to do this using the %in% function.\nCall this assignment: airquality_filter.\n\n\n\nSolution\nairquality_filter &lt;- airquality |&gt;\n  filter(Month %in% c(5, 6))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#rename",
    "href": "data_manipulation.html#rename",
    "title": "3  Manipulating Data",
    "section": "3.5 Rename",
    "text": "3.5 Rename\nWe can rename variables using the dplyr function rename(). Let’s amend our previous code creating the mean_life_exp dataset to change the name of the “year” column to “selected_year”.\n\nmean_life_exp &lt;- gapminder |&gt; \n  filter(continent == \"Asia\" & year &gt; 1992) |&gt;\n  group_by(year, country) |&gt;\n  summarise(life_exp = mean(lifeExp)) |&gt;\n  rename(selected_year = year)\n\nWithin the rename function, the new name “selected_year” is specified on the left and the old name on the right of the equal sign.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#mutate",
    "href": "data_manipulation.html#mutate",
    "title": "3  Manipulating Data",
    "section": "3.6 Mutate",
    "text": "3.6 Mutate\nYou can create new columns and perform calculations on existing columns using the dplyr command mutate().\n\n?mutate\n\nFor example, imagine we wanted to calculate overall GDP as a new column in the gapminder dataset. We could do this by multiplying the gdpPercap and pop columns:\n\ngapminder |&gt;\n  mutate(gdp_total = gdpPercap * pop)\n\nYou can also use functions like mean() and sum() in mutate(). For example, using x / sum(x) for calculating proportions of a total and y - mean(y) for difference from the mean.\nNotice that by default, mutate calculates values on a rowwise basis; each value in the gdp_total column is made by multiplying the values in the corresponding row. This default behaviour can be changed by grouping data before mutate, e.g. this code produces a mean population column by country:\n\ngapminder |&gt;\n group_by(country) |&gt;\n mutate(mean_pop = mean(pop))\n\nYou can also combine mutate with the case_when function to perform one or more if/else conditions. Maybe we want to have coded values for each year by decade. The case_when function allows you to provide multiple instances of a statement which evaluates to TRUE/FALSE, and then a result if that condition is true (after ~). The function evaluates these statements in order, so if an earlier statement is TRUE, a later one will not be evaluated. Finally, for cases that don’t meet any of the conditions, the final TRUE value is used (this defaults to NA if not specified)\n\ngapminder |&gt;\n  mutate(\n    decade =\n      case_when(\n        year &gt;= 1950 & year &lt; 1960 ~ \"1950s\",\n        year &gt;= 1960 & year &lt; 1970 ~ \"1960s\",\n        year &gt;= 1970 & year &lt; 1980 ~ \"1970s\",\n        year &gt;= 1980 & year &lt; 1990 ~ \"1980s\",\n        year &gt;= 1990 & year &lt; 2000 ~ \"1990s\",\n        TRUE ~ \"Post-2000\"\n      )\n  )\n\nYou can download the Data Transformation Cheat Sheet (and other cheatsheets) at: https://www.rstudio.com/resources/cheatsheets/\n\n3.6.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nThe inbuilt trees dataset includes columns for tree girth and height in inches. Using mutate, create two new columns (Girth_cm and Height_cm) containing the equivalent values in centimetres.\nHow can you replace an existing column with a new column using mutate?\n\n\n\nSolution\n# 1. conversion is 2.54\n\ntrees_with_cm &lt;- trees |&gt;\n  mutate(Girth_cm = Girth * 2.54,\n         Height_cm = Height * 2.54)\n\n# 2. Using mutate to replace existing column\n\ntrees_with_cm &lt;- trees |&gt;\n  mutate(Girth = Girth * 2.54)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#arrange",
    "href": "data_manipulation.html#arrange",
    "title": "3  Manipulating Data",
    "section": "3.7 Arrange",
    "text": "3.7 Arrange\narrange() is used to change the order of rows. It takes a data frame as it’s first argument and a column name to sort by as it’s second. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. By default arrange() will sort in ascending order (1-9 and A-Z). If you’d like to sort in descending order wrap the column name in desc(). Using arrange with one column sorts how you’d might expect:\n\ngapminder |&gt;\n arrange(year)\n\nSorting with multiple columns sorts within the hierarchy specified:\n\ngapminder |&gt;\n  arrange(year, desc(continent))\n\nIt’s worth noting that missing values (NA) are always sorted at the end:\n\ndf &lt;- tibble(x = c(1, 2, 3, NA))\narrange(df, x)\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1     1\n2     2\n3     3\n4    NA\n\narrange(df, desc(x))\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1     3\n2     2\n3     1\n4    NA\n\n\n\n3.7.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nArrange the rows of the trees dataset by increasing height and decreasing girth.\nWhat do you notice?\n\n\n\nSolution\ntrees_arranged &lt;- trees |&gt;\n  arrange(Height, desc(Girth))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html",
    "href": "importing_data.html",
    "title": "4  Importing Data",
    "section": "",
    "text": "4.1 Local Reading\nSo far, we have only made use of data which is pre-loaded into R via packages, but it is also possible to load your own data in from a variety of sources. We will focus on two different file types;\nAs described in Chapter 1, the bottom right pane of RStudio allows you to view files that are within your own personal filesystem. You are free to create new folders in this area, using the New Folder button.\n−+\n05:00\ngcp &lt;- read_csv(\"data/gcp.csv\")\nGreat, we will come back to using these two files, but first let’s discuss how to read in csv files.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#local-reading",
    "href": "importing_data.html#local-reading",
    "title": "4  Importing Data",
    "section": "",
    "text": "Navigate to the Files at the bottom right pane of your RStudio\nCreate a new folder called data\nSave this GCP dataset to this folder as gcp.csv. You will need to press the download button in the top right corner.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#csv",
    "href": "importing_data.html#csv",
    "title": "4  Importing Data",
    "section": "4.2 CSV",
    "text": "4.2 CSV\nAlthough there is a read.csv() function in base R, like most things there is a better tidyverse alternative! read_csv() from the readr package reads CSVs in as a tibble (which has additional features compared to a standard data frame), is much faster (~10X), and allows you to specify how you read data in more easily.\nAs always, let’s read the function documentation using ?read_csv. This tells us we need to provide a path to the file. This path can be either local or remote; so it will work equally well for data inside your project or from the internet.\nTo read in a local file, you have to specify the exact location of the file. You can do this as either an absolute filepath, which starts from the drive name right through to the final file (e.g. C:/Documents/My_work/file.csv), or as a relative file path. A relative file path just gives the location of the file starting from your current working environment. You can check what your current working environment is using the command getwd(). The advantage of using relative file paths is if someone duplicates your project from Github, the code will still work on their own computer.\nWe will start by reading in some local data, which contains details of Kenyan Gross County Product by economic activity for 2017:\n\ngcp &lt;- read_csv(\"data/gcp.csv\")\n\nNew names:\nRows: 48 Columns: 22\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Coun- ty Code, County Name, FISIM1 num (18): Agriculture, forestry and\nfishing, Mining and quarrying, Manufactu... lgl (1): ...22\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...22`\n\n\nNotice that the file is inside the data folder inside the current working directory.\ngcp will now show in your environment. The environment viewer (top right) shows you basic information about the data that has been loaded in. You can also click on any object to view it in your script window.\nYou can also read in data directly from the web using the same function. For example, with the same GCP data:\n\ngcp &lt;- read_csv(\"https://raw.githubusercontent.com/cmbsteinberg/knbs_intro/refs/heads/main/data/gcp.csv\")\n\nNew names:\nRows: 48 Columns: 22\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): Coun- ty Code, County Name, FISIM1 num (18): Agriculture, forestry and\nfishing, Mining and quarrying, Manufactu... lgl (1): ...22\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...22`\n\n\nThis works exactly the same way as reading in local data, and the object you have created will appear in your environment (top right).\n\n4.2.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nRead in the frogs dataset found here:‘https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv’ as an object called frogs\n\n\n\nSolution\nfrogs &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv\")\n\n\n\n\n4.2.2 Inspecting the dataset\nAs noted in the previous section, you can see by looking in the environment window that the eurovision dataset has 48 observations and 22 variables. You can also return this information (and more) about datasets programatically, using the glimpse() function, again from the dplyr package:\n\nglimpse(gcp)\n\nRows: 48\nColumns: 22\n$ `Coun- ty Code`                                        &lt;chr&gt; \"01\", \"02\", \"03…\n$ `County Name`                                          &lt;chr&gt; \"MOMBASA\", \"KWA…\n$ `Agriculture, forestry and fishing`                    &lt;dbl&gt; 1459, 39610, 38…\n$ `Mining and quarrying`                                 &lt;dbl&gt; 1158, 1447, 333…\n$ Manufacturing                                          &lt;dbl&gt; 47348, 300, 845…\n$ `Electricity supply`                                   &lt;dbl&gt; 20546, 730, 147…\n$ `Water supply; waste collection`                       &lt;dbl&gt; 1078, 513, 1353…\n$ Construction                                           &lt;dbl&gt; 37168, 3184, 22…\n$ `Wholesale and retail trade; repair of motor vehicles` &lt;dbl&gt; 36912, 5051, 60…\n$ `Transport and storage`                                &lt;dbl&gt; 88308, 4198, 11…\n$ `Accom- modation and food service activities`          &lt;dbl&gt; 12780, 6699, 11…\n$ `Informa- tion and communi- cation`                    &lt;dbl&gt; 5413, 879, 2423…\n$ `Financial and insurance activities`                   &lt;dbl&gt; 31155, 4941, 86…\n$ `Real estate activities`                               &lt;dbl&gt; 35526, 5733, 14…\n$ `Profes- sional, technical and support services`       &lt;dbl&gt; 7124, 349, 286,…\n$ `Public admin- istration and defence`                  &lt;dbl&gt; 12024, 4566, 64…\n$ Education                                              &lt;dbl&gt; 4229, 5384, 921…\n$ `Human health and social work activities`              &lt;dbl&gt; 4539, 1575, 253…\n$ `Other service activities`                             &lt;dbl&gt; 2379, 1647, 281…\n$ FISIM1                                                 &lt;chr&gt; \"(17,026)\", \"(5…\n$ Total                                                  &lt;dbl&gt; 332122, 86278, …\n$ ...22                                                  &lt;lgl&gt; NA, NA, NA, NA,…\n\n\nAs well as returning the number of rows and columns in the data, the glimpse function also shows you the names of the columns, the column classes (indicated in ), and an example of the first few rows of data.\n\n\n4.2.3 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nUse the glimpse and View functions to examine the frogs dataset. How many rows and columns does it have?\n\n\n\nSolution\nView(frogs)\n\nglimpse(frogs)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#excel",
    "href": "importing_data.html#excel",
    "title": "4  Importing Data",
    "section": "4.3 Excel",
    "text": "4.3 Excel\nReading excel files works in much the same way as CSV files. However, due to the difference in underlying structures we require the function read_excel() from a different package called readxl.\nThe main difference when reading excel files is three additional arguments that we can set;\n\nsheet which allows us to specify which sheet to read. It can take the form of a string (the name of the sheet) or an integer (the position of the sheet); and\nrange which allows us to specify a cell range. It takes a typical cell reference like “B3:D10”.\nskip an alternative to specifying a cell range, you can simply indicate how many rows to skip at the start of the sheet. This is ideal if you want to read in a sheet with an unknown number of columns and/or rows, but know there are several lines of metadata at the top of the sheet.\n\nIf we don’t set any of these arguments it will assume our data is in the first row of the first sheet (and it becomes almost identical to read_csv above).\n\n# One option is to download the file \ntourism &lt;- read_excel(\"data/tourism.xlsx\")\n\n\n4.3.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nInstall and load the readxl package.\nDownload this dataset published by KNBS about the tourism sector, saving it into /data\nRead it in, specifying the sheet name you want to read in.\nExamine the data you have read in; are the column names what you want? Work out how to skip these and only read in the data, with the correct column names.\n\n\n\nSolution\nurl = \"https://www.knbs.or.ke/wp-content/uploads/2024/04/2023-Economic-Survey-Kenya-Tourism-Sector.xlsx\"\ndownload_first &lt;- download.file(url,destfile = \"data/tourism.xlsx\")\ntourism &lt;- read_excel(\"data/tourism.xlsx\", sheet = \"Table 12.5\", skip = 2)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#rio",
    "href": "importing_data.html#rio",
    "title": "4  Importing Data",
    "section": "4.4 Rio",
    "text": "4.4 Rio\nSometimes you may want to read a selection of files of all different types. This is where Rio can come in handy. Rio is a wrapper around the libraries we’ve used above and many more, which lets you use import() to read almost any file in. This isn’t always useful, when you want to do very specific things with a certain file, but can be much cleaner.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "5  Plotting",
    "section": "",
    "text": "5.1 Structure\nThis chapter will teach you how to visualise your data using ggplot2. R has several systems for making graphs, but ggplot2 is the most elegant and versatile. The syntax behind ggplot2 looks complicated at first, but once you understand it, it’s incredibly powerful and can be used to visualise a wide range of data.\nThe main function in ggplot2 is ggplot() which is used to initialise a plot. A plot in ggplot2 is made up of multiple elements added to each other to create layers which each add something to the appearance of the chart. The basic template for a graph is as follows:\nggplot(data = &lt;DATA&gt;) +\n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\nA geom function defines the way data and an aesthetic mapping is statistically transformed to create a plot. A plot can come in many forms, such as a bar graph, line and scatter graph, to name a few.\nA ggplot object must contain\nThis might look confusing initially, so let’s show an example with one of the pre-loaded R datasets mpg by creating a scatter plot of displacement against hwy.\n#Data to be plotted\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  #The geometric to draw the aesthetics with (in this case a point geom)\n     #The aesthetic mapping; the x axis to displacement and the y to hmwy\n  geom_point()\nThis is the basic structure of any ggplot chart, but there are plenty of things you can do to change the appearance and function of your charts within ggplot.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#structure",
    "href": "ggplot.html#structure",
    "title": "5  Plotting",
    "section": "",
    "text": "the data to be plotted as the first argument\nhow that data should be mapped to the different aspects of the plot, defined using aes() (short for aesthetics).\na geometric to draw the aesthetics with\nggplot works with layers, each added with the + operator.\nMappings are always added using the aes() command, which can be inside the ggplot() or geom.\n\n\n\n\n\n5.1.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUse the iris dataset (another built-in dataset in R) to create a simple scatter (geom_point) chart, plotting Sepal.Length as the x axis and Sepal.Width as the y\n\n\n\nSolution\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#types-of-geom-functions",
    "href": "ggplot.html#types-of-geom-functions",
    "title": "5  Plotting",
    "section": "5.2 Types of Geom Functions",
    "text": "5.2 Types of Geom Functions\nYou aren’t just limited to scatter plots; there are lots of geoms available in ggplot - the best resource for choosing an appropriate geom is the cheat sheet. This can be found at https://github.com/rstudio/cheatsheets/blob/main/data-visualization-2.1.pdf\nThe most commonly used geoms are:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_bar\nBar chart\n\n\ngeom_point\nScatter chart\n\n\ngeom_line\nLine graph\n\n\ngeom_histogram\nHistogram\n\n\ngeom_boxplot\nBox and whisker plot\n\n\ngeom_smooth\nLine of best fit style overlay\n\n\n\nYou can also add multiple geoms to a single plot, for example you can add a smoothed line to the scatter plot you have already created using geom_smooth. You can either define the aes in each of the geom calls if they are different for each layer, or define them in the initial ggplot call if they are consistent across all layers.\n\n#Aes defined in ggplot calll\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point() + #Add a + sign\n  geom_smooth()#Include a smoothed line\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n5.2.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nUse the geom_smooth aesthetic to add a smoothed line to your scatter plot.\n\n\n\nSolution\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  geom_smooth()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-different-aesthetics",
    "href": "ggplot.html#adding-different-aesthetics",
    "title": "5  Plotting",
    "section": "5.3 Adding different aesthetics",
    "text": "5.3 Adding different aesthetics\nIt’s normal that you will want to explore more than two variables within your datasets. You can do this by mapping those variables to different aspects of the chart in ggplot; things like colour, point shape, or line type.\nFor example, we could set the colour of the point to be determined by the vehicle class.\n\n# Aesthetics\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot does some clever things when deciding what colours to use - for factorial variables it will assign each factor a unique colour (as in the above example), whilst for continuous variables it will assign a colour scale.\n\n# Here year is coloured as a continuous variable with a colour scale\nggplot(data = mpg, aes(x = displ, y = hwy, colour = year))+\n  geom_point()\n\n\n\n\n\n\n\n# Here by setting year to a factor it is coloured as a discrete variable with a unique colour for each\nggplot(data = mpg, aes(x = displ, y = hwy, colour = factor(year)))+\n  geom_point()\n\n\n\n\n\n\n\n\nThere are a wide range of other aesthetics you can set to indicate different categories including:\n\nPoint shape (shape)\nLine type (linetype)\nSize of points (size)\nTransparancy of points (alpha)\n\nApplying multiple aesthetics should be used with caution though; indicating more than one variable using aesthetics can quickly make a chart difficult to read!\n\n# A chart wit multiple aesthetics applied.\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class, size = cty))+\n  geom_point(shape = 5)\n\n\n\n\n\n\n\n\nYou also don’t have to map aesthetics onto variables; you can specify them manually if you don’t want them to be related to a variable. To do this, you need to specify the colour, shape, linetype, etc outside of the aesthetic call. For example, you can define the colour of the points:\n\n#Aesthetics related to variables are mapped inside the aes call\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  #Aesthetics that are manually set are mapped outside the aes call\n  geom_point(colour = \"orange\")\n\n\n\n\n\n\n\n\n\n5.3.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nMap the colour aesthetic of your chart to correspond to the Species in the iris dataset.\nManually map the shape of the geom_point to be type 3\n\n\n\nSolution\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point(shape = 3)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-layers",
    "href": "ggplot.html#adding-layers",
    "title": "5  Plotting",
    "section": "5.4 Adding Layers",
    "text": "5.4 Adding Layers\nThis produces the basics of any ggplot2 chart, however it doesn’t always make the most attractive chart. To improve the appearance of the chart, the ggplot2 package has a wide range of functions which can be added to your basic chart to change everything from the legend, titles, or scales shown in the chart.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#scales",
    "href": "ggplot.html#scales",
    "title": "5  Plotting",
    "section": "5.5 Scales",
    "text": "5.5 Scales\nChanging the x and y axes can be done using the scale_x_ and scale_y_ group of functions. There is a different type of these functions for each different type of scale and axis, and you need to take care you use the right one in each case!\n\n##For a continuous Y axis\nggplot(data, aes(x = x_axis, y = y_axis))+\n  scale_y_continuous()\n\n##For dates on the X axis\nggplot(data, aes(x = x_axis, y = y_axis))+\n  scale_x_date()\n\nAn example of using a percent scale:\n\n# Scales \nggplot(data = mpg1, aes(x = displ, y = gallon_percent, colour = class))+\n  geom_point()+\n  #Set name for axis\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nYou can change a large number of aspects of both the appearance and function of the axes using these functions, including:\n\nName on the axis\nChange the minimum and maximum values on the scale\nSet major and minor values on the scale\nPosition of the axis\nType-specific changes such as setting the appearance of dates or transforming to log scale\n\n\n# Aesthetics\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  #Set name for axis\n  scale_x_continuous(name = \"displacement\",\n                     #Set min and max limits\n                     limits = c(0,8))\n\n\n\n\n\n\n\n\nCheck the arguments available for any scale function using ? in front of it in the console; e.g. ?scale_x_date",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#changing-colour-palettes",
    "href": "ggplot.html#changing-colour-palettes",
    "title": "5  Plotting",
    "section": "5.6 Changing colour palettes",
    "text": "5.6 Changing colour palettes\nIf you don’t specify colours to use, ggplot will default to the (relatively ugly) standard palette. Luckily, there are loads of ways to easily choose more attractive colour options!\nNote that when you are changing colours in a chart, there are two different options; colour is used for points and lines in charts, while fill is for the central fill colour in objects like bars. Make sure you use the right one when calling scale arguments!\nUsing scale_colour_brewer() or scale_fill_brewer() allows you to select from one of the ColorBrewer palettes; these are designed to be attractive, and many of them are colour-blind friendly.\n\n#Chart using the standard colour brewer palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer()\n\n\n\n\n\n\n\n\nChange the palette used with the palette argument:\n\n#Chart using the Dark2 palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nYou can see the full range of palettes available with their names here:\n\n\n\n\n\n\n\n\n\nYou can also design your own custom palettes using either named colours or hex codes and pass them to your charts using the scale_*x*_manual functions:\n\n#Chart using a custom defined palette\n\nmy_cols &lt;- c(\"#DAF7A6\", \"#CCDC6D\", \"#FFC300\", \"#FF5733\", \"#C70039\", \"#900C3F\", \"#581845\")\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_manual(values = my_cols)\n\n\n\n\n\n\n\n\n\n5.6.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nChange the colour palette your chart uses to something you like better than the default! Use the scale_colour_brewer or scale_colour_manual to do this.\n\n\n\nSolution\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point(shape = 3) +\n  scale_colour_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#facets",
    "href": "ggplot.html#facets",
    "title": "5  Plotting",
    "section": "5.7 Facets",
    "text": "5.7 Facets\nFaceting charts in R is a good way to produce multiple identical charts; this feature splits data by a provided variable and plots one value per chart. It is very useful when overlapping data is difficult to read. Using the facet_wrap() function, you can pass any variable to the first argument (prefacing it with ~), as well as specifying the row/column layout of the result\n\n#Chart using the Dark2 palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  #Facet by class\n  facet_wrap(~class)+\n  scale_colour_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#titles",
    "href": "ggplot.html#titles",
    "title": "5  Plotting",
    "section": "5.8 Titles",
    "text": "5.8 Titles\nLabels and titles can be added without changing the axes using the labs command.\n\n#Themes, titles, and multiple plots\nggplot(data = mpg, aes(x = class, y =..prop.., group = 1))+\n  geom_bar()+\n  labs(title = \"Proportion of sample by class\", x = \"Class\", y = \"Proportion\")\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-themes",
    "href": "ggplot.html#adding-themes",
    "title": "5  Plotting",
    "section": "5.9 Adding themes",
    "text": "5.9 Adding themes\nChanging the theme is a quick and easy way to set many of the visual aspects of your charts, such as the appearance of grid lines, size of text, and position of the legends. You can change the theme to a number of presets:\n\n\nplot &lt;- ggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer(palette = \"Dark2\")\n\n#Applying different themes\n plot+theme_bw() \n\n\n\n\n\n\n\n plot+theme_classic() \n\n\n\n\n\n\n\n plot+theme_minimal()\n\n\n\n\n\n\n\n plot+theme_light()\n\n\n\n\n\n\n\n\n\nYou can also make your own custom themes; plot are made up of four elements element_text, element_line, element_rect, and element_blank. Plots can be modified using these element commands. For example:\n\n#You can also make your own custom themes\n#\nugly.theme &lt;-\n  theme(\n    text = element_text(colour ='orange', face ='bold'),\n    panel.grid.major = element_line(colour = \"violet\", linetype = \"dashed\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = 'black', colour = 'red')\n  )\n\n\n\nplot+ugly.theme\n\n\n\n\n\n\n\n\n\n5.9.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nAdd one of the default themes to your chart to improve its appearance.\nAdd a title and labels to your axes\n\n\n\nSolution\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point() +\n  #Add title and labels to axes\n  labs(title = \"Chart\", x = \"Sepal.Length\", y = \"Sepal.Width\") +\n  scale_colour_brewer(palette = \"Dark2\") +\n  #Add in-built R theme\n  theme_bw()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#saving-plots",
    "href": "ggplot.html#saving-plots",
    "title": "5  Plotting",
    "section": "5.10 Saving plots",
    "text": "5.10 Saving plots\nMost of the time you will want to create plots directly into an R Markdown output, or a shiny app. However plots can also be saved as image (png) file:\n\n‘Export’ button in RStudio viewer\nggsave(filename = “plotname.png”, plot = myplot) - saves the plot into your current working directory in R Studio. Can then be downloaded from the platform via ‘More’ -&gt; ‘Export…’\n\n\n5.10.1 Saving a plot with today’s date\nSomething that has been raised in this section is: what if I want to add today’s date in the filename when saving a plot? This can be useful for organising and tracking plots over time. To do this, you can use the Sys.Date() function in R, which returns the current date in YYYY-MM-DD format.\nFor example, if you wanted to save a plot with today’s date included in the filename, you could use:\n\nggsave(filename = paste0(\"plot_\", Sys.Date(), \".png\"), plot = myplot)\n\nThis will save the plot myplot as a PNG file with a filename that includes the current date, like plot_2025-03-07.png, where Sys.Date() provides today’s date in the format YYYY-MM-DD. It’s a great way to ensure your plot files are uniquely named based on the date.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Plotting</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#application-for-knbs",
    "href": "data_manipulation.html#application-for-knbs",
    "title": "1  Data Manipulation",
    "section": "1.8 Application for KNBS",
    "text": "1.8 Application for KNBS\n −+ 45:00 \nIn this application, we will practice some of the coding skills learned in the training so far. Whereas the examples used in the textbook use toy datasets which are already clean and well-formatted, the datasets used in our applications may require some initial cleaning prior to analysis. This is likely closer to tasks you might encounter in your everyday work at KNBS.\nIn this particular application, we would like you to produce 3 simple pieces of analysis using Kenya’s 2019 census data: the breakdown of Kenya by religious belief, the share of people who are migrants in each county, and the average working hours for men and women in rural vs. urban areas.\nThere are two stages to this task. The first is to read in the data and prepare it to be analysed. This will involve reading the data in properly, fixing any column name issues, reducing the size of the dataset if it is too large, and finally dealing with any missing values, or NAs, that are found in the data.\nThe second is to perform the analysis on your dataset. This will involve creating new columns,\nThe 2019 Census data you need is found at this link\n\nRead in the dataset and use janitor to clean any column names\nWe need columns relating to migration, age, etc. Select the columns we will need - look at survey metadata\nLet’s rename some columns to make them easier to understand\nFor our religion question, let’s examine the data in this column\nHow should we approach the summary? How do we want to treat NAs, or DK\n\n\n\n\nSolution\n\n\n\nlibrary(tidyverse)\n\n# Example 1\n\n\n\n\n# Example 1\n\n## Reading and cleaning names\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\n## Creating a unique identifier, to allow us to drop columns\ncensus &lt;-  census |&gt; \n  mutate(id = paste(sublocation_code, ea, ea_type, strnumber, hhnumber, line_number, sep = \"_\"),\n         hhid = paste(sublocation_code, ea, ea_type, hhnumber, sep = \"_\"))\n\nlength(unique(census$id))\n## Dropping columns that we don't need, ro red\n\nreligion &lt;- census |&gt; \n  select(sublocation_code, ea, hhnumber, line_number, p17) |&gt; \n  rename(religion = p17) |&gt; \n\nsum(is.na(census$p19))\n\nmigration &lt;- census |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19) |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmig_nairobi &lt;- migration |&gt;\n  group_by(subcounty_code) |&gt;\n  summarise(migration_prop = mean(migrant))\n\n\nworking_hours &lt;- census |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(hours_worked_if_work = p52,\n         age = p12,\n         sex = p11) |&gt;\n  filter(age &gt;= 18) |&gt;\n  mutate(\n    rural = if_else(ea_type == 1, 1, 0),\n    female = if_else(sex == 2, 1, 0),\n    hours_worked = replace_na(hours_worked_if_work, 0)\n  )\n\n\nworking_hours_mean_adult &lt;- working_hours |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )\n\nworking_hours_mean_adult |&gt;\n  ggplot(aes(y = hours_worked,  x = as.factor(female), fill = as.factor(rural))) +\n  geom_bar(stat = \"identity\", position = \"dodge\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "mapping.html",
    "href": "mapping.html",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "Aims",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#aims",
    "href": "mapping.html#aims",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "Know how to load spatial data into R using the sf library.\nBe familiar with using GSS codes to join statistics to geographies.\nUnderstand how spatial objects can be manipulated using dplyr.\nUnderstand how to use spatial joins.\nBe aware of map projections and Coordinate Reference Systems (CRS) and be able to modify them.\nKnow how to make static and interactive maps in ggplot2.\nBe able to export your maps and shapefiles.",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#gis-and-r",
    "href": "mapping.html#gis-and-r",
    "title": "Introduction to GIS in R",
    "section": "GIS and R",
    "text": "GIS and R\nR is commonly used for statistical analysis and programming, however it also has a range of geospatial libraries developed by a community of researchers and programmers. In the last few years, working with spatial data became much easier in R, with the development of the sf package. sf keeps all the spatial information for each observation in a geometry column which means that we can treat it like a normal data frame and also perform spatial operations on the data.\n\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 35.81531 ymin: -3.246071 xmax: 40.08169 ymax: 4.061832\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 18\n  name   name_en amenity man_made shop  tourism opening_ho beds  rooms addr_full\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n1 The S… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n2 Masji… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n3 Hotel… &lt;NA&gt;    pub     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n4 Rosog… &lt;NA&gt;    clinic  &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n5 AIC R… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n6 FGCK … &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n# ℹ 8 more variables: addr_house &lt;chr&gt;, addr_stree &lt;chr&gt;, addr_city &lt;chr&gt;,\n#   source &lt;chr&gt;, name_sw &lt;chr&gt;, osm_id &lt;dbl&gt;, osm_type &lt;chr&gt;,\n#   geometry &lt;POINT [°]&gt;",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#working-with-spatial-data-in-r",
    "href": "mapping.html#working-with-spatial-data-in-r",
    "title": "Introduction to GIS in R",
    "section": "Working with spatial data in R",
    "text": "Working with spatial data in R\n\nOpen Street Map Data\nThroughout this tutorial you will be using data about Kenyan rainfall - Kenyan Rainfall Data. It covers total rainfall at the subcounty level in the last 5 years, from 2021-01-01. The data is updated weekly and includes a range of variables capturing amount of rain.\nWe want to visualise, and better understand how much rain has fallen in the last 5 years at the sub-county and county level, and what the distribution is at the MSOA level of geography. To achieve this we will have to import spatial data, manipulate it, create summary statistics, and then plot it.",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#loading-spatial-and-non-spatial-data",
    "href": "mapping.html#loading-spatial-and-non-spatial-data",
    "title": "Introduction to GIS in R",
    "section": "Loading spatial and non-spatial data",
    "text": "Loading spatial and non-spatial data\nRainfall data has been tidied up and saved as a Comma Separated Value file (.csv). We can use read_csv to open it in R.\n\nExercise\n −+ 10:00 \n\nCreate a new object called poi by using read_csv(). Load data located in “https://data.humdata.org/dataset/ken-rainfall-subnational”.\nUse glimpse() or head() to view health structure.\n\n\n\n5.2.2. Solution\n\n\n\n\n\n5.2.2. Solution\n\n\n\npoi &lt;- readr::read_csv(\"/Users/christophersteinberg/Documents/GitHub/intro_R-main/data/poi.csv\")\n\nRows: 34182 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): name, amenity\ndbl (2): easting, northing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoi\n\n# A tibble: 34,182 × 4\n   name                                easting northing amenity         \n   &lt;chr&gt;                                 &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;           \n 1 The Salvation Army_Machakos citadel    37.3   -1.53  place_of_worship\n 2 Masjid Umar bin Khattab                40.1   -3.25  place_of_worship\n 3 Hotel and Pub                          35.9    4.06  pub             \n 4 Rosoga Dispensary                      35.9    0.103 clinic          \n 5 AIC Rosoga Church                      35.9    0.102 place_of_worship\n 6 FGCK Kimngorom                         35.8    0.165 place_of_worship\n 7 Schools of Excellence Primary          36.6   -0.661 school          \n 8 Greenview Secondary School             36.6   -0.869 school          \n 9 Karati Secondary School                36.6   -0.733 school          \n10 Sint Paul Secondary School             36.6   -0.701 school          \n# ℹ 34,172 more rows\n\n\n\n\npoi is currently just a data frame - it has not got an explicit geometry column which links observations to their geographic location. It does however contain several columns which can be used to convert it into a spatial data format.\nWard_code column references the GSS codes of wards within which the observations fall. GSS codes can be used to join lfb data to boundaries from the Open Geography Portal. One issue with this particular column is that it does not indicate the currency of GSS codes. Wards are subject to frequent change, and as such it is best practice to be clear about the dates of any boundaries used by stating the exact code used, e.g. wd19cd. Because LFB data does not include this information we have no guarantee that the boundaries and GSS codes we join will match.\nFortunately we have also been provided with columns recording the easting, and northing of each incident. We can use those to convert lfb into an sf object. To achieve this we will use the st_as_sf() function which takes the following arguments:\n\nnew_object &lt;- st_as_sf(x = input_data_frame, coords = c(\"x_coordinate_column\", \"y_coordinate_column\"), crs = 27700)\n\n\n\nExercise\n −+ 10:00 \n\nCreate a new object called poi_sf by converting poi using the st_as_sf() function.\nUse glimpse() or head() to view poi_sf structure.\n\n\n\n5.2.2. Solution\n\n\n\npoi_sf &lt;- sf::st_as_sf(x = poi, coords = c(\"easting\", \"northing\"), crs = 4326)\nhead(poi_sf)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 35.81531 ymin: -3.246071 xmax: 40.08169 ymax: 4.061832\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  name                                amenity                      geometry\n  &lt;chr&gt;                               &lt;chr&gt;                     &lt;POINT [°]&gt;\n1 The Salvation Army_Machakos citadel place_of_worship (37.26376 -1.534213)\n2 Masjid Umar bin Khattab             place_of_worship (40.08169 -3.246071)\n3 Hotel and Pub                       pub               (35.88242 4.061832)\n4 Rosoga Dispensary                   clinic            (35.8573 0.1026951)\n5 AIC Rosoga Church                   place_of_worship (35.85683 0.1024965)\n6 FGCK Kimngorom                      place_of_worship (35.81531 0.1654002)\n\n\n\n\nst_as_sf() converted the easting and northing columns to simple feature geometries and created a new column called geometry which holds spatial information for each row. Now that poi is a spatial object we can plot it using the ggplot2 package. For now we will use the geom_sf() function which creates a quick map, using ggplot2's default settings. geom_sf() only needs to be supplied with a simple feature object and is very useful for quickly inspecting your data. To quickly plot multiple layers on the same map use geom_sf() + geom_sf().\n\n\nExercise\n −+ 03:00 \n\nPlot poi_sf using the geom_sf() function.\n\n\n\n5.2.2. Solution\n\n\n\nggplot2::ggplot(poi_sf) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nWe can also create interactive maps using plotly or leaflet. For plotly we ca write the map using ggplot2, before executing ggplotly().\n\n\nExercise\n −+ 03:00 \n\nMake an interactive map of poi_sf using the ggplotly() function.\n\n\n\n5.2.2. Solution\n\n\n\nggplot_graph &lt;- ggplot2::ggplot(poi_sf) + \n  ggplot2::geom_sf() \n\nplotly::ggplotly(ggplot_graph)",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#filtering-by-administration-code",
    "href": "mapping.html#filtering-by-administration-code",
    "title": "Introduction to GIS in R",
    "section": "Filtering by Administration Code",
    "text": "Filtering by Administration Code\nThis dataset covers all of Kenya at subnational administrative boundaries. Let’s look at solely locations in Nairobi to narrow down our exploration more. To remove all points outside of Nairobi we will have to import a shapefile with the right county level and then use it to spatially filter our poi_df data.\nSo far we have created our own sf objects by adding a geometry column. The kenya data set is already a spatial one and as such we can use the st_read() function from the sf package to import it. st_read isextremely versatile and able to import most spatial data formats into R. The only argument that needs to be supplied to st_read is the fullpath to the UTLA file\n\nExercise\n −+ 10:00 \n\nUse st_read() to load the kenya sub-county boundaries you downloaded at the beginning of the tutorial, askenya_2017.\nUTLA path - https://data.humdata.org/dataset/cod-ab-ken\nMake a static map of the object you have just created using geom_sf().\n\n\n\n5.2.2. Solution\n\n\n\nkenya_2017 &lt;- sf::st_read(\"/Users/christophersteinberg/Documents/GitHub/admn2/ken_admbnda_adm2_iebc_20191031.shp\", quiet = TRUE)\nggplot2::ggplot(kenya_2017) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nSub-county level boundaries have loaded correctly but they currently cover all of Kenya. Because simple feature objects are data frames with a geometry column attached, any operations that we would perform on a normal data frame can also be performed on an object of class sf. We will use dplyr::filter and stringr::str_detect() to only keep UTLAs where their ADMN_PCODE code starts with “KE47”. “KE47” denotes that an sub-county is part of Nairobi county.\n\n\nExercise\n −+ 10:00 \n\nInspect kenya_2017 using head() or glimpse(), and identify which column holds the GSS codes - it should end in “cd”.\nCreate a new object called nairobi_subcounty. Use dplyr::filter alongside stringr::str_detect() to only keep observations which have a Administrative code starting with “KE47”.\nPlot nairobi_subcounty to see if the results look correct.\n\n\n\n5.2.2. Solution\n\n\n\nhead(kenya_2017)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 34.04426 ymin: -1.016204 xmax: 36.24609 ymax: 0.5589468\nGeodetic CRS:  WGS 84\n  Shape_Leng Shape_Area      ADM2_EN ADM2_PCODE ADM2_REF ADM2ALT1EN ADM2ALT2EN\n1  1.7469864 0.04082931     Ainabkoi   KE027144     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n2  0.9173066 0.01995665      Ainamoi   KE035190     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n3  1.4026374 0.03799999        Aldai   KE029152     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n4  1.0813543 0.04935731 Alego Usonga   KE041234     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n5  0.7439150 0.02136547       Awendo   KE044254     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n6  0.7970553 0.02423416       Bahati   KE032174     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n      ADM1_EN ADM1_PCODE ADM0_EN ADM0_PCODE       date    validOn    ValidTo\n1 Uasin Gishu      KE027   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n2     Kericho      KE035   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n3       Nandi      KE029   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n4       Siaya      KE041   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n5      Migori      KE044   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n6      Nakuru      KE032   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n                        geometry\n1 MULTIPOLYGON (((35.35933 0....\n2 MULTIPOLYGON (((35.26262 -0...\n3 MULTIPOLYGON (((34.93989 0....\n4 MULTIPOLYGON (((34.20727 0....\n5 MULTIPOLYGON (((34.54577 -0...\n6 MULTIPOLYGON (((36.20478 -0...\n\n\n\nnairobi_subcounty &lt;- dplyr::filter(kenya_2017, stringr::str_detect(ADM2_PCODE, \"KE047\"))\nggplot2::ggplot(nairobi_subcounty) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nFinally, for the next step, we only need the outer boundary of Nairobi - all the internal subcounty boundaries have to be removed and only the outer edges kept. sf has a function exactly for this purpose called st_union(). It only takes one argument, which is the sf object we want to merge.\n\n\nExercise\n −+ 10:00 \n\nCreate a new object called nairobi_boundary using the st_union function.\nPlot it to check the results.\n\n\n\n5.2.2. Solution\n\n\n\nnairobi_boundary &lt;- sf::st_union(nairobi_subcounty)\nggplot2::ggplot(nairobi_boundary) + \n  ggplot2::geom_sf()",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "curriculum.html",
    "href": "curriculum.html",
    "title": "1  KNBS R Programming Curriculum",
    "section": "",
    "text": "1.1 Curriculum Overview\nThis comprehensive curriculum is designed to take KNBS employees from novice R users to advanced practitioners, equipping them with the skills necessary to perform sophisticated data analysis and programming tasks. The curriculum is structured into three distinct levels - Beginner, Intermediate, and Advanced - each building upon the knowledge gained in the previous level.\nThe journey begins with the foundational “Introduction to R” course, which provides non-R users with the essential skills to perform basic work tasks in R. This is followed by a series of courses that broaden the base knowledge of R and introduce complementary languages and tools such as SQL, Bash, and Git. These courses are designed to give analysts a well-rounded skill set that is crucial for most data analysis tasks in R.\nAs learners progress, the Intermediate level courses delve into more specific and advanced topics. These courses introduce various analytical methods and best practices in coding for automation. They are designed to enhance the analysts’ capabilities in handling complex data tasks and improving code efficiency and reproducibility.\nThe Advanced level courses introduce cutting-edge techniques and tools in data science and software development. These courses cover topics such as machine learning, natural language processing, big data processing with Sparklyr, and modern software development practices like continuous integration.\ngantt\n    title KNBS R Curriculum\n    dateFormat  X\n    axisFormat %H\n    \n    section Beginner\n    Introduction to R           :a1, 0, 16h\n    Introduction to Data Visualisation :a2, after a1 start, 16h\n    Optional Modules            :a3, after a2, 4h\n    Introduction to GIS in R    :a4, after a2, 3h\n    R Control Flow Loops and Functions :a5, after a2, 3h\n\n    section Intermediate Level\n    Optional Modules            :b1, after a5, 25h\n    More GIS in R               :b3, after a5, 3h\n    Introduction to Git         :b4, after a5, 4h\n    Statistics in R             :b5, after a5, 16h\n    \n    section Advanced\n    Introduction to NLP in R    :c1, after b5, 16h\n    Introduction to Machine Learning in R :c2, after b5, 24h\n    Additional Analysis Modules: c3, after c2, 38h\n    Packaging and Documentation :c4, after b5, 4h\n    Additional Developer Modules: c5, after c4, 6h",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#beginner-level",
    "href": "curriculum.html#beginner-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.1 Beginner Level",
    "text": "2.1 Beginner Level\n\n2.1.1 Introduction to R\nThis redesigned course focuses on applying skills and building confidence in R programming. Participants will learn about data types, importing data, and working with DataFrames through hands-on exercises. The course is structured with a week between sessions to allow for practice and assimilation of learning, making it ideal for beginners to develop independence and resilience in their R programming journey.\n\nData Types\nImporting Data\nDataFrames, Manipulation, and Cleaning\n\nCourse Length: 2 Days\nPrerequisites: None\n\n\n2.1.2 Introduction to Data Visualisation\nData Visualisation is the art of displaying data in a clear and understandable way that allows for maximum impact. This comprehensive course covers both theoretical and practical aspects of data visualization in R. Participants will learn best practices for presenting data clearly and professionally. The course then transitions into practical application, teaching how to produce production-ready visualizations using R. By the end, students will have a solid foundation in creating impactful and consistent data visualizations.\nCourse Length: 2 Days\nPrerequisites: Introduction to R\n\n\n2.1.3 Introduction to GIS in R\nThis course introduces the fundamentals of working with geospatial data in R. Participants will learn to load spatial data, manipulate spatial objects, and create both static and interactive maps. The course covers important concepts such as GSS codes, spatial joins, and coordinate reference systems. By the end, students will be able to perform basic spatial analysis and create visually appealing maps using R.\nCourse Length: 2-3 hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.4 R Control Flow Loops and Functions\nThis course focuses on more advanced programming concepts in R, specifically loops, control flow, and function writing. Participants will learn how to use these tools to reduce code repetition, improve readability, and follow good coding practices. The course includes practical examples and exercises to reinforce learning, and provides guidance on areas for additional study to further enhance programming skills.\nCourse Length: 3 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5 Optional Modules\n\n2.1.5.1 Introduction to RAP\nThis course introduces the concepts, motivation, and techniques for creating a Reproducible Analytical Pipeline (RAP). Participants will learn about the benefits of RAP and how to overcome common barriers in implementation. The course is tailored for government analysts and managers of analysis, providing a foundation for enhancing reproducibility and efficiency in analytical workflows.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.2 Introduction to RMarkdown\nThis course provides a comprehensive introduction to RMarkdown, a powerful tool for creating dynamic, reproducible documents in R. Participants will learn how to set up new documents, create various types of content, and output to different formats including HTML, PDF, and Word. The course covers text formatting, tables, images, and RMarkdown-specific features, equipping students with the skills to create professional, data-driven reports.\nCourse Length: 1-2 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.3 Best Practice in Programming - Clean Code\nThis workshop emphasizes best practices in programming for improved code readability and collaboration. Participants will learn general principles that enhance coding skills and make code more accessible to others. The course covers techniques for writing clean, maintainable code and encourages the development of good coding habits. By the end, students will have a solid foundation in creating code that is easy to read, understand, and modify.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#intermediate-level",
    "href": "curriculum.html#intermediate-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.2 Intermediate Level",
    "text": "2.2 Intermediate Level\n\n2.2.1 Introduction to Git\nThis course provides a comprehensive understanding of the Git version control system. Participants will gain hands-on experience working both locally and collaboratively with Git. The course covers the fundamentals of Git, its practical applications, and the benefits of using it for individual and team projects. By the end, students will be equipped with the skills to effectively use Git for version control and collaboration in their development workflows.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.2 Statistics in R\nThis comprehensive course covers both statistical theory and its practical application in R. Participants will learn about exploratory data analysis, various statistical tests, linear regression, model adequacy and selection, and generalized linear models. The course provides a balanced mix of theoretical understanding and hands-on coding experience. By the end, students will be equipped to perform advanced statistical analyses and interpret results using R.\nCourse Length: 16 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.3 More GIS in R\nBuilding on the introductory GIS course, this advanced session covers more complex geospatial operations and analysis techniques in R. Participants will learn about buffers, intersections, area summary statistics, and network analysis. The course also focuses on troubleshooting common error messages and improving the accuracy of spatial analyses. By the end, students will be able to conduct and present comprehensive spatial analyses using R.\nCourse Length: 2-3 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4 Optional Modules\n\n2.2.4.1 Foundations of SQL\nThis course provides a comprehensive introduction to SQL, covering syntax applicable to various database systems. Using an online platform with SQLite, participants will learn through hands-on exercises. The course covers basic SQL queries, table manipulation, joining tables, and database alterations. By the end, students will have a solid foundation in SQL, enabling them to work with databases effectively in their data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.2 Command Line Basics\nThis course introduces the powerful world of command line interfaces for both UNIX and Windows systems. Participants will learn basic commands and understand how to navigate file systems and execute operations using the command line. The course aims to make participants comfortable using this essential tool in their work, enhancing their ability to interact with computers efficiently and perform advanced operations.\nCourse Length: 2 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.3 Reproducible Reporting with RMarkdown\nThis course delves into advanced features of RMarkdown for creating reproducible reports. Participants will learn to embed executable code and data into reports, work with YAML headers and theme options, and use parameters for dynamic reporting. The course also covers markdown syntax, code chunks, and chunk options. By the end, students will be able to create professional, reproducible reports that seamlessly integrate code, data, and narrative.\nCourse Length: 2 Hours\nPrerequisites: Introduction to RMarkdown, Introduction to RAP\n\n\n2.2.4.4 Modular Programming in R\nThis course focuses on the principles of modular design in R programming. Participants will learn the importance of well-structured, reproducible code and how to implement modular design principles. The course covers techniques for converting code into functions and modules, improving code organization and reusability. By the end, students will be able to write more efficient, maintainable, and scalable R code.\nCourse Length: 4-5 hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.5 Hypothesis Testing in R\nThis intermediate course focuses on advanced concepts in hypothesis testing using R. Participants will learn to define and calculate Type I and Type II errors, determine effect sizes, and calculate statistical power. The course also covers sample size calculations for various statistical tests. By the end, students will have a deeper understanding of the nuances of hypothesis testing and be able to design more robust statistical analyses in R.\nCourse Length: 4-6 Hours\nPrerequisites: Statistics in R\n\n\n2.2.4.6 Dates and Times in R\nThis course provides a comprehensive overview of handling date and time data in R. Participants will learn how to create, convert, and manipulate date-time objects, understanding the underlying storage mechanisms. The course covers practical operations such as extracting parts of dates, performing date-time arithmetic, and working with time zones. By the end, students will be proficient in managing and analyzing time-based data in R.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#advanced-level",
    "href": "curriculum.html#advanced-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.3 Advanced Level",
    "text": "2.3 Advanced Level\n\n2.3.1 Introduction to NLP in R\nThis course provides a comprehensive introduction to Natural Language Processing (NLP) using R. Participants will learn fundamental NLP concepts and techniques, including text cleaning, exploratory analysis, and feature engineering. The course covers practical applications such as sentiment analysis on real datasets. By the end, students will have the skills to preprocess text data, extract meaningful features, and perform basic NLP tasks in R.\nCourse Length: 2 days\nPrerequisites: Statistics in R\n\n\n2.3.2 Introduction to Machine Learning in R\nThis course introduces the fundamentals of machine learning using R’s state-of-the-art “mlr3” package. Participants will learn about classification, regression, and cluster analysis experiments. The course covers the entire machine learning workflow, from data preparation to model evaluation and interpretation. By the end, students will have hands-on experience implementing various machine learning algorithms and understand how to apply them to real-world problems.\nCourse Length: 3 days\nPrerequisites: Statistics in R\n\n\n2.3.3 Packaging and Documentation\nThis course guides participants through the process of building and sharing R packages. Students will learn how to create custom functions, organize them into a package structure, and document their code effectively. The course covers best practices in package development, including version control and collaboration. By the end, participants will be able to create their own R packages, enhancing code reusability and facilitating collaboration with other R users.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions, Best Practice in Programming - Clean Code, Introduction to Git\n\n\n2.3.4 Additional Analysis Modules\n\n2.3.4.1 Quality Assurance of Predictive Modelling\nThis course explores critical quality issues in statistical modeling and machine learning for prediction. Participants will learn best practices for model design, validation, and usage across various industries. The course covers topics such as model interpretability, bias detection, and robustness testing. By the end, students will be equipped with the knowledge to ensure the reliability and effectiveness of their predictive models.\nCourse Length: 6-8 hours\nPrerequisites: Statistics in R\n\n\n2.3.4.2 Introduction to Sparklyr\nThis course introduces Sparklyr, the R interface to Apache Spark for big data processing. Participants will learn how to handle and analyze massive datasets that exceed the capabilities of traditional R. The course covers data manipulation, querying, and processing techniques specific to Sparklyr. By the end, students will be able to leverage the power of distributed computing for their large-scale data analysis projects in R.\nCourse Length: 2 days\nPrerequisites: Introduction to Machine Learning in R\n\n\n2.3.4.3 Introduction to Bayesian Data Analysis\nThis course provides a foundational understanding of Bayesian Data Analysis and its implementation in R. Participants will explore the principles of Bayesian statistics, including prior and posterior distributions, and Markov Chain Monte Carlo (MCMC) methods. The course includes hands-on examples of Bayesian analysis in R, covering model fitting and interpretation. By the end, students will be able to apply Bayesian techniques to their own data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: Statistics in R\n\n\n\n2.3.5 Additional Developer Modules\n\n2.3.5.1 Introduction to Unit Testing\nThis course focuses on the principles and implementation of unit testing in R for ensuring code quality. Participants will learn how to design, create, and execute tests for their R code. The course covers both the theoretical aspects of software development and practical implementation in R. By the end, students will be able to implement robust testing strategies in their R projects, improving code reliability and maintainability.\nCourse Length: 4 Hours\nPrerequisites: Packaging and Documentation\n\n\n2.3.5.2 Introduction to Continuous Integration\nThis course introduces modern software development approaches, focusing on Continuous Integration (CI) concepts. Participants will learn about DevOps and CI/CD philosophies and how they contribute to efficient software development. The course covers tools and practices that automate testing and deployment processes. While not hands-on, this course provides essential background knowledge for advanced technical courses on building modern development infrastructures. Course Length: 2 Hours\nPrerequisites: Packaging and Documentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Session aims\nIn this course, you will explore the versatility of R, a powerful language for statistical computing and graphics. You will discover the benefits of using R and get started with the basics, and fain confidence with the user-friendly R Studio interface and learn fundamental R concepts. You will also dive into the Tidyverse, a collection of packages for data storage, visualisation and manipulation. This course offers a solid foundation to kickstart your journey with R!\nThis book is designed to accompany the Introduction to R training that is starting at KNBS. To complete this course, you will need to have R Studio installed on your computer.\nIf you’re running through this book solo, it is recommended to run through it in order and try out all the of the exercises as you go through. Each exercise has a Solution dropdown, which allows you to view prompts to help with the question and see the answers.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#session-aims",
    "href": "intro.html#session-aims",
    "title": "Introduction to R",
    "section": "",
    "text": "navigate the R and R Studio environment\nunderstand and use the common R functions for data manipulation\nunderstand the basics of data visualisation using the ggplot2 package\nunderstand the term tidy data and why it is important for writing efficient code",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "Introduction to R",
    "section": "What is R?",
    "text": "What is R?\nR is an open-source programming language and software environment, designed primarily for statistical computing. It has a long history - it is based on the S language, which was developed in 1976 in Bell Labs, where the UNIX operating system and the C and C++ languages were developed. The R language itself was developed in the 1990s, with the first stable version release in 2000.\nR has grown rapidly in popularity particularly in the last five years, due to the increased interest in the data science field. It is now a key tool used by analysts in governments globally.\nSome of the advantages:\n\nIt is popular - there is a large, active and rapidly growing community of R programmers, which has resulted in a plethora of resources and extensions.\nIt is powerful - the history as a statistical language means it is well suited for data analysis and manipulation.\nIt is extensible - there are a vast array of packages that can be added to extend the functionality of R, produced by statisticians and programmers around the world. These can range from obscure statistical techniques to tools for making interactive charts.\nIt’s free and open source - a large part of its popularity can be owed to its low cost, particularly relative to proprietary software such as SAS or STATA.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#introducing-rstudio",
    "href": "intro.html#introducing-rstudio",
    "title": "Introduction to R",
    "section": "Introducing RStudio",
    "text": "Introducing RStudio\nRStudio is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with. RStudio has four main panels;\n\nScript Editor (top left) - used to write and save your code, which is only run when you explicitly tell RStudio to do so.\nConsole (bottom left) - all code is run through the console, even the code you write in the script editor is sent to the console to be run. It’s perfect for quickly viewing data structures and help for functions but should not be used to write code you want to save (that should be done in the script editor).\nEnvironment (top right) - all data, objects and functions that you have read in/created will appear here.\nFiles/Plots/Help (bottom right) - this pane groups a few miscellaneous areas of RStudio.\n\nFiles acts like the windows folder to navigate between files and folders.\nPlots shows any graphs that you generate.\nPackages let’s you install and manage packages currently in use.\nHelp provides information about packages or functions, including how to use them.\nViewer is essentially RStudio’s built-in browser, which can be used for web app development.\n\n\nYou may have noticed that your Script Editor is bigger than the Console or your Environment has suddenly disappeared. In RStudio, you can adjust the size of different panes by clicking and dragging the dividers between them. If you want to maximize a specific pane, such as the Script Editor, use the shortcut Ctrl + Shift + 1 (Windows/Linux) or Cmd + Shift + 1 (Mac) to focus on it. To restore the default layout, press Ctrl + Shift + 0 (Windows/Linux) or Cmd + Shift + 0 (Mac). You can also use the View menu to toggle different panes on and off, ensuring your workspace suits your needs.\nIf you find the text difficult to read or prefer a different appearance, you can customise the theme, font, and text size in RStudio. Go to Tools &gt; Global Options &gt; Appearance, where you can choose from different editor themes (e.g., light or dark mode), adjust the font type, and increase or decrease the text size for better readability. These changes can help make coding more comfortable, especially during long sessions.\n\nRecommended Changes\nWhile not necessary, certain changes are almost always recommended for visibility reasons. These include:\n\nChoosing a different theme, as Textmate can be hard on the eyes. This can be done in Tools &gt; Global Options &gt; Appearance &gt; Editor theme:.\nHighlight R function calls. This makes functions look a different colour than normal text, which can make reading your code much easier. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Highlight R function calls.\nUse Rainbow Parenthesis. This makes each pair of () in a line a different colour, which can help you catch if you’re missing one and it’s breaking your code. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Use rainbow parenthesis.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#basic-syntax",
    "href": "intro.html#basic-syntax",
    "title": "Introduction to R",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nExercise\n\n\n\n−+\n03:00\n\n\n\nAs a quick exercise, try out some arithmetic in your console:\n\n25 * 15\n(45 + 3) ^ 2\n78 / 4\n\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n\nSolution\n25 * 15\n\n\n[1] 375\n\n\nSolution\n(45 + 3) ^ 2\n\n\n[1] 2304\n\n\nSolution\n78 / 4\n\n\n[1] 19.5\n\n\n\n\nThe assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\nSolution\nx1 &lt;- 14\nx1\n\n\n[1] 14\n\n\n\n\nSolution\nx2 &lt;- x1 + 7\nx2\n\n\n[1] 21\n\n\n\n\nSolution\nx3 &lt;- x2 / 3\nx3\n\n\n[1] 7\n\n\n\n\nCombining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n\n[1] 1 2 3\n\n\nSolution\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSolution\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x121bae720&gt;\n&lt;environment: namespace:lubridate&gt;\n\n\nSolution\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n\n[1] 36",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "Introduction to R",
    "section": "Packages",
    "text": "Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#the-tidyverse",
    "href": "intro.html#the-tidyverse",
    "title": "Introduction to R",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\nExercise\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\nlibrary(tidyverse)",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "application_1.html",
    "href": "application_1.html",
    "title": "7  Application 1: Data Manipulation",
    "section": "",
    "text": "7.1 A real world example\nIn this application, we will practice some of the coding skills learned in the training so far. Whereas the examples used in the textbook use toy datasets which are already clean and well-formatted, the datasets used in our applications may require some initial cleaning prior to analysis. This is likely closer to tasks you might encounter in your everyday work at KNBS.\nIn this particular application, we would like you to produce 3 simple pieces of analysis using Kenya’s 2019 census data:\nThere are two stages to this task. The first is to read in the data and prepare it to be analysed. This will involve reading the data in properly, fixing any column name issues, reducing the size of the dataset if it is too large, and finally dealing with any missing values, or NAs, that are found in the data.\nThe second is to perform the analysis on your dataset. This will involve creating new columns, filtering out certain data, and creating summaries.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Application 1: Data Manipulation</span>"
    ]
  },
  {
    "objectID": "application_1.html#application-for-knbs",
    "href": "application_1.html#application-for-knbs",
    "title": "3  KNBS Application",
    "section": "",
    "text": "Read in the dataset and use janitor to clean any column names\nWe need columns relating to migration, age, etc. Select the columns we will need - look at survey metadata\nLet’s rename some columns to make them easier to understand\nFor our religion question, let’s examine the data in this column\nHow should we approach the summary? How do we want to treat NAs, or DK\nRead the dataset using read_csv and clean the column names using janitor::clean_names.\nSelect the columns required for the religion analysis (e.g., p17 for religion) using select.\nRename the selected religion column to be more descriptive (e.g., religion) using rename.\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(count = n()).\nAddress how to handle any NA or “Don’t Know” responses during this summary.\nSelect the columns required for the migration analysis (e.g., county, p19 for birthplace) using select. Rename the birthplace column (e.g., birthplace) using rename.\nCreate a new binary column named migrant using mutate and if_else to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).\nSelect the columns required for the working hours analysis (e.g., ea_type for rural/urban, p11 for sex, p12 for age, p52 for hours worked) using select.\nRename the selected columns for clarity (e.g., age, sex, hours_worked_if_work) using rename. Filter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and if_else.\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNBS Application</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum-overview",
    "href": "index.html#curriculum-overview",
    "title": "KNBS: Introduction to R",
    "section": "1.1 Curriculum Overview",
    "text": "1.1 Curriculum Overview\nThe journey begins with the foundational “Introduction to R” course, which provides non-R users with the essential skills to perform basic work tasks in R. This is followed by a series of courses that broaden the base knowledge of R and introduce complementary languages and tools such as SQL, Bash, and Git. These courses are designed to give analysts a well-rounded skill set that is crucial for most data analysis tasks in R.\nAs learners progress, the Intermediate level courses delve into more specific and advanced topics. These courses introduce various analytical methods and best practices in coding for automation. They are designed to enhance the analysts’ capabilities in handling complex data tasks and improving code efficiency and reproducibility.\nThe Advanced level courses introduce cutting-edge techniques and tools in data science and software development. These courses cover topics such as machine learning, natural language processing, big data processing with Sparklyr, and modern software development practices like continuous integration.\n\n\n\n\n\ngantt\n    title KNBS R Curriculum\n    dateFormat  X\n    axisFormat %H\n    \n    section Beginner\n    Introduction to R           :a1, 0, 16h\n    Introduction to Data Visualisation :a2, after a1 start, 16h\n    Optional Modules            :a3, after a2, 4h\n    Introduction to GIS in R    :a4, after a2, 3h\n    R Control Flow Loops and Functions :a5, after a2, 3h\n\n    section Intermediate Level\n    Optional Modules            :b1, after a5, 25h\n    More GIS in R               :b3, after a5, 3h\n    Introduction to Git         :b4, after a5, 4h\n    Statistics in R             :b5, after a5, 16h\n    \n    section Advanced\n    Introduction to NLP in R    :c1, after b5, 16h\n    Introduction to Machine Learning in R :c2, after b5, 24h\n    Additional Analysis Modules: c3, after c2, 38h\n    Packaging and Documentation :c4, after b5, 4h\n    Additional Developer Modules: c5, after c4, 6h",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#beginner-level",
    "href": "index.html#beginner-level",
    "title": "KNBS: Introduction to R",
    "section": "2.1 Beginner Level",
    "text": "2.1 Beginner Level\n\n2.1.1 Introduction to R\nThis redesigned course focuses on applying skills and building confidence in R programming. Participants will learn about data types, importing data, and working with DataFrames through hands-on exercises. The course is structured with a week between sessions to allow for practice and assimilation of learning, making it ideal for beginners to develop independence and resilience in their R programming journey.\n\nData Types\nImporting Data\nDataFrames, Manipulation, and Cleaning\n\nCourse Length: 2 Days\nPrerequisites: None\n\n\n2.1.2 Introduction to Data Visualisation\nData Visualisation is the art of displaying data in a clear and understandable way that allows for maximum impact. This comprehensive course covers both theoretical and practical aspects of data visualization in R. Participants will learn best practices for presenting data clearly and professionally. The course then transitions into practical application, teaching how to produce production-ready visualizations using R. By the end, students will have a solid foundation in creating impactful and consistent data visualizations.\nCourse Length: 2 Days\nPrerequisites: Introduction to R\n\n\n2.1.3 Introduction to GIS in R\nThis course introduces the fundamentals of working with geospatial data in R. Participants will learn to load spatial data, manipulate spatial objects, and create both static and interactive maps. The course covers important concepts such as spatial joins, and coordinate reference systems. By the end, students will be able to perform basic spatial analysis and create visually appealing maps using R.\nCourse Length: 2-3 hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.4 R Control Flow Loops and Functions\nThis course focuses on more advanced programming concepts in R, specifically loops, control flow, and function writing. Participants will learn how to use these tools to reduce code repetition, improve readability, and follow good coding practices. The course includes practical examples and exercises to reinforce learning, and provides guidance on areas for additional study to further enhance programming skills.\nCourse Length: 3 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5 Optional Modules\n\n2.1.5.1 Introduction to RAP\nThis course introduces the concepts, motivation, and techniques for creating a Reproducible Analytical Pipeline (RAP). Participants will learn about the benefits of RAP and how to overcome common barriers in implementation. The course is tailored for government analysts and managers of analysis, providing a foundation for enhancing reproducibility and efficiency in analytical workflows.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.2 Introduction to RMarkdown\nThis course provides a comprehensive introduction to RMarkdown, a powerful tool for creating dynamic, reproducible documents in R. Participants will learn how to set up new documents, create various types of content, and output to different formats including HTML, PDF, and Word. The course covers text formatting, tables, images, and RMarkdown-specific features, equipping students with the skills to create professional, data-driven reports.\nCourse Length: 1-2 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.3 Best Practice in Programming - Clean Code\nThis workshop emphasizes best practices in programming for improved code readability and collaboration. Participants will learn general principles that enhance coding skills and make code more accessible to others. The course covers techniques for writing clean, maintainable code and encourages the development of good coding habits. By the end, students will have a solid foundation in creating code that is easy to read, understand, and modify.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#intermediate-level",
    "href": "index.html#intermediate-level",
    "title": "KNBS: Introduction to R",
    "section": "2.2 Intermediate Level",
    "text": "2.2 Intermediate Level\n\n2.2.1 Introduction to Git\nThis course provides a comprehensive understanding of the Git version control system. Participants will gain hands-on experience working both locally and collaboratively with Git. The course covers the fundamentals of Git, its practical applications, and the benefits of using it for individual and team projects. By the end, students will be equipped with the skills to effectively use Git for version control and collaboration in their development workflows.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.2 Statistics in R\nThis comprehensive course covers both statistical theory and its practical application in R. Participants will learn about exploratory data analysis, various statistical tests, linear regression, model adequacy and selection, and generalized linear models. The course provides a balanced mix of theoretical understanding and hands-on coding experience. By the end, students will be equipped to perform advanced statistical analyses and interpret results using R.\nCourse Length: 16 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.3 More GIS in R\nBuilding on the introductory GIS course, this advanced session covers more complex geospatial operations and analysis techniques in R. Participants will learn about buffers, intersections, area summary statistics, and network analysis. The course also focuses on troubleshooting common error messages and improving the accuracy of spatial analyses. By the end, students will be able to conduct and present comprehensive spatial analyses using R.\nCourse Length: 2-3 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4 Optional Modules\n\n2.2.4.1 Foundations of SQL\nThis course provides a comprehensive introduction to SQL, covering syntax applicable to various database systems. Using an online platform with SQLite, participants will learn through hands-on exercises. The course covers basic SQL queries, table manipulation, joining tables, and database alterations. By the end, students will have a solid foundation in SQL, enabling them to work with databases effectively in their data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.2 Command Line Basics\nThis course introduces the powerful world of command line interfaces for both UNIX and Windows systems. Participants will learn basic commands and understand how to navigate file systems and execute operations using the command line. The course aims to make participants comfortable using this essential tool in their work, enhancing their ability to interact with computers efficiently and perform advanced operations.\nCourse Length: 2 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.3 Reproducible Reporting with RMarkdown\nThis course delves into advanced features of RMarkdown for creating reproducible reports. Participants will learn to embed executable code and data into reports, work with YAML headers and theme options, and use parameters for dynamic reporting. The course also covers markdown syntax, code chunks, and chunk options. By the end, students will be able to create professional, reproducible reports that seamlessly integrate code, data, and narrative.\nCourse Length: 2 Hours\nPrerequisites: Introduction to RMarkdown, Introduction to RAP\n\n\n2.2.4.4 Modular Programming in R\nThis course focuses on the principles of modular design in R programming. Participants will learn the importance of well-structured, reproducible code and how to implement modular design principles. The course covers techniques for converting code into functions and modules, improving code organization and reusability. By the end, students will be able to write more efficient, maintainable, and scalable R code.\nCourse Length: 4-5 hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.5 Hypothesis Testing in R\nThis intermediate course focuses on advanced concepts in hypothesis testing using R. Participants will learn to define and calculate Type I and Type II errors, determine effect sizes, and calculate statistical power. The course also covers sample size calculations for various statistical tests. By the end, students will have a deeper understanding of the nuances of hypothesis testing and be able to design more robust statistical analyses in R.\nCourse Length: 4-6 Hours\nPrerequisites: Statistics in R\n\n\n2.2.4.6 Dates and Times in R\nThis course provides a comprehensive overview of handling date and time data in R. Participants will learn how to create, convert, and manipulate date-time objects, understanding the underlying storage mechanisms. The course covers practical operations such as extracting parts of dates, performing date-time arithmetic, and working with time zones. By the end, students will be proficient in managing and analyzing time-based data in R.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#advanced-level",
    "href": "index.html#advanced-level",
    "title": "KNBS: Introduction to R",
    "section": "2.3 Advanced Level",
    "text": "2.3 Advanced Level\n\n2.3.1 Introduction to NLP in R\nThis course provides a comprehensive introduction to Natural Language Processing (NLP) using R. Participants will learn fundamental NLP concepts and techniques, including text cleaning, exploratory analysis, and feature engineering. The course covers practical applications such as sentiment analysis on real datasets. By the end, students will have the skills to preprocess text data, extract meaningful features, and perform basic NLP tasks in R.\nCourse Length: 2 days\nPrerequisites: Statistics in R\n\n\n2.3.2 Introduction to Machine Learning in R\nThis course introduces the fundamentals of machine learning using R’s state-of-the-art “mlr3” package. Participants will learn about classification, regression, and cluster analysis experiments. The course covers the entire machine learning workflow, from data preparation to model evaluation and interpretation. By the end, students will have hands-on experience implementing various machine learning algorithms and understand how to apply them to real-world problems.\nCourse Length: 3 days\nPrerequisites: Statistics in R\n\n\n2.3.3 Packaging and Documentation\nThis course guides participants through the process of building and sharing R packages. Students will learn how to create custom functions, organize them into a package structure, and document their code effectively. The course covers best practices in package development, including version control and collaboration. By the end, participants will be able to create their own R packages, enhancing code reusability and facilitating collaboration with other R users.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions, Best Practice in Programming - Clean Code, Introduction to Git\n\n\n2.3.4 Additional Analysis Modules\n\n2.3.4.1 Quality Assurance of Predictive Modelling\nThis course explores critical quality issues in statistical modeling and machine learning for prediction. Participants will learn best practices for model design, validation, and usage across various industries. The course covers topics such as model interpretability, bias detection, and robustness testing. By the end, students will be equipped with the knowledge to ensure the reliability and effectiveness of their predictive models.\nCourse Length: 6-8 hours\nPrerequisites: Statistics in R\n\n\n2.3.4.2 Introduction to Sparklyr\nThis course introduces Sparklyr, the R interface to Apache Spark for big data processing. Participants will learn how to handle and analyze massive datasets that exceed the capabilities of traditional R. The course covers data manipulation, querying, and processing techniques specific to Sparklyr. By the end, students will be able to leverage the power of distributed computing for their large-scale data analysis projects in R.\nCourse Length: 2 days\nPrerequisites: Introduction to Machine Learning in R\n\n\n2.3.4.3 Introduction to Bayesian Data Analysis\nThis course provides a foundational understanding of Bayesian Data Analysis and its implementation in R. Participants will explore the principles of Bayesian statistics, including prior and posterior distributions, and Markov Chain Monte Carlo (MCMC) methods. The course includes hands-on examples of Bayesian analysis in R, covering model fitting and interpretation. By the end, students will be able to apply Bayesian techniques to their own data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: Statistics in R\n\n\n\n2.3.5 Additional Developer Modules\n\n2.3.5.1 Introduction to Unit Testing\nThis course focuses on the principles and implementation of unit testing in R for ensuring code quality. Participants will learn how to design, create, and execute tests for their R code. The course covers both the theoretical aspects of software development and practical implementation in R. By the end, students will be able to implement robust testing strategies in their R projects, improving code reliability and maintainability.\nCourse Length: 4 Hours\nPrerequisites: Packaging and Documentation\n\n\n2.3.5.2 Introduction to Continuous Integration\nThis course introduces modern software development approaches, focusing on Continuous Integration (CI) concepts. Participants will learn about DevOps and CI/CD philosophies and how they contribute to efficient software development. The course covers tools and practices that automate testing and deployment processes. While not hands-on, this course provides essential background knowledge for advanced technical courses on building modern development infrastructures. Course Length: 2 Hours\nPrerequisites: Packaging and Documentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "importing_data.html#column-names",
    "href": "importing_data.html#column-names",
    "title": "4  Importing Data",
    "section": "4.5 Column Names",
    "text": "4.5 Column Names\nIn the previous section we looked at reading data into R and also inspecting it. In this section we are going to look at our first steps once it’s read in.\nIn the previous session we stated that every column in a data frame is a variable and it is good practice to not have spaces within variable names, as spaces makes it harder for us to call on the variables when we need to use them.\nWhen you enter data in Excel, you most often don’t think too much about what you call each column. After all, you just label them once and as long as they are meaningful to you, what does it matter if the column name is a long combination of CAPITALLETTERS, lowercaseletters, and numbers?\nWhen you are working with variables in R though, you need to type the name of each variable, every time you want to work with it. So, it makes sense to make your column names as simple, but meaningful as possible.\n\nIdeally, they should also be consistently formatted.\n\nFor example if we wanted to pick the name of Passenger column from the Titanic dataset.\n\nTitanic$name Of Passenger\n\nTo get around this we enclose name of passenger with back ticks like the code below - this is the key above the tab key on the left hand side of your keyboard.\n\n# Selecting data using the $ symbol\n# note this now works because of the back ticks\nTitanic$`name Of Passenger`\n\nIf your column names have spaces and you don’t get rid of them, you must use backticks.\nHowever its good practise to remove spaces and symbols.\nWe can see the column names by using the names() function to access the name attribute of the data.\n\n# Getting the column names using the names function\n\nnames(Titanic)\n\nNULL\n\n\nAs we can see our column names have spaces and some start with capital letters and some with small letters, we can clean the names using the janitor package.\n\n4.5.1 Cleaning Column Names\nThe janitor package offers many functions used to manipulate data, for example removing empty rows and columns, finding duplicates within a data frame. In this session we will use the library to to clean our data set names.\nWe can clean the names of our dataset with the janitor::clean_names() function as shown below.\nWe are overwriting the original Titanic data frame with a version with the column names cleaned.\n\n# Cleaning the column names using the janitor\n# package and the clean_names() function.\n# This will put all names in lower case letters and \n# replace blank spaces with underscores.\n\ntitanic &lt;- janitor::clean_names(Titanic)\n\n# Getting the column names of the dataset\n\nnames(titanic)\n\nNULL\n\n\nclean_names() removes spaces, symbols, changes characters to lower case and makes all columns start with letters.\nThis is the default setting, there are many other options such as snake, lower_camel and all_caps. These can be put inside the clean_names() function as shown below:\n\n# Specifying the case within the clean_names function\n\njanitor::clean_names(Titanic, case = \"snake\")\n\n, , Age = child, Survived = no\n\n      Sex\nClass  male female\n  x1st    0      0\n  x2nd    0      0\n  x3rd   35     17\n  crew    0      0\n\n, , Age = adult, Survived = no\n\n      Sex\nClass  male female\n  x1st  118      4\n  x2nd  154     13\n  x3rd  387     89\n  crew  670      3\n\n, , Age = child, Survived = yes\n\n      Sex\nClass  male female\n  x1st    5      1\n  x2nd   11     13\n  x3rd   13     14\n  crew    0      0\n\n, , Age = adult, Survived = yes\n\n      Sex\nClass  male female\n  x1st   57    140\n  x2nd   14     80\n  x3rd   75     76\n  crew  192     20",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "application_1.html#the-2019-census-data-you-need-is-found-at-this-link",
    "href": "application_1.html#the-2019-census-data-you-need-is-found-at-this-link",
    "title": "3  KNBS Application",
    "section": "4.1 The 2019 Census data you need is found at this link",
    "text": "4.1 The 2019 Census data you need is found at this link",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNBS Application</span>"
    ]
  },
  {
    "objectID": "application_1.html#task",
    "href": "application_1.html#task",
    "title": "5  Application 1",
    "section": "7.1 Task",
    "text": "7.1 Task\n\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(total = n()).",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "application_1.html#task-1",
    "href": "application_1.html#task-1",
    "title": "5  Application 1",
    "section": "7.2 Task",
    "text": "7.2 Task\n\nCreate a new binary column named migrant using mutate and case_when to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "application_1.html#task-2",
    "href": "application_1.html#task-2",
    "title": "5  Application 1",
    "section": "7.3 Task",
    "text": "7.3 Task\n\nFilter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and case_when\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).\n\n\n\nSolution\nlibrary(tidyverse)\n\n\n# start\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\ncensus_small &lt;- census |&gt;\n  select(county, subcounty_code, ea_type, p11, p12, p17, p19, p52)\n\n# religion\nreligion &lt;- census |&gt;\n  select(p17) |&gt;\n  rename(religion = p17)\n\nreligion_without_dk &lt;- religion |&gt;\n  filter(religion != 99)\n\nreligion_by_county &lt;- religion_without_dk |&gt;\n  group_by(county, religion) |&gt;\n  summarise(total = n())\n\n# migration\nmigration &lt;- census_small |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19)\n\nmigration_col &lt;- migration |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmigration_prop &lt;- migration_col |&gt;\n  summarise(migration_prop = mean(migrant))\n\n# working_hours\n\nworking_hours &lt;- census_small |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(age = p12,\n         sex = p11,\n         hours_worked_if_work = p52)\n\nworking_hours_18 &lt;- working_hours |&gt;\n  filter(age &gt;= 18)\n\nworking_hours_gender_rural &lt;- working_hours_18 |&gt;\n  mutate(rural = if_else(ea_type == 1, 1, 0),\n         female = if_else(sex == 2, 1, 0))\n\nworking_hours_no_nas &lt;-  working_hours_gender_rural |&gt;\n  mutate(hours_worked = if_else(is.na(hours_worked_if_work), 0, hours_worked_if_work))\n\n\nworking_hours_mean_adult &lt;- working_hours_no_nas |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Basics",
    "section": "",
    "text": "2.1 Basic Syntax\n−+\n03:00\nAs a quick exercise, try out some arithmetic in your console:\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#basic-syntax",
    "href": "basics.html#basic-syntax",
    "title": "2  Basics",
    "section": "",
    "text": "25 \\* 15\n(45 + 3) \\^ 2\n78 / 4\n\n\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n2.1.1 The assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\n\n−+\n05:00\n\n\n\n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\n2.1.2 Combining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\n\n−+\n05:00\n\n\n\n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n\n[1] 1 2 3\n\n\nSolution\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSolution\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x107b84370&gt;\n&lt;environment: namespace:lubridate&gt;\n\n\nSolution\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#functions",
    "href": "basics.html#functions",
    "title": "2  Basics",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\n\n\n−+\n05:00\n\n\n\n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n\n[1] 36",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#packages",
    "href": "basics.html#packages",
    "title": "2  Basics",
    "section": "2.3 Packages",
    "text": "2.3 Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#the-tidyverse",
    "href": "basics.html#the-tidyverse",
    "title": "2  Basics",
    "section": "2.4 The Tidyverse",
    "text": "2.4 The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\nlibrary(tidyverse)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "application_1.html#setup",
    "href": "application_1.html#setup",
    "title": "5  Application 1",
    "section": "5.2 Setup",
    "text": "5.2 Setup\n\nRead the dataset using read_csv and clean the column names using janitor::clean_names. The 2019 Census data you need is found at this link. The survey information is here\nSelect the columns required for the three pieces of analysis (e.g., p17 for religion) using select. Rename the selected columns to be more descriptive (e.g., religion) using rename.\nExamine your data. For each task, are there any NA’s or other strange values in your columns? Think about how to handle any NA’s and use mutate and case_when, or filter to address them.\n\n\n5.2.1 Breakdown of Kenya by religious belief\n\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(total = n()).\n\n\n\n5.2.2 Share of people who are migrants in each county\n\nCreate a new binary column named migrant using mutate and case_when to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).\n\n\n\n5.2.3 Average working hours for men and women in rural vs. urban areas.\n\nFilter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and case_when\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).\n\n\n\nSolution\nlibrary(tidyverse)\n\n\n# start\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\ncensus_small &lt;- census |&gt;\n  select(county, subcounty_code, ea_type, p11, p12, p17, p19, p52)\n\n# religion\nreligion &lt;- census |&gt;\n  select(p17) |&gt;\n  rename(religion = p17)\n\nreligion_without_dk &lt;- religion |&gt;\n  filter(religion != 99)\n\nreligion_by_county &lt;- religion_without_dk |&gt;\n  group_by(county, religion) |&gt;\n  summarise(total = n())\n\n# migration\nmigration &lt;- census_small |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19)\n\nmigration_col &lt;- migration |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmigration_prop &lt;- migration_col |&gt;\n  summarise(migration_prop = mean(migrant))\n\n# working_hours\n\nworking_hours &lt;- census_small |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(age = p12,\n         sex = p11,\n         hours_worked_if_work = p52)\n\nworking_hours_18 &lt;- working_hours |&gt;\n  filter(age &gt;= 18)\n\nworking_hours_gender_rural &lt;- working_hours_18 |&gt;\n  mutate(rural = if_else(ea_type == 1, 1, 0),\n         female = if_else(sex == 2, 1, 0))\n\nworking_hours_no_nas &lt;-  working_hours_gender_rural |&gt;\n  mutate(hours_worked = if_else(is.na(hours_worked_if_work), 0, hours_worked_if_work))\n\n\nworking_hours_mean_adult &lt;- working_hours_no_nas |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "KNBS Applications",
    "section": "",
    "text": "KNBS Training\nAs part of training for the Kenya National Bureau of Statistics (KNBS), real world examples are provided to complement traditional learning.",
    "crumbs": [
      "KNBS Applications"
    ]
  },
  {
    "objectID": "application_1.html#preparing-your-dataset-for-analyis",
    "href": "application_1.html#preparing-your-dataset-for-analyis",
    "title": "7  Application 1: Data Manipulation",
    "section": "7.2 Preparing your dataset for analyis",
    "text": "7.2 Preparing your dataset for analyis\n\nRead the dataset using read_csv and clean the column names using janitor::clean_names. The 2019 Census data you need is found at this link. The survey information is here\nThere are lots of columns in this dataset, making it slow to perform analysis. Select the columns required for the three pieces of analysis (e.g., p17 for religion) using select. Rename the selected columns to be more descriptive (e.g., religion) using rename.\nExamine your data. For each task, are there any NA’s or other strange values in your columns? Think about how to handle any NA’s and use mutate and case_when, or filter to address them.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Application 1: Data Manipulation</span>"
    ]
  },
  {
    "objectID": "application_1.html#analysis",
    "href": "application_1.html#analysis",
    "title": "7  Application 1: Data Manipulation",
    "section": "7.3 Analysis",
    "text": "7.3 Analysis\n\n7.3.1 Breakdown of Kenya by religious belief\n\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(total = n()) and mutate.\n\n\n\n7.3.2 Share of people who are migrants in each county\n\nCreate a new binary column named migrant using mutate and case_when to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).\n\n\n\n7.3.3 Average working hours for men and women in rural vs. urban areas.\n\nFilter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and case_when\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Application 1: Data Manipulation</span>"
    ]
  },
  {
    "objectID": "application_2.html",
    "href": "application_2.html",
    "title": "8  Application 2: Plotting",
    "section": "",
    "text": "−+\n30:00\n\n\n\n\n9 Plotting in the real world\nMuch as with our data manipulation applications, examples in textbooks are different from real life. Hopefully any data you are plotting you have already tidied, but this can still lead to problems. In this application, we will practice some of the your plotting skills on the datasets you already analysed in the previous applications.\nIf you remember, you performed analysis of 3 different issues using Kenya’s 2019 census data: 1. Breakdown of Kenya by religious belief 2. Share of people who are migrants in each county 3. Average working hours for men and women in rural vs. urban areas.\n\n9.0.1 Visualising Religious Breakdown\n\nUsing the religion_overall dataframe created in step 4:\nCreate a bar chart (geom_col or geom_bar(stat=\"identity\")) showing the share of each religion_label.\nMap religion_label to the x-axis and share to the y-axis.\nAdd appropriate labels using labs() for the title, x-axis, and y-axis.\nFormat the y-axis to display percentages using scale_y_continuous(labels = scales::label_percent()).\nApply a theme, for example theme_minimal().\nChange the colour palette using scale_fill_brewer() (you’ll need to map religion_label to the fill aesthetic as well).\n\n\n\n9.0.2 Visualising Migration Proportion by County\n\nUsing the migration_prop dataframe created in step 6, create a bar chart showing the migration_prop for each county. Map county to the y-axis and migration_prop to the x-axis for better readability (use geom_col).\nUse coord_flip() if you mapped county to x initially to make labels readable, or map directly as described in (a).\nFormat the x-axis (which represents the proportion) to display percentages.\nAdd appropriate labels using labs().\nApply a theme like theme_bw().\nOrder the counties by migration proportion.\n\n\n\n9.0.3 Visualising Average Working Hours\n\nUsing the working_hours_mean_adult dataframe created in step 10:\nCreate a grouped bar chart (geom_col) showing the mean_hours_worked_if_work (average hours for those who work).\nMap area_type (Rural/Urban) to the x-axis.\nMap mean_hours_worked_if_work to the y-axis.\nMap sex_label (Male/Female) to the fill aesthetic to get different coloured bars for men and women.\nUse position = position_dodge() within geom_col() to place the bars for men and women side-by-side within each area type.\nAdd appropriate labels using labs()for the title, axes, and fill legend.\nApply a theme.\nUse scale_fill_brewer() to choose a different colour palette for the fill.\n\n\n\n9.0.4 Saving a Plot\nFinally, practice saving one of the plots you created.\nAssign one of your plots to a variable (e.g., migration_plot &lt;- ggplot(…) + …). Use ggsave() to save this plot to a file. Include today’s date in the filename using Sys.Date(). For example, save it as “migration_plot_YYYY-MM-DD.png”.\n\n\nSolution: Religion Plot\nggplot(data = religion_overall, aes(x = religion_label, y = share, fill = religion_label)) +\n  geom_col() + # Use geom_col as we have pre-summarised y values\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_fill_brewer(palette = \"Set3\") + # Optional: Change colour palette\n  labs(\n    title = \"Religious Breakdown in Kenya (2019 Census)\",\n    x = \"Religion\",\n    y = \"Share of Population\",\n    fill = \"Religion\" # Optional: Rename legend title\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels if needed\n\n#Reorder counties before plotting\nmigration_prop_ordered &lt;- migration_prop |&gt;\n  mutate(county = fct_reorder(county, migration_prop))\n\nggplot(data = migration_prop_ordered, aes(x = migration_prop, y = county)) +\n  geom_col(fill = \"skyblue\") + # Manually set a fill colour\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(\n    title = \"Share of Migrants by County (2019 Census)\",\n    x = \"Proportion of Residents Born Elsewhere\",\n    y = \"County\"\n  ) +\n  theme_bw()\n\nggplot(working_hours_mean_adult, aes(x = area_type, y = mean_hours_worked_if_work, fill = sex_label)) +\n  geom_col(position = position_dodge()) +\n  scale_fill_brewer(palette = \"Paired\") + # Optional: Choose a palette\n  labs(\n    title = \"Average Weekly Hours Worked by Area and Sex (Adults, 2019 Census)\",\n    subtitle = \"Among those reporting working hours\",\n    x = \"Area Type\",\n    y = \"Average Weekly Hours Worked\",\n    fill = \"Sex\" # Renames the legend\n  ) +\n  theme_light()\n\n# First, assign the migration plot code to a variable\nmigration_plot &lt;- ggplot(data = migration_prop_ordered, aes(x = migration_prop, y = county)) +\n  geom_col(fill = \"skyblue\") +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(\n    title = \"Share of Migrants by County (2019 Census)\",\n    x = \"Proportion of Residents Born Elsewhere\",\n    y = \"County\"\n  ) +\n  theme_bw()\n\n# Now, save the plot\nggsave(filename = paste0(\"migration_plot_\", Sys.Date(), \".png\"), plot = migration_plot, width = 8, height = 10) # Adjust width/height as needed\n\n# Note: This saves the file to your current working directory in RStudio.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Application 2: Plotting</span>"
    ]
  },
  {
    "objectID": "tidy_data.html",
    "href": "tidy_data.html",
    "title": "6  Tidy Data",
    "section": "",
    "text": "6.1 What is tidy data anyway?\nSo far, your manipulation of data has focused on processes which make use of tidyverse functions, including filtering rows, selecting columns and creating new columns, as well as plotting data on charts. All of the functions you’ve seen have been easy to apply to the data you have.\nUnfortunately, in real projects, this often won’t be the case because the data you’ll have often won’t be in tidy format, which is how most tidyverse functions expect to receive data.\nTidy data is a standardised structure of data designed make data cleaning and processing steps easy, and is straightforward even if it seems unusual at first.\nYou are likely to be most familiar with data laid out in a “messy” structure like the one below:\n# A tibble: 30 × 8\n   country                continent `1967` `1972` `1977` `1982` `1987` `1992`\n   &lt;fct&gt;                  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Albania                Europe      66.2   67.7   68.9   70.4   72     71.6\n 2 Austria                Europe      70.1   70.6   72.2   73.2   74.9   76.0\n 3 Belgium                Europe      70.9   71.4   72.8   73.9   75.4   76.5\n 4 Bosnia and Herzegovina Europe      64.8   67.4   69.9   70.7   71.1   72.2\n 5 Bulgaria               Europe      70.4   70.9   70.8   71.1   71.3   71.2\n 6 Croatia                Europe      68.5   69.6   70.6   70.5   71.5   72.5\n 7 Czech Republic         Europe      70.4   70.3   70.7   71.0   71.6   72.4\n 8 Denmark                Europe      73.0   73.5   74.7   74.6   74.8   75.3\n 9 Finland                Europe      69.8   70.9   72.5   74.6   74.8   75.7\n10 France                 Europe      71.6   72.4   73.8   74.9   76.3   77.5\n# ℹ 20 more rows\nHere, the data has one row per country and then multiple columns, with one for each year. This may seem relatively logical and quite tidy, but the problems start when you realise that there are multiple other quite logical and tidy ways to lay this data out if there are few restrictions. For example, it could quite easily be shown with one country per column, and a year per row:\n# A tibble: 12 × 143\n    year Afghanistan Albania Algeria Angola Argentina Australia Austria Bahrain\n   &lt;int&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1952        28.8    55.2    43.1   30.0      62.5      69.1    66.8    50.9\n 2  1957        30.3    59.3    45.7   32.0      64.4      70.3    67.5    53.8\n 3  1962        32.0    64.8    48.3   34        65.1      70.9    69.5    56.9\n 4  1967        34.0    66.2    51.4   36.0      65.6      71.1    70.1    59.9\n 5  1972        36.1    67.7    54.5   37.9      67.1      71.9    70.6    63.3\n 6  1977        38.4    68.9    58.0   39.5      68.5      73.5    72.2    65.6\n 7  1982        39.9    70.4    61.4   39.9      69.9      74.7    73.2    69.1\n 8  1987        40.8    72      65.8   39.9      70.8      76.3    74.9    70.8\n 9  1992        41.7    71.6    67.7   40.6      71.9      77.6    76.0    72.6\n10  1997        41.8    73.0    69.2   41.0      73.3      78.8    77.5    73.9\n11  2002        42.1    75.7    71.0   41.0      74.3      80.4    79.0    74.8\n12  2007        43.8    76.4    72.3   42.7      75.3      81.2    79.8    75.6\n# ℹ 134 more variables: Bangladesh &lt;dbl&gt;, Belgium &lt;dbl&gt;, Benin &lt;dbl&gt;,\n#   Bolivia &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, Botswana &lt;dbl&gt;,\n#   Brazil &lt;dbl&gt;, Bulgaria &lt;dbl&gt;, `Burkina Faso` &lt;dbl&gt;, Burundi &lt;dbl&gt;,\n#   Cambodia &lt;dbl&gt;, Cameroon &lt;dbl&gt;, Canada &lt;dbl&gt;,\n#   `Central African Republic` &lt;dbl&gt;, Chad &lt;dbl&gt;, Chile &lt;dbl&gt;, China &lt;dbl&gt;,\n#   Colombia &lt;dbl&gt;, Comoros &lt;dbl&gt;, `Congo, Dem. Rep.` &lt;dbl&gt;,\n#   `Congo, Rep.` &lt;dbl&gt;, `Costa Rica` &lt;dbl&gt;, `Cote d'Ivoire` &lt;dbl&gt;, …\nHaving this kind of wide variety of potential structures means that when you receive data and try to use it, it’s almost impossible to have any kind of standardised approach to this non-standard data structuring. For example, if you want the most recent year of data from an annual dataset, would you need to select a column or filter some rows; it would depend entirely on the dataset!\nIn contrast, tidy data always has the same format:\nThis is best explained in a tidy example:\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\nIn this dataset, each column is a different variable; years, life expectancy, countries. These are best thought of as different categories that your data can fall into, not values of those categories. For example, year is a variable, but each possible year in that column is a value, and therefore shouldn’t be assigned its own column.\nEvery row is an observation; a unique combination of the variables included in the data. For example, there is only one observation (row) of data in Afghanistan for 1952, and then a separate one for the same country in 1957.\nAnd importantly, there is only one value in each cell; this seems obvious but is suprisingly often not the case, especially when combined values such as age and sex (25M, 54F) or section and subsection (5b or 8c) are created.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#what-is-tidy-data-anyway",
    "href": "tidy_data.html#what-is-tidy-data-anyway",
    "title": "6  Tidy Data",
    "section": "",
    "text": "Every column is a variable.\nEvery row is an observation.\nEvery cell is a single value.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#tidying-messy-data",
    "href": "tidy_data.html#tidying-messy-data",
    "title": "6  Tidy Data",
    "section": "6.2 Tidying messy data",
    "text": "6.2 Tidying messy data\nInevitably, you will often want to use data which is not in tidy format. Luckily the tidyverse package tidyr is designed to help with this, and contains a number of functions designed to tidy messy data. The functions you will want to use most frequently from this are the pivot verbs.\n\n6.2.1 Pivot longer\nThe pivot_longer function does exactly what the name suggests, pivots data from a wide format (with many columns and few rows) to a long format (with many rows and few columns). This is very useful when you have data which has values as column names, and you want to gather these together into a single column of multiple values instead.\n\npay_gap &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-06-28/paygap.csv') |&gt;\n  select(employer_name, male_bonus_percent, female_bonus_percent,  date_submitted) \n\nThis example uses the above dataset; a subsection UK company pay gap data, with male and female bonus percentages displayed in different columns. According to tidy data principles, this data would be easier to use if this data was stored in a single column with an additional variable indicating whether these were male or female bonuses.\nUsing pivot_longer we can specify the columns we’re talking about using the cols argument. This can be done in the same way as in the select function, either by specifying columns you wish to include or those you want to exclude (using the -).\nThere are several ways to use the pivot_longer function.\n\n##Pivoting data; note that cols listed are the ones we want to include in the pivoting\npay_gap_long &lt;- pay_gap |&gt;\n  pivot_longer(cols = c(male_bonus_percent, female_bonus_percent))\n\n\n##Pivoting data; this returns the same result as the previous code, but the columns listed are excluded from the pivoting, so any not listed are pivoted\npay_gap_long &lt;- pay_gap |&gt;\n  pivot_longer(cols = -c(employer_name,  date_submitted))\n\nThe other two arguments you may want to use are names_to and values_to, which allow you to specify the names of the columns you want to put the current column names (male_bonus_percent and female_bonus_percent) and values (the values in the two existing columns) into. If these are not specified, they will default to “name” and “value”, as in the examples above.\n\npay_gap_long &lt;- pay_gap |&gt;\n  pivot_longer(cols = c(male_bonus_percent, female_bonus_percent),\n                      names_to = \"gender\",\n                      values_to = \"percent\")\n\n\n\n6.2.2 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUsing the inbuilt dataset billboard, pivot the week columns into a single long column. Call the dataset billboard_long\n\n\n\nSolution\n#Option 1: Excluding columns not being pivoted\nbillboard_long &lt;- billboard |&gt;\n  pivot_longer(cols = -c(artist, track, date.entered),\n               names_to = \"week\",\n               values_to = \"rank\")\n\n#Option 2: Specifying columns range explicitly\nbillboard_long &lt;- billboard |&gt;\n  pivot_longer(cols = wk1:w76,\n               names_to = \"week\",\n               values_to = \"rank\")\n\n#Option 3: Using starts_with()\nbillboard_long &lt;- billboard |&gt;\n  pivot_longer(cols = starts_with(\"wk\"), \n               names_to = \"week\",\n               values_to = \"rank\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#combining-data-tidying-with-data-manipulation",
    "href": "tidy_data.html#combining-data-tidying-with-data-manipulation",
    "title": "6  Tidy Data",
    "section": "6.3 Combining data tidying with data manipulation",
    "text": "6.3 Combining data tidying with data manipulation\nThe real power in tidy data comes with using it in combination with other dplyr verbs such as filtering, selecting and mutating. For example, in the full pay gap dataset, we can calculate whether there’s a difference in the hourly pay and bonuses by gender for different sizes of company:\n\npay_gap_summary &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-06-28/paygap.csv') |&gt;\n  #Select the columns we're interested in\n  select(employer_name, diff_mean_hourly_percent, diff_mean_bonus_percent, employer_size) |&gt;\n  #Pivot to gather up all of the mean values into one column\n  pivot_longer(cols = -c(employer_name, employer_size), names_to = \"gender\", values_to = \"percent\") |&gt;\n  #Group by employer size and variable type\n  group_by(employer_size, gender) |&gt;\n  #Summarise by calculating means\n  dplyr::summarise(percent = mean(percent, na.rm = TRUE)) |&gt;\n  #Remove companies which didn't specify a size\n  filter(employer_size != \"Not Provided\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#tidy-data-and-plotting",
    "href": "tidy_data.html#tidy-data-and-plotting",
    "title": "6  Tidy Data",
    "section": "6.4 Tidy data and plotting",
    "text": "6.4 Tidy data and plotting\nThe value of having a tidy dataset is also obvious when you come to plot your data in ggplot. Having a single variable in a column means that you’re able to assign that column to an aesthetic in ggplot, and therefore change the colour, shape, faceting, etc on the basis of that value! This opens the door to creating increasingly complex and informative (and pretty!) charts.\n\n##Plot using the tidy data\nggplot(pay_gap_summary, aes(x = employer_size, \n                            y = percent, \n                            fill = gender))+ #Colours can be assigned to the different data types\n  geom_col(position = \"dodge\") #makes bars sit next to eachother\n\n\n\n\n\n\n\n\n\n6.4.1 Exercise\n\n\n\n−+\n15:00\n\n\n\n\nUsing the inbuilt dataset relig_income, pivot the income columns into a long dataset.\nAssign this to an object called income_long\nUsing the dataset income_long you created, plot a bar chart in ggplot. Use religion as the x axis and the count of people as the y axis.\nAssign the income groupings to the colour aesthetic.\n\n\n\nSolution\n#Tidy the data using pivot_longer()\nincome_long &lt;- relig_income |&gt;\n  pivot_longer(cols = -religion,\n               names_to = \"income_group\",\n               values_to = \"count\")\n\n#Plot your data \nincome_chart &lt;- \n  ggplot(income_long, aes(x = religion, y = count, fill = income_group)) +\n  #stat = identity creates a stacked bar chart\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Religion\", y = \"Number of People\", fill = \"Income Group\") +\n  theme_minimal()\n\n#print() allows you to view the chart in the 'Plots' tab\nprint(income_chart)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#multiple-data-frames",
    "href": "tidy_data.html#multiple-data-frames",
    "title": "6  Tidy Data",
    "section": "6.5 Multiple data frames",
    "text": "6.5 Multiple data frames\nIt’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.\nIn this lesson, you’ll learn about the most important types of joins:\n\nMutating joins, which add new variables to one data frame from matching observations in another.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#keys-connecting-your-data-frames",
    "href": "tidy_data.html#keys-connecting-your-data-frames",
    "title": "6  Tidy Data",
    "section": "6.6 Keys: Connecting your data frames",
    "text": "6.6 Keys: Connecting your data frames\nTo understand joins, you need to first understand how two tables can be connected through a pair of keys. Every join involves a pair of keys:\nA primary key is a variable or set of variables that uniquely identifies each observation. A foreign key is a variable (or set of variables) that corresponds to a primary key in another table.\nWhen more than one variable is needed to form a key, it’s called a compound key. Let’s look at some examples from the nycflights13 package:\n\nairlines records two pieces of data about each airline: its carrier code and its full name. You can identify an airline with its two letter carrier code, making carrier the primary key.\n\nnycflights13::airlines\n\n# A tibble: 16 × 2\n   carrier name                       \n   &lt;chr&gt;   &lt;chr&gt;                      \n 1 9E      Endeavor Air Inc.          \n 2 AA      American Airlines Inc.     \n 3 AS      Alaska Airlines Inc.       \n 4 B6      JetBlue Airways            \n 5 DL      Delta Air Lines Inc.       \n 6 EV      ExpressJet Airlines Inc.   \n 7 F9      Frontier Airlines Inc.     \n 8 FL      AirTran Airways Corporation\n 9 HA      Hawaiian Airlines Inc.     \n10 MQ      Envoy Air                  \n11 OO      SkyWest Airlines Inc.      \n12 UA      United Air Lines Inc.      \n13 US      US Airways Inc.            \n14 VX      Virgin America             \n15 WN      Southwest Airlines Co.     \n16 YV      Mesa Airlines Inc.         \n\n\nairports records data about each airport. You can identify each airport by its three letter airport code, making faa the primary key.\n\nairports\n\n# A tibble: 1,458 × 8\n   faa   name                    lat    lon   alt    tz dst   tzone\n   &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 04G   Lansdowne Airport      41.1  -80.6  1044    -5 A     Amer…\n 2 06A   Moton Field Municipa…  32.5  -85.7   264    -6 A     Amer…\n 3 06C   Schaumburg Regional    42.0  -88.1   801    -6 A     Amer…\n 4 06N   Randall Airport        41.4  -74.4   523    -5 A     Amer…\n 5 09J   Jekyll Island Airport  31.1  -81.4    11    -5 A     Amer…\n 6 0A9   Elizabethton Municip…  36.4  -82.2  1593    -5 A     Amer…\n 7 0G6   Williams County Airp…  41.5  -84.5   730    -5 A     Amer…\n 8 0G7   Finger Lakes Regiona…  42.9  -76.8   492    -5 A     Amer…\n 9 0P2   Shoestring Aviation …  39.8  -76.6  1000    -5 U     Amer…\n10 0S9   Jefferson County Intl  48.1 -123.    108    -8 A     Amer…\n# ℹ 1,448 more rows\n\n\nplanes records data about each plane. You can identify a plane by its tail number, making tailnum the primary key.\n\nplanes\n\n# A tibble: 3,322 × 9\n   tailnum  year type        manufacturer model engines seats speed\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 N10156   2004 Fixed wing… EMBRAER      EMB-…       2    55    NA\n 2 N102UW   1998 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 3 N103US   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 4 N104UW   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 5 N10575   2002 Fixed wing… EMBRAER      EMB-…       2    55    NA\n 6 N105UW   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 7 N107US   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 8 N108UW   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n 9 N109UW   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n10 N110UW   1999 Fixed wing… AIRBUS INDU… A320…       2   182    NA\n# ℹ 3,312 more rows\n# ℹ 1 more variable: engine &lt;chr&gt;\n\n\nweather records data about the weather at the origin airports. You can identify each observation by the combination of location and time, making origin and time_hour the compound primary key.\n\nweather\n\n# A tibble: 26,115 × 15\n   origin  year month   day  hour  temp  dewp humid wind_dir\n   &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270\n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240\n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250\n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260\n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240\n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240\n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250\n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260\n10 EWR     2013     1     1    10  41    28.0  59.6      260\n# ℹ 26,105 more rows\n# ℹ 6 more variables: wind_speed &lt;dbl&gt;, wind_gust &lt;dbl&gt;,\n#   precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\n\nThe foreign keys in the flights dataset connect to the primary keys of these other tables:\nflights$tailnumconnects to planes$tailnum flights$carrier connects to airlines$carrier flights$origin and flights$dest connect to airports$faa flights$origin and flights$time_hour together connect to weather$origin and weather$time_hour\nThese relationships are summarized visually in Figure 6.1.\n\n\n\n\n\n\n\n\nFigure 6.1: Connections between all five data frames in the nycflights13 package. Variables making up a primary key are colored grey, and are connected to their corresponding foreign keys with arrows.\n\n\n\n\n\n\n6.6.1 Checking primary keys\nIt’s good practice to verify that primary keys truly uniquely identify each observation. One way to do that is to count the primary keys and look for entries where the count is greater than one. One way to do that is to count() the primary keys and look for entries where n is greater than one. This reveals that planes and weather both look good:\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\nweather |&gt; \n  count(time_hour, origin) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: time_hour &lt;dttm&gt;, origin &lt;chr&gt;, n &lt;int&gt;\n\n\nYou should also check for missing values in your primary keys — if a value is missing then it can’t identify an observation!\n\nplanes |&gt; \n  filter(is.na(tailnum))\n\n# A tibble: 0 × 9\n# ℹ 9 variables: tailnum &lt;chr&gt;, year &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;,\n#   model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\nweather |&gt; \n  filter(is.na(time_hour) | is.na(origin))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: origin &lt;chr&gt;, year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, hour &lt;int&gt;,\n#   temp &lt;dbl&gt;, dewp &lt;dbl&gt;, humid &lt;dbl&gt;, wind_dir &lt;dbl&gt;, wind_speed &lt;dbl&gt;,\n#   wind_gust &lt;dbl&gt;, precip &lt;dbl&gt;, pressure &lt;dbl&gt;, visib &lt;dbl&gt;,\n#   time_hour &lt;dttm&gt;\n\n\n\n\n6.6.2 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nweather only contains information for the three origin airports in NYC. If it contained weather records for all airports in the USA, what additional connection would it make to flights?\nWe know that some days of the year are special and fewer people than usual fly on them (e.g., Christmas eve and Christmas day). How might you represent that data as a data frame? What would be the primary key? How would it connect to the existing data frames?\n\n\n\nSolution\n# 1. weather$origin would join with flights%origin\n\n# 2. special_days \n#  month   day  name  \n#  &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;\n# Primary key: Compound key of month and day",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#basic-joins",
    "href": "tidy_data.html#basic-joins",
    "title": "6  Tidy Data",
    "section": "6.7 Basic joins",
    "text": "6.7 Basic joins\nNow that you understand how data frames are connected via keys, we can start using joins to better understand the flights dataset. dplyr provides six join functions:\n\nleft_join(): keeps all observations in the first table\ninner_join(): keeps only observations that appear in both tables\nright_join(): keeps all observations in the second table\nfull_join(): keeps all observations in either table\nsemi_join(): filters the first table to include only observations that match the second table\nanti_join(): filters the first table to include only observations that don’t match the second table\n\nIn this course, you’ll learn how to use one mutating join, left_join(), and two filtering joins, semi_join() and anti_join().\n\n6.7.1 Mutating joins\nA mutating join allows you to combine variables from two data frames: it first matches observations by their keys, then copies across variables from one data frame to the other. Like mutate(), the join functions add variables to the right, so if your dataset has many variables, you won’t see the new ones. For these examples, we’ll make it easier to see what’s going on by creating a narrower dataset with just six variables.\n\nflights2 &lt;- flights |&gt; \n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2\n\n# A tibble: 336,776 × 6\n    year time_hour           origin dest  tailnum carrier\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6     \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV     \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6     \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA     \n# ℹ 336,766 more rows\n\n\nThere are four types of mutating join, but there’s one that you’ll use almost all of the time: left_join(). It’s special because the output will always have the same rows as x, the data frame you’re joining to. The primary use of left_join() is to add in additional metadata. For example, we can use left_join() to add the full airline name to the flights2 data:\n\nflights2 |&gt;\n  left_join(airlines)\n\nJoining with `by = join_by(carrier)`\n\n\n# A tibble: 336,776 × 7\n    year time_hour           origin dest  tailnum carrier name                  \n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                 \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines Inc. \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines Inc. \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines Inc.\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways       \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.  \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines Inc. \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      JetBlue Airways       \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      ExpressJet Airlines I…\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      JetBlue Airways       \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      American Airlines Inc.\n# ℹ 336,766 more rows\n\n\nOr we could find out the temperature and wind speed when each plane departed:\n\nflights2 |&gt; \n  left_join(weather |&gt; select(origin, time_hour, temp, wind_speed))\n\nJoining with `by = join_by(time_hour, origin)`\n\n\n# A tibble: 336,776 × 8\n    year time_hour           origin dest  tailnum carrier  temp wind_speed\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6       37.9       11.5\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV       39.9       16.1\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6       37.9       13.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA       39.9       16.1\n# ℹ 336,766 more rows\n\n\nOr what size of plane was flying:\n\nflights2 |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 336,776 × 9\n    year time_hour           origin dest  tailnum carrier type     engines seats\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Fixed w…       2   149\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      Fixed w…       2   149\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Fixed w…       2   178\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      Fixed w…       2   200\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Fixed w…       2   178\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Fixed w…       2   191\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Fixed w…       2   200\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      Fixed w…       2    55\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      Fixed w…       2   200\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;          NA    NA\n# ℹ 336,766 more rows\n\n\nWhen left_join() fails to find a match for a row in x, it fills in the new variables with missing values. For example, there’s no information about the plane with tail number N3ALAA so the type, engines, and seats will be missing:\n\nflights2 |&gt; \n  filter(tailnum == \"N3ALAA\") |&gt; \n  left_join(planes |&gt; select(tailnum, type, engines, seats))\n\nJoining with `by = join_by(tailnum)`\n\n\n# A tibble: 63 × 9\n    year time_hour           origin dest  tailnum carrier type  engines seats\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;\n 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 7  2013 2013-01-20 18:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 8  2013 2013-01-22 17:00:00 JFK    ORD   N3ALAA  AA      &lt;NA&gt;       NA    NA\n 9  2013 2013-10-11 06:00:00 EWR    MIA   N3ALAA  AA      &lt;NA&gt;       NA    NA\n10  2013 2013-10-14 08:00:00 JFK    BOS   N3ALAA  AA      &lt;NA&gt;       NA    NA\n# ℹ 53 more rows\n\n\nWe’ll come back to this problem a few times in the rest of the chapter.\n\n\n6.7.2 Specifying join keys\nBy default, left_join() will use all variables that appear in both data frames as the join key (a “natural join”). This is convenient but doesn’t always work. For example, if we try to join flights2 with the complete planes dataset:\n\nflights2 |&gt; \n  left_join(planes)\n\nJoining with `by = join_by(year, tailnum)`\n\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier type  manufacturer\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       \n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      &lt;NA&gt;  &lt;NA&gt;        \n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      &lt;NA&gt;  &lt;NA&gt;        \n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      &lt;NA&gt;  &lt;NA&gt;        \n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      &lt;NA&gt;  &lt;NA&gt;        \n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      &lt;NA&gt;  &lt;NA&gt;        \n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      &lt;NA&gt;  &lt;NA&gt;        \n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      &lt;NA&gt;  &lt;NA&gt;        \n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      &lt;NA&gt;  &lt;NA&gt;        \n# ℹ 336,766 more rows\n# ℹ 5 more variables: model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;,\n#   engine &lt;chr&gt;\n\n\nWe get many missing matches because our join is trying to use both tailnum and year as a compound key. But these year columns mean different things: in flights%year, it’s the year the flight occurred; in planes$year, it’s the year the plane was built.\n\nflights2 |&gt; \n  left_join(planes, join_by(tailnum))\n\n# A tibble: 336,776 × 14\n   year.x time_hour           origin dest  tailnum carrier year.y type          \n    &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;         \n 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999 Fixed wing mu…\n 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998 Fixed wing mu…\n 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990 Fixed wing mu…\n 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012 Fixed wing mu…\n 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991 Fixed wing mu…\n 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012 Fixed wing mu…\n 7   2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6        2000 Fixed wing mu…\n 8   2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV        1998 Fixed wing mu…\n 9   2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6        2004 Fixed wing mu…\n10   2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA          NA &lt;NA&gt;          \n# ℹ 336,766 more rows\n# ℹ 6 more variables: manufacturer &lt;chr&gt;, model &lt;chr&gt;, engines &lt;int&gt;,\n#   seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;\n\n\nNote that the year variables are disambiguated in the output with a suffix (year.x and year.y).\njoin_by(tailnum) is short for join_by(tailnum == tailnum). The latter form is useful when the key variables have different names in each table:\n\nflights2 |&gt; \n  left_join(airports, join_by(dest == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bu…  30.0 -95.3\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bu…  30.0 -95.3\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl  25.8 -80.3\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      &lt;NA&gt;        NA    NA  \n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfiel…  33.6 -84.4\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago O…  42.0 -87.9\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Fort Laud…  26.1 -80.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      Washingto…  38.9 -77.5\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      Orlando I…  28.4 -81.3\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      Chicago O…  42.0 -87.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\nflights2 |&gt; \n  left_join(airports, join_by(origin == faa))\n\n# A tibble: 336,776 × 13\n    year time_hour           origin dest  tailnum carrier name         lat   lon\n   &lt;int&gt; &lt;dttm&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Li…  40.7 -74.2\n 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia  40.8 -73.9\n 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Ke…  40.6 -73.8\n 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Ke…  40.6 -73.8\n 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia  40.8 -73.9\n 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Li…  40.7 -74.2\n 7  2013 2013-01-01 06:00:00 EWR    FLL   N516JB  B6      Newark Li…  40.7 -74.2\n 8  2013 2013-01-01 06:00:00 LGA    IAD   N829AS  EV      La Guardia  40.8 -73.9\n 9  2013 2013-01-01 06:00:00 JFK    MCO   N593JB  B6      John F Ke…  40.6 -73.8\n10  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      La Guardia  40.8 -73.9\n# ℹ 336,766 more rows\n# ℹ 4 more variables: alt &lt;dbl&gt;, tz &lt;dbl&gt;, dst &lt;chr&gt;, tzone &lt;chr&gt;\n\n\ninner_join(), right_join(), full_join() have the same interface as left_join(). The difference is which rows they keep: left join keeps all the rows in x, the right join keeps all rows in y, the full join keeps all rows in either x or y, and the inner join only keeps rows that occur in both x and y. We’ll come back to these in more detail later.\n\n\n6.7.3 Exercise\n\n\n\n−+\n20:00\n\n\n\n\nFind the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?\nImagine you’ve found the top 10 most popular destinations using this code:\n\ntop_dest &lt;- flights2 |&gt;\n  count(dest, sort = TRUE) |&gt;\n  head(10)\n\nHow can you find all flights to those destinations?\nDoes every departing flight have corresponding weather data for that hour?\nWhat do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "tidy_data.html#how-do-joins-work",
    "href": "tidy_data.html#how-do-joins-work",
    "title": "6  Tidy Data",
    "section": "6.8 How do joins work?",
    "text": "6.8 How do joins work?\nNow that you’ve used joins a few times it’s time to learn more about how they work, focusing on how each row in x matches rows in y. We’ll begin by introducing a visual representation of joins, using the simple tibbles defined below and shown in Figure 6.2. In these examples we’ll use a single key called key and a single value column (val_x and val_y), but the ideas all generalize to multiple keys and multiple values.\n\nx &lt;- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     3, \"x3\"\n)\ny &lt;- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     4, \"y3\"\n)\n\n\n\n\n\n\n\n\n\nFigure 6.2: Graphical representation of two simple tables. The colored key columns map background color to key value. The grey columns represent the “value” columns that are carried along for the ride.\n\n\n\n\n\nFigure 6.3 introduces the foundation for our visual representation. It shows all potential matches between x and y as the intersection between lines drawn from each row of x and each row of y. The rows and columns in the output are primarily determined by x, so the x table is horizontal and lines up with the output.\n\n\n\n\n\n\n\n\nFigure 6.3: To understand how joins work, it’s useful to think of every possible match. Here we show that with a grid of connecting lines.\n\n\n\n\n\nTo describe a specific type of join, we indicate matches with dots. The matches determine the rows in the output, a new data frame that contains the key, the x values, and the y values. For example, Figure 6.4 shows an inner join, where rows are retained if and only if the keys are equal.\n\n\n\n\n\n\n\n\nFigure 6.4: An inner join matches each row in x to the row in y that has the same value of key. Each match becomes a row in the output.\n\n\n\n\n\nWe can apply the same principles to explain the outer joins, which keep observations that appear in at least one of the data frames. These joins work by adding an additional “virtual” observation to each data frame. This observation has a key that matches if no other key matches, and values filled with NA. There are three types of outer joins:\n\nA left join keeps all observations in x, Figure 6.5. Every row of x is preserved in the output because it can fall back to matching a row of NAs in y.\n\n\n\n\n\n\n\n\nFigure 6.5: A visual representation of the left join where every row in x appears in the output.\n\n\n\n\n\nA right join keeps all observations in y, Figure 6.6. Every row of y is preserved in the output because it can fall back to matching a row of NAs in x. The output still matches x as much as possible; any extra rows from y are added to the end.\n\n\n\n\n\n\n\n\nFigure 6.6: A visual representation of the right join where every row of y appears in the output.\n\n\n\n\n\nA full join keeps all observations that appear in x or y, Figure 6.7. Every row of x and y is included in the output because both x and y have a fall back row of NAs. Again, the output starts with all rows from x, followed by the remaining unmatched y rows.\n\n\n\n\n\n\n\n\nFigure 6.7: A visual representation of the full join where every row in x and y appears in the output.\n\n\n\n\n\n\nAnother way to show how the types of outer join differ is with a Venn diagram, as in Figure 6.8. However, this is not a great representation because while it might jog your memory about which rows are preserved, it fails to illustrate what’s happening with the columns.\n\n\n\n\n\n\n\n\nFigure 6.8: Venn diagrams showing the difference between inner, left, right, and full joins.\n\n\n\n\n\nThe joins shown here are the so-called equi joins, where rows match if the keys are equal. Equi joins are the most common type of join, so we’ll typically omit the equi prefix, and just say “inner join” rather than “equi inner join”.\n\n6.8.1 Row matching\nSo far we’ve explored what happens if a row in x matches zero or one row in y. What happens if it matches more than one row? To understand what’s going on let’s first narrow our focus to the inner_join() and then draw a picture, Figure 6.9.\n\n\n\n\n\n\n\n\nFigure 6.9: The three ways a row in x can match. x1 matches one row in y, x2 matches two rows in y, x3 matches zero rows in y. Note that while there are three rows in x and three rows in the output, there isn’t a direct correspondence between the rows.\n\n\n\n\n\nThere are three possible outcomes for a row in x:\n\nIf it doesn’t match anything, it’s dropped.\nIf it matches 1 row in y, it’s preserved.\nIf it matches more than 1 row in y, it’s duplicated once for each match.\n\nIn principle, this means that there’s no guaranteed correspondence between the rows in the output and the rows in x, but in practice, this rarely causes problems. There is, however, one particularly dangerous case which can cause a combinatorial explosion of rows. Imagine joining the following two tables:\n\ndf1 &lt;- tibble(key = c(1, 2, 2), val_x = c(\"x1\", \"x2\", \"x3\"))\ndf2 &lt;- tibble(key = c(1, 2, 2), val_y = c(\"y1\", \"y2\", \"y3\"))\n\nWhile the first row in df1 only matches one row in df2, the second and third rows both match two rows. This is sometimes called a many-to-many join, and will cause dplyr to emit a warning:\n\ndf1 |&gt; \n  inner_join(df2, join_by(key))\n\nWarning in inner_join(df1, df2, join_by(key)): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 2 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 5 × 3\n    key val_x val_y\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 x1    y1   \n2     2 x2    y2   \n3     2 x2    y3   \n4     2 x3    y2   \n5     2 x3    y3   \n\n\nIf you are doing this deliberately, you can set relationship = \"many-to-many\", as the warning suggests.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "application_2.html#visualising-religious-breakdown",
    "href": "application_2.html#visualising-religious-breakdown",
    "title": "8  Application 2: Plotting",
    "section": "9.1 Visualising Religious Breakdown",
    "text": "9.1 Visualising Religious Breakdown\n\nUsing the religion_overall dataframe created in step 4:\nCreate a bar chart (geom_col or geom_bar(stat=\"identity\")) showing the share of each religion_label.\nMap religion_label to the x-axis and share to the y-axis.\nAdd appropriate labels using labs() for the title, x-axis, and y-axis.\nFormat the y-axis to display percentages using scale_y_continuous(labels = scales::label_percent()).\nApply a theme, for example theme_minimal().\nChange the colour palette using scale_fill_brewer() (you’ll need to map religion_label to the fill aesthetic as well).\nVisualising Migration Proportion by County Using the migration_prop dataframe created in step 6:\n\n\nCreate a bar chart showing the migration_prop for each county. Map county to the y-axis and migration_prop to the x-axis for better readability (use geom_col).\nUse coord_flip() if you mapped county to x initially to make labels readable, or map directly as described in (a).\nFormat the x-axis (which represents the proportion) to display percentages.\nAdd appropriate labels using labs().\nApply a theme like theme_bw().\n(Optional): Order the counties by migration proportion. You can do this by modifying the migration_prop dataframe first using mutate(county = fct_reorder(county, migration_prop)) from the forcats package (part of tidyverse).\n\n\nVisualising Average Working Hours Using the working_hours_mean_adult dataframe created in step 10:\n\n\nCreate a grouped bar chart (geom_col) showing the mean_hours_worked_if_work (average hours for those who work).\nMap area_type (Rural/Urban) to the x-axis.\nMap mean_hours_worked_if_work to the y-axis.\nMap sex_label (Male/Female) to the fill aesthetic to get different coloured bars for men and women.\nUse position = position_dodge() within geom_col() to place the bars for men and women side-by-side within each area type.\nAdd appropriate labels using labs() for the title, axes, and fill legend.\nApply a theme.\n(Optional): Use scale_fill_brewer() to choose a different colour palette for the fill.\n\n\n\nSolution: Religion Plot\nggplot(data = religion_overall, aes(x = religion_label, y = share, fill = religion_label)) +\n  geom_col() + # Use geom_col as we have pre-summarised y values\n  scale_y_continuous(labels = scales::label_percent()) +\n  scale_fill_brewer(palette = \"Set3\") + # Optional: Change colour palette\n  labs(\n    title = \"Religious Breakdown in Kenya (2019 Census)\",\n    x = \"Religion\",\n    y = \"Share of Population\",\n    fill = \"Religion\" # Optional: Rename legend title\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels if needed\n\n\n\n\nSolution: Migration Plot\n# Optional: Reorder counties before plotting\nmigration_prop_ordered &lt;- migration_prop |&gt;\n  mutate(county = fct_reorder(county, migration_prop))\n\nggplot(data = migration_prop_ordered, aes(x = migration_prop, y = county)) +\n  geom_col(fill = \"skyblue\") + # Manually set a fill colour\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(\n    title = \"Share of Migrants by County (2019 Census)\",\n    x = \"Proportion of Residents Born Elsewhere\",\n    y = \"County\"\n  ) +\n  theme_bw()\n\n\n\n\nSolution: Working Hours Plot\nggplot(working_hours_mean_adult, aes(x = area_type, y = mean_hours_worked_if_work, fill = sex_label)) +\n  geom_col(position = position_dodge()) +\n  scale_fill_brewer(palette = \"Paired\") + # Optional: Choose a palette\n  labs(\n    title = \"Average Weekly Hours Worked by Area and Sex (Adults, 2019 Census)\",\n    subtitle = \"Among those reporting working hours\",\n    x = \"Area Type\",\n    y = \"Average Weekly Hours Worked\",\n    fill = \"Sex\" # Renames the legend\n  ) +\n  theme_light()\n\n\nSaving a Plot Finally, practice saving one of the plots you created.\nAssign one of your plots to a variable (e.g., migration_plot &lt;- ggplot(…) + …). Use ggsave() to save this plot to a file. Include today’s date in the filename using Sys.Date(). For example, save it as “migration_plot_YYYY-MM-DD.png”. \n\n\nSolution: Saving Plot\n# First, assign the migration plot code to a variable\nmigration_plot &lt;- ggplot(data = migration_prop_ordered, aes(x = migration_prop, y = county)) +\n  geom_col(fill = \"skyblue\") +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(\n    title = \"Share of Migrants by County (2019 Census)\",\n    x = \"Proportion of Residents Born Elsewhere\",\n    y = \"County\"\n  ) +\n  theme_bw()\n\n# Now, save the plot\nggsave(filename = paste0(\"migration_plot_\", Sys.Date(), \".png\"), plot = migration_plot, width = 8, height = 10) # Adjust width/height as needed\n\n# Note: This saves the file to your current working directory in RStudio.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Application 2: Plotting</span>"
    ]
  },
  {
    "objectID": "application_1.html#a-real-world-example",
    "href": "application_1.html#a-real-world-example",
    "title": "7  Application 1: Data Manipulation",
    "section": "",
    "text": "Breakdown of Kenya by religious belief\nShare of people who are migrants in each county\nAverage working hours for men and women in rural vs. urban areas.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Application 1: Data Manipulation</span>"
    ]
  }
]