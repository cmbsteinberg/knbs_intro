[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KNBS: Introduction to R",
    "section": "",
    "text": "1 R Curriculum\nThis comprehensive curriculum is designed to take KNBS employees from novice R users to advanced practitioners, equipping them with the skills necessary to perform sophisticated data analysis and programming tasks. The curriculum is structured into three distinct levels - Beginner, Intermediate, and Advanced - each building upon the knowledge gained in the previous level.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#session-aims",
    "href": "index.html#session-aims",
    "title": "Introduction to R - KNBS",
    "section": "1.1 Session aims",
    "text": "1.1 Session aims\n\nnavigate the R and R Studio environment\nunderstand and use the common R functions for data manipulation\nunderstand the basics of data visualisation using the ggplot2 package\nunderstand the term tidy data and why it is important for writing efficient code",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#what-is-r",
    "href": "index.html#what-is-r",
    "title": "Introduction to R - KNBS",
    "section": "1.2 What is R?",
    "text": "1.2 What is R?\nR is an open-source programming language and software environment, designed primarily for statistical computing. It has a long history - it is based on the S language, which was developed in 1976 in Bell Labs, where the UNIX operating system and the C and C++ languages were developed. The R language itself was developed in the 1990s, with the first stable version release in 2000.\nR has grown rapidly in popularity particularly in the last five years, due to the increased interest in the data science field. It is now a key tool used by analysts in governments globally.\nSome of the advantages:\n\nIt is popular - there is a large, active and rapidly growing community of R programmers, which has resulted in a plethora of resources and extensions.\nIt is powerful - the history as a statistical language means it is well suited for data analysis and manipulation.\nIt is extensible - there are a vast array of packages that can be added to extend the functionality of R, produced by statisticians and programmers around the world. These can range from obscure statistical techniques to tools for making interactive charts.\nIt’s free and open source - a large part of its popularity can be owed to its low cost, particularly relative to proprietary software such as SAS or STATA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#introducing-rstudio",
    "href": "index.html#introducing-rstudio",
    "title": "Introduction to R - KNBS",
    "section": "1.3 Introducing RStudio",
    "text": "1.3 Introducing RStudio\nRStudio is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with. RStudio has four main panels;\n\nScript Editor (top left) - used to write and save your code, which is only run when you explicitly tell RStudio to do so.\nConsole (bottom left) - all code is run through the console, even the code you write in the script editor is sent to the console to be run. It’s perfect for quickly viewing data structures and help for functions but should not be used to write code you want to save (that should be done in the script editor).\nEnvironment (top right) - all data, objects and functions that you have read in/created will appear here.\nFiles/Plots/Help (bottom right) - this pane groups a few miscellaneous areas of RStudio.\n\nFiles acts like the windows folder to navigate between files and folders.\nPlots shows any graphs that you generate.\nPackages let’s you install and manage packages currently in use.\nHelp provides information about packages or functions, including how to use them.\nViewer is essentially RStudio’s built-in browser, which can be used for web app development.\n\n\nYou may have noticed that your Script Editor is bigger than the Console or your Environment has suddenly disappeared. In RStudio, you can adjust the size of different panes by clicking and dragging the dividers between them. If you want to maximize a specific pane, such as the Script Editor, use the shortcut Ctrl + Shift + 1 (Windows/Linux) or Cmd + Shift + 1 (Mac) to focus on it. To restore the default layout, press Ctrl + Shift + 0 (Windows/Linux) or Cmd + Shift + 0 (Mac). You can also use the View menu to toggle different panes on and off, ensuring your workspace suits your needs.\nIf you find the text difficult to read or prefer a different appearance, you can customise the theme, font, and text size in RStudio. Go to Tools &gt; Global Options &gt; Appearance, where you can choose from different editor themes (e.g., light or dark mode), adjust the font type, and increase or decrease the text size for better readability. These changes can help make coding more comfortable, especially during long sessions.\n\n1.3.1 Recommended Changes\nWhile not necessary, certain changes are almost always recommended for visibility reasons. These include: - Choosing a different theme, as Textmate can be hard on the eyes. This can be done in Tools &gt; Global Options &gt; Appearance &gt; Editor theme:. - Highlight R function calls. This makes functions look a different colour than normal text, which can make reading your code much easier. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Highlight R function calls. - Use Rainbow Parenthesis. This makes each pair of () in a line a different colour, which can help you catch if you’re missing one and it’s breaking your code. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Use rainbow parenthesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#basic-syntax",
    "href": "index.html#basic-syntax",
    "title": "Introduction to R - KNBS",
    "section": "1.4 Basic Syntax",
    "text": "1.4 Basic Syntax\n\n1.4.1 Exercise\n −+ 03:00 \nAs a quick exercise, try out some arithmetic in your console:\n\n25 * 15\n(45 + 3) ^ 2\n78 / 4\n\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n\nSolution\n\n\n\n25 * 15\n\n[1] 375\n\n(45 + 3) ^ 2\n\n[1] 2304\n\n78 / 4\n\n[1] 19.5\n\n\n\n\n\n\n1.4.2 The assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\n1.4.3 Exercise\n −+ 05:00 \n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\nSolution\n\n\n\nx1 &lt;- 14\nx1\n\n[1] 14\n\n\n\nx2 &lt;- x1 + 7\nx2\n\n[1] 21\n\n\n\nx3 &lt;- x2 / 3\nx3\n\n[1] 7\n\n\n\n\n\n\n1.4.4 Combining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\n1.4.5 Exercise\n −+ 05:00 \n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n\n\n\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n[1] 1 2 3\n\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x10a8e78b8&gt;\n&lt;environment: namespace:lubridate&gt;\n\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "Introduction to R - KNBS",
    "section": "1.5 Functions",
    "text": "1.5 Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\n1.5.1 Exercise\n −+ 05:00 \n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n\n\n\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n[1] 36",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Introduction to R - KNBS",
    "section": "1.6 Packages",
    "text": "1.6 Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "index.html#the-tidyverse",
    "href": "index.html#the-tidyverse",
    "title": "Introduction to R - KNBS",
    "section": "1.7 The Tidyverse",
    "text": "1.7 The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\n1.7.1 Exercise\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\n\n\n\nlibrary(tidyverse)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html",
    "href": "data_manipulation.html",
    "title": "3  Manipulating Data",
    "section": "",
    "text": "3.1 Select\nIn this chapter, we will cover dplyr, one of the most essential packages in an R user’s toolkit. As a key part of the tidyverse, dplyr offers easy-to-use functions for manipulating data frames, which is a vital step in the data analysis process.\nTo illustrate the key functions of dplyr, we’ll be using the gapminder dataset. You can view this dataset by installing and loading the gapminder package, just as you did with tidyverse.\nThe exercises in this chapter will use an inbuilt R dataset. However, if you’d like to follow along with the examples, you’re welcome to load the gapminder dataset, although please note it will not be required for the exercises themselves.\nThere are six key dplyr functions that allow you to solve the vast majority of data-manipulation challenges;\nThese functions look similar to SQL statements and are designed to replace the need for any data manipulation in SQL.\nAll dplyr functions allow you to specify the column names without “quotations”. However, if there are spaces in the column name, you’ll need to use `back ticks`.\nSelect allows you to choose the columns that you’d like to keep from a dataset.\n?select\nLooking at the gapminder dataset, if we want to create a new dataset which only included the year, country and life expectancy, we could do this by selecting those columns:\ngapminder_life_exp &lt;- select(gapminder, year, country, lifeExp)\nThe first argument within the select command specifies use of the gapminder dataset. Following this we list the variables we want to keep.\nIt is also possible to select to exclude specific columns. This is ideal if you want to keep all columns except for one or two, and can be done by using a - minus sign in front of column names.\n#Removes the continent column but keeps all others\ngapminder_no_continent &lt;- select(gapminder, -continent)\nYou can also use the select function to reorder columns as it will select columns in the order specified.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#select",
    "href": "data_manipulation.html#select",
    "title": "3  Manipulating Data",
    "section": "",
    "text": "3.1.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUsing the inbuilt dataset airquality, select to keep the columns Ozone, Temp, Month and Day.\n\n\n\nSolution\nairquality_exercise &lt;-\n  select(airquality, Ozone, Temp, Month, Day)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#pipes",
    "href": "data_manipulation.html#pipes",
    "title": "3  Manipulating Data",
    "section": "3.2 Pipes",
    "text": "3.2 Pipes\nBefore we continue, let’s visit one of the most important (and cool) operators in R… the pipe |&gt;. You may have seen the old pipe %&gt;% before. This has been replaced with a native pipe in R 4.1 onwards. It’s likely that you’ll want to use multiple functions consecutively, especially when using dplyr. Currently, we may do something like this:\n\ngapminder_new &lt;- select(gapminder, -continent)\ngapminder_new &lt;- select(gapminder_new, year, country, lifeExp)\n\nThis code is a little frustrating to write because we have to name each intermediate data frame, even though we don’t care about it. Naming things is hard, and having separate names for each step makes it difficult to read. Let’s see how we can rewrite this code using the pipe:\n\ngapminder_new &lt;- gapminder |&gt;\n select(-continent) |&gt; \n select(year, country, lifeExp)\n\nThe pipe means we can read this code as a series of statements separated by the pipe representing “and then”; e.g. take the gapminder data and then remove the continent column and then select the year, country and lifeExp columns.\nYou may notice that we don’t need to specify the data argument in each function when using |&gt;. By piping, the subsequent function recognises we want to use the result of our previous statement as our data.\nYou can quickly insert the pipe operator in RStudio by pressing Ctrl + Shift + M (Windows) or Cmd + Shift + M (Mac). This will still insert the old pipe %&gt;%. This can be changed in Tools &gt; Global Options &gt; Code &gt; Editing &gt; Use native pipe operator, |&gt;",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#grouping-and-summarising-data",
    "href": "data_manipulation.html#grouping-and-summarising-data",
    "title": "3  Manipulating Data",
    "section": "3.3 Grouping and summarising data",
    "text": "3.3 Grouping and summarising data\nWe can produce breakdowns of statistics using the group_by and summarise commands from the dplyr package:\n\ngroup_by() identifies which variables we want to produce breakdowns by.\nsummarise() is used to indicate which values we want to calculate.\n\nUsing these functions together we can produce summary statistics in a similar way to pivot tables in Excel. We can use the pipe (|&gt;) operator to chain these functions together.\nSo if we want the mean life expectancy by continent and year:\n\nmean_life_exp &lt;- gapminder |&gt;\n  group_by(year, continent) |&gt;\n  summarise(life_exp = mean(lifeExp))\n\nHere R takes the dataset, then groups it first by year and then by continent and then outputs the mean life expectancy. The mean life expentancy variable is created as a new column called life_exp. The results are saved into a new dataset called mean_life_exp.\nThere are other functions that could be used here instead of mean e.g. n, n_distinct, min, max, mean, median, var and sd.\nIf we want to add a new variable that we decide to call country_count that provides the counts by year and continent we can rerun as follows using the pipe operator:\n\nmean_life_exp &lt;- gapminder |&gt;\n  group_by(year, continent) |&gt;\n  summarise(life_exp = mean(lifeExp), country_count = n())\n\n\n3.3.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nUsing the pipe function, group the airquality dataset by month.\nSummarise the grouped dataset to produce an average of Ozone and Temp by month.\nAssign this to an object called airquality_summarised.\n\n\n\nSolution\nairquality_summarised &lt;- airquality |&gt;\n  group_by(Month) |&gt;\n  summarise(avg_ozone = mean(Ozone, na.rm = TRUE),\n            avg_temp = mean(Temp, na.rm = TRUE))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#filter",
    "href": "data_manipulation.html#filter",
    "title": "3  Manipulating Data",
    "section": "3.4 Filter",
    "text": "3.4 Filter\nIf you would like to produce statistics for a subset of rows or observations, a good function to use is filter() from the dplyr package.\nLet’s first take a look at the different possible values of the continent variable. We can do that quickly using the group_by/summarise combination.\n\ngapminder |&gt;\n  group_by(continent) |&gt;\n  summarise(count = n())\n\nTo filter we just specify the data that we want to filter (gapminder) and the value that we want to filter on. In this case lets filter where continent is “Asia” and year is after 1992 then recalculate the mean life expectancy by country:\n\nmean_life_exp &lt;- gapminder |&gt; \nfilter(continent == \"Asia\" & year &gt; 1992) |&gt;\ngroup_by(country) |&gt;\nsummarise(life_exp = mean(lifeExp))\n\nR provides the standard suite of comparison operators which can be used to filter:\n\n\n\nComparison\nOperator\n\n\n\n\nGreater than\n&gt;\n\n\nGreater than or equal to\n&gt;=\n\n\nLess than\n&lt;\n\n\nLess than or equal to\n&lt;=\n\n\nEqual to\n==\n\n\nNot equal to\n!=\n\n\nAnd\n&\n\n\nOr\n|\n\n\nNot\n!\n\n\nGroup membership\n%in%\n\n\n\nThe %in% operator allows you to compare a column against a vector of values to see if it matches any one of them; this is much more convenient than comparing against each value individually.\n\n##This does work to filter the data for the three given years but is clunky to read and edit\ngapminder |&gt; \nfilter(year == 1992 | year == 1998 | year == 2002) \n\n##Using the %in% operator is simple and clean to read, and gives exactly the same result\ngapminder |&gt; \nfilter(year %in% c(1992, 1998, 2002)) \n\n\n3.4.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nFilter the original airquality dataset to only include data for May and June. Try to do this using the %in% function.\nCall this assignment: airquality_filter.\n\n\n\nSolution\nairquality_filter &lt;- airquality |&gt;\n  filter(Month %in% c(5, 6))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#rename",
    "href": "data_manipulation.html#rename",
    "title": "3  Manipulating Data",
    "section": "3.5 Rename",
    "text": "3.5 Rename\nWe can rename variables using the dplyr function rename(). Let’s amend our previous code creating the mean_life_exp dataset to change the name of the “year” column to “selected_year”.\n\nmean_life_exp &lt;- gapminder |&gt; \n  filter(continent == \"Asia\" & year &gt; 1992) |&gt;\n  group_by(year, country) |&gt;\n  summarise(life_exp = mean(lifeExp)) |&gt;\n  rename(selected_year = year)\n\nWithin the rename function, the new name “selected_year” is specified on the left and the old name on the right of the equal sign.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#mutate",
    "href": "data_manipulation.html#mutate",
    "title": "3  Manipulating Data",
    "section": "3.6 Mutate",
    "text": "3.6 Mutate\nYou can create new columns and perform calculations on existing columns using the dplyr command mutate().\n\n?mutate\n\nFor example, imagine we wanted to calculate overall GDP as a new column in the gapminder dataset. We could do this by multiplying the gdpPercap and pop columns:\n\ngapminder |&gt;\n  mutate(gdp_total = gdpPercap * pop)\n\nYou can also use functions like mean() and sum() in mutate(). For example, using x / sum(x) for calculating proportions of a total and y - mean(y) for difference from the mean.\nNotice that by default, mutate calculates values on a rowwise basis; each value in the gdp_total column is made by multiplying the values in the corresponding row. This default behaviour can be changed by grouping data before mutate, e.g. this code produces a mean population column by country:\n\ngapminder |&gt;\n group_by(country) |&gt;\n mutate(mean_pop = mean(pop))\n\nYou can also combine mutate with the case_when function to perform one or more if/else conditions. Maybe we want to have coded values for each year by decade. The case_when function allows you to provide multiple instances of a statement which evaluates to TRUE/FALSE, and then a result if that condition is true (after ~). The function evaluates these statements in order, so if an earlier statement is TRUE, a later one will not be evaluated. Finally, for cases that don’t meet any of the conditions, the final TRUE value is used (this defaults to NA if not specified)\n\ngapminder |&gt;\n  mutate(\n    decade =\n      case_when(\n        year &gt;= 1950 & year &lt; 1960 ~ \"1950s\",\n        year &gt;= 1960 & year &lt; 1970 ~ \"1960s\",\n        year &gt;= 1970 & year &lt; 1980 ~ \"1970s\",\n        year &gt;= 1980 & year &lt; 1990 ~ \"1980s\",\n        year &gt;= 1990 & year &lt; 2000 ~ \"1990s\",\n        TRUE ~ \"Post-2000\"\n      )\n  )\n\nYou can download the Data Transformation Cheat Sheet (and other cheatsheets) at: https://www.rstudio.com/resources/cheatsheets/\n\n3.6.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nThe inbuilt trees dataset includes columns for tree girth and height in inches. Using mutate, create two new columns (Girth_cm and Height_cm) containing the equivalent values in centimetres.\nHow can you replace an existing column with a new column using mutate?\n\n\n\nSolution\n# 1. conversion is 2.54\n\ntrees_with_cm &lt;- trees |&gt;\n  mutate(Girth_cm = Girth * 2.54,\n         Height_cm = Height * 2.54)\n\n# 2. Using mutate to replace existing column\n\ntrees_with_cm &lt;- trees |&gt;\n  mutate(Girth = Girth * 2.54)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#arrange",
    "href": "data_manipulation.html#arrange",
    "title": "3  Manipulating Data",
    "section": "3.7 Arrange",
    "text": "3.7 Arrange\narrange() is used to change the order of rows. It takes a data frame as it’s first argument and a column name to sort by as it’s second. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns. By default arrange() will sort in ascending order (1-9 and A-Z). If you’d like to sort in descending order wrap the column name in desc(). Using arrange with one column sorts how you’d might expect:\n\ngapminder |&gt;\n arrange(year)\n\nSorting with multiple columns sorts within the hierarchy specified:\n\ngapminder |&gt;\n  arrange(year, desc(continent))\n\nIt’s worth noting that missing values (NA) are always sorted at the end:\n\ndf &lt;- tibble(x = c(1, 2, 3, NA))\narrange(df, x)\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1     1\n2     2\n3     3\n4    NA\n\narrange(df, desc(x))\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1     3\n2     2\n3     1\n4    NA\n\n\n\n3.7.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nArrange the rows of the trees dataset by increasing height and decreasing girth.\nWhat do you notice?\n\n\n\nSolution\ntrees_arranged &lt;- trees |&gt;\n  arrange(Height, desc(Girth))",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Manipulating Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html",
    "href": "importing_data.html",
    "title": "3  Importing Data",
    "section": "",
    "text": "3.1 Local Reading\nSo far, we have only made use of data which is pre-loaded into R via packages, but it is also possible to load your own data in from a variety of sources. We will focus on two different file types;\nAs described in Chapter 1, the bottom right pane of RStudio allows you to view files that are within your own personal filesystem. You are free to create new folders in this area, using the New Folder button.\n−+\n05:00\nGreat, we will come back to using these two files, but first let’s discuss how to read in csv files.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#local-reading",
    "href": "importing_data.html#local-reading",
    "title": "3  Importing Data",
    "section": "",
    "text": "Navigate to the Files at the bottom right pane of your RStudio\nCreate a new folder called data\nSave the GCP dataset to this folder as gcp.csv",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#csv",
    "href": "importing_data.html#csv",
    "title": "3  Importing Data",
    "section": "3.2 CSV",
    "text": "3.2 CSV\nAlthough there is a read.csv() function in base R, like most things there is a better tidyverse alternative! read_csv() from the readr package reads CSVs in as a tibble (which has additional features compared to a standard data frame), is much faster (~10X), and allows you to specify how you read data in more easily.\nAs always, let’s read the function documentation using ?read_csv. This tells us we need to provide a path to the file. This path can be either local or remote; so it will work equally well for data inside your project or from the internet.\nTo read in a local file, you have to specify the exact location of the file. You can do this as either an absolute filepath, which starts from the drive name right through to the final file (e.g. C:/Documents/My_work/file.csv), or as a relative file path. A relative file path just gives the location of the file starting from your current working environment. You can check what your current working environment is using the command getwd(). The advantage of using relative file paths is if someone duplicates your project from Github, the code will still work on their own computer.\nWe will start by reading in some local data, which contains details of Kenyan Gross County Product by economic activity for 2017:\n\ngcp &lt;- read_csv(\"data/gcp.csv\")\n\nNotice that the file is inside the data folder inside the current working directory.\ngcp will now show in your environment. The environment viewer (top right) shows you basic information about the data that has been loaded in. You can also click on any object to view it in your script window.\nYou can also read in data directly from the web using the same function. For example, reading in this CSV of Eurovision Song Contest data:\n\neurovision_data &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-17/eurovision.csv\")\n\nRows: 2005 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): event, host_city, host_country, event_url, section, artist, song, ...\ndbl  (4): year, running_order, total_points, rank\nlgl  (2): qualified, winner\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis works exactly the same way as reading in local data, and the object you have created will appear in your environment (top right).\n\n3.2.1 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nRead in the frogs dataset found here:‘https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv’ as an object called frogs\n\n\n\nSolution\nfrogs &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-08-02/frogs.csv\")\n\n\n\n\n3.2.2 Inspecting the dataset\nAs noted in the previous section, you can see by looking in the environment window that the eurovision dataset has 2005 observations and 18 variables. You can also return this information (and more) about datasets programatically, using the glimpse() function, again from the dplyr package:\n\nglimpse(eurovision_data)\n\nRows: 2,005\nColumns: 18\n$ event          &lt;chr&gt; \"Turin 2022\", \"Turin 2022\", \"Turin 2022\", \"Turin 2022\",…\n$ host_city      &lt;chr&gt; \"Turin\", \"Turin\", \"Turin\", \"Turin\", \"Turin\", \"Turin\", \"…\n$ year           &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2…\n$ host_country   &lt;chr&gt; \"Italy\", \"Italy\", \"Italy\", \"Italy\", \"Italy\", \"Italy\", \"…\n$ event_url      &lt;chr&gt; \"https://eurovision.tv/event/turin-2022\", \"https://euro…\n$ section        &lt;chr&gt; \"first-semi-final\", \"first-semi-final\", \"first-semi-fin…\n$ artist         &lt;chr&gt; \"Kalush Orchestra\", \"S10\", \"Amanda Georgiadi Tenfjord\",…\n$ song           &lt;chr&gt; \"Stefania\", \"De Diepte\", \"Die Together\", \"Saudade, Saud…\n$ artist_url     &lt;chr&gt; \"https://eurovision.tv/participant/kalush-orchestra-22\"…\n$ image_url      &lt;chr&gt; \"https://static.eurovision.tv/hb-cgi/images/963164d0-06…\n$ artist_country &lt;chr&gt; \"Ukraine\", \"Netherlands\", \"Greece\", \"Portugal\", \"Bulgar…\n$ country_emoji  &lt;chr&gt; \":flag_ua:\", \":flag_nl:\", \":flag_gr:\", \":flag_pt:\", \":f…\n$ running_order  &lt;dbl&gt; 6, 8, 15, 10, 7, 5, 17, 16, 3, 9, 4, 14, 11, 1, 12, 2, …\n$ total_points   &lt;dbl&gt; 337, 221, 211, 208, 29, 15, 187, 177, 159, 154, 118, 10…\n$ rank           &lt;dbl&gt; 1, 2, 3, 4, 16, 17, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ rank_ordinal   &lt;chr&gt; \"1st\", \"2nd\", \"3rd\", \"4th\", \"16th\", \"17th\", \"5th\", \"6th…\n$ qualified      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE,…\n$ winner         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n\n\nAs well as returning the number of rows and columns in the data, the glimpse function also shows you the names of the columns, the column classes (indicated in ), and an example of the first few rows of data.\n\n\n3.2.3 Exercise\n\n\n\n−+\n05:00\n\n\n\n\nUse the glimpse and View functions to examine the frogs dataset. How many rows and columns does it have?\n\n\n\nSolution\nView(frogs)\n\nglimpse(frogs)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#excel",
    "href": "importing_data.html#excel",
    "title": "3  Importing Data",
    "section": "3.3 Excel",
    "text": "3.3 Excel\nReading excel files works in much the same way as CSV files. However, due to the difference in underlying structures we require the function read_excel() from a different package called readxl.\nThe main difference when reading excel files is three additional arguments that we can set;\n\nsheet which allows us to specify which sheet to read. It can take the form of a string (the name of the sheet) or an integer (the position of the sheet); and\nrange which allows us to specify a cell range. It takes a typical cell reference like “B3:D10”.\nskip an alternative to specifying a cell range, you can simply indicate how many rows to skip at the start of the sheet. This is ideal if you want to read in a sheet with an unknown number of columns and/or rows, but know there are several lines of metadata at the top of the sheet.\n\nIf we don’t set any of these arguments it will assume our data is in the first row of the first sheet (and it becomes almost identical to read_csv above).\n\n# One option is to download the file \nurl = \"https://www.knbs.or.ke/wp-content/uploads/2024/04/2023-Economic-Survey-Kenya-Tourism-Sector.xlsx\"\ndownload_first &lt;- download.file(,destfile = \"tourism.xlsx\")\ntourism &lt;- read_excel(\"tourism.xlsx\")\n\n\n3.3.1 Exercise\n\n\n\n−+\n10:00\n\n\n\n\nInstall and load the readxl package.\nTry downloading an excel spreadsheet from KNBS. For example: “https://www.knbs.or.ke/wp-content/uploads/2024/04/2023-Economic-Survey-Kenya-Tourism-Sector.xlsx”\nSpecify the sheet name you want to read in.\nExamine the data you have read in; are the column names what you want? Work out how to skip these and only read in the data, with the correct column names.\n\n\n\nSolution\nurl = \"https://www.knbs.or.ke/wp-content/uploads/2024/04/2023-Economic-Survey-Kenya-Tourism-Sector.xlsx\"\ndownload_first &lt;- download.file(url,destfile = \"data/tourism.xlsx\")\ntourism &lt;- read_excel(\"data/tourism.xlsx\", sheet = \"Table 12.5\", skip = 2)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "importing_data.html#rio",
    "href": "importing_data.html#rio",
    "title": "3  Importing Data",
    "section": "3.4 Rio",
    "text": "3.4 Rio\nSometimes you may want to read a selection of files of all different types. This is where Rio can come in handy. Rio is a wrapper around the libraries we’ve used above and many more, which lets you use import() to read almost any file in. This isn’t always useful, when you want to do very specific things with a certain file, but can be much cleaner.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "ggplot.html",
    "href": "ggplot.html",
    "title": "5  Basic Plotting",
    "section": "",
    "text": "5.1 Structure\nThis chapter will teach you how to visualise your data using ggplot2. R has several systems for making graphs, but ggplot2 is the most elegant and versatile. The syntax behind ggplot2 looks complicated at first, but once you understand it, it’s incredibly powerful and can be used to visualise a wide range of data.\nThe main function in ggplot2 is ggplot() which is used to initialise a plot. A plot in ggplot2 is made up of multiple elements added to each other to create layers which each add something to the appearance of the chart. The basic template for a graph is as follows:\nggplot(data = &lt;DATA&gt;) +\n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\nA geom function defines the way data and an aesthetic mapping is statistically transformed to create a plot. A plot can come in many forms, such as a bar graph, line and scatter graph, to name a few.\nA ggplot object must contain\nThis might look confusing initially, so let’s show an example with one of the pre-loaded R datasets mpg by creating a scatter plot of displacement against hwy.\n#Data to be plotted\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  #The geometric to draw the aesthetics with (in this case a point geom)\n     #The aesthetic mapping; the x axis to displacement and the y to hmwy\n  geom_point()\nThis is the basic structure of any ggplot chart, but there are plenty of things you can do to change the appearance and function of your charts within ggplot.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#structure",
    "href": "ggplot.html#structure",
    "title": "5  Basic Plotting",
    "section": "",
    "text": "the data to be plotted as the first argument\nhow that data should be mapped to the different aspects of the plot, defined using aes() (short for aesthetics).\na geometric to draw the aesthetics with\nggplot works with layers, each added with the + operator.\nMappings are always added using the aes() command, which can be inside the ggplot() or geom.\n\n\n\n\n\n5.1.1 Exercise\n −+ 10:00 \n\nUse the iris dataset (another built-in dataset in R) to create a simple scatter (geom_point) chart, plotting Sepal.Length as the x axis and Sepal.Width as the y\n\n\n\n3.1.1. Solution\n\n\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#types-of-geom-functions",
    "href": "ggplot.html#types-of-geom-functions",
    "title": "5  Basic Plotting",
    "section": "5.2 Types of Geom Functions",
    "text": "5.2 Types of Geom Functions\nYou aren’t just limited to scatter plots; there are lots of geoms available in ggplot - the best resource for choosing an appropriate geom is the cheat sheet. This can be found at https://github.com/rstudio/cheatsheets/blob/main/data-visualization-2.1.pdf\nThe most commonly used geoms are:\n\n\n\nGeom Function\nDescription\n\n\n\n\ngeom_bar\nBar chart\n\n\ngeom_point\nScatter chart\n\n\ngeom_line\nLine graph\n\n\ngeom_histogram\nHistogram\n\n\ngeom_boxplot\nBox and whisker plot\n\n\ngeom_smooth\nLine of best fit style overlay\n\n\n\nYou can also add multiple geoms to a single plot, for example you can add a smoothed line to the scatter plot you have already created using geom_smooth. You can either define the aes in each of the geom calls if they are different for each layer, or define them in the initial ggplot call if they are consistent across all layers.\n\n#Aes defined in ggplot calll\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  geom_point() + #Add a + sign\n  geom_smooth()#Include a smoothed line\n\n\n\n\n\n\n\n\n\n5.2.1 Exercise\n −+ 05:00 \n\nUse the geom_smooth aesthetic to add a smoothed line to your scatter plot.\n\n\n\n3.2.1. Solution\n\n\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point() +\n  geom_smooth()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-different-aesthetics",
    "href": "ggplot.html#adding-different-aesthetics",
    "title": "5  Basic Plotting",
    "section": "5.3 Adding different aesthetics",
    "text": "5.3 Adding different aesthetics\nIt’s normal that you will want to explore more than two variables within your datasets. You can do this by mapping those variables to different aspects of the chart in ggplot; things like colour, point shape, or line type.\nFor example, we could set the colour of the point to be determined by the vehicle class.\n\n# Aesthetics\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()\n\n\n\n\n\n\n\n\n\nggplot does some clever things when deciding what colours to use - for factorial variables it will assign each factor a unique colour (as in the above example), whilst for continuous variables it will assign a colour scale.\n\n# Here year is coloured as a continuous variable with a colour scale\nggplot(data = mpg, aes(x = displ, y = hwy, colour = year))+\n  geom_point()\n\n\n\n\n\n\n\n# Here by setting year to a factor it is coloured as a discrete variable with a unique colour for each\nggplot(data = mpg, aes(x = displ, y = hwy, colour = factor(year)))+\n  geom_point()\n\n\n\n\n\n\n\n\nThere are a wide range of other aesthetics you can set to indicate different categories including:\n\nPoint shape (shape)\nLine type (linetype)\nSize of points (size)\nTransparancy of points (alpha)\n\nApplying multiple aesthetics should be used with caution though; indicating more than one variable using aesthetics can quickly make a chart difficult to read!\n\n# A chart wit multiple aesthetics applied.\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class, size = cty))+\n  geom_point(shape = 5)\n\n\n\n\n\n\n\n\nYou also don’t have to map aesthetics onto variables; you can specify them manually if you don’t want them to be related to a variable. To do this, you need to specify the colour, shape, linetype, etc outside of the aesthetic call. For example, you can define the colour of the points:\n\n#Aesthetics related to variables are mapped inside the aes call\nggplot(data = mpg, aes(x = displ, y = hwy))+\n  #Aesthetics that are manually set are mapped outside the aes call\n  geom_point(colour = \"orange\")\n\n\n\n\n\n\n\n\n\n5.3.1 Exercise\n −+ 10:00 \n\nMap the colour aesthetic of your chart to correspond to the Species in the iris dataset.\nManually map the shape of the geom_point to be type 3\n\n\n\n3.3.1. Solution\n\n\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point(shape = 3)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-layers",
    "href": "ggplot.html#adding-layers",
    "title": "5  Basic Plotting",
    "section": "5.4 Adding Layers",
    "text": "5.4 Adding Layers\nThis produces the basics of any ggplot2 chart, however it doesn’t always make the most attractive chart. To improve the appearance of the chart, the ggplot2 package has a wide range of functions which can be added to your basic chart to change everything from the legend, titles, or scales shown in the chart.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#scales",
    "href": "ggplot.html#scales",
    "title": "5  Basic Plotting",
    "section": "5.5 Scales",
    "text": "5.5 Scales\nChanging the x and y axes can be done using the scale_x_ and scale_y_ group of functions. There is a different type of these functions for each different type of scale and axis, and you need to take care you use the right one in each case!\n\n##For a continuous Y axis\nggplot(data, aes(x = x_axis, y = y_axis))+\n  scale_y_continuous()\n\n##For dates on the X axis\nggplot(data, aes(x = x_axis, y = y_axis))+\n  scale_x_date()\n\nAn example of using a percent scale:\n\n# Scales \nggplot(data = mpg1, aes(x = displ, y = gallon_percent, colour = class))+\n  geom_point()+\n  #Set name for axis\n  scale_y_continuous(labels = scales::label_percent())\n\n\n\n\n\n\n\n\nYou can change a large number of aspects of both the appearance and function of the axes using these functions, including:\n\nName on the axis\nChange the minimum and maximum values on the scale\nSet major and minor values on the scale\nPosition of the axis\nType-specific changes such as setting the appearance of dates or transforming to log scale\n\n\n# Aesthetics\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  #Set name for axis\n  scale_x_continuous(name = \"displacement\",\n                     #Set min and max limits\n                     limits = c(0,8))\n\n\n\n\n\n\n\n\nCheck the arguments available for any scale function using ? in front of it in the console; e.g. ?scale_x_date",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#changing-colour-palettes",
    "href": "ggplot.html#changing-colour-palettes",
    "title": "5  Basic Plotting",
    "section": "5.6 Changing colour palettes",
    "text": "5.6 Changing colour palettes\nIf you don’t specify colours to use, ggplot will default to the (relatively ugly) standard palette. Luckily, there are loads of ways to easily choose more attractive colour options!\nNote that when you are changing colours in a chart, there are two different options; colour is used for points and lines in charts, while fill is for the central fill colour in objects like bars. Make sure you use the right one when calling scale arguments!\nUsing scale_colour_brewer() or scale_fill_brewer() allows you to select from one of the ColorBrewer palettes; these are designed to be attractive, and many of them are colour-blind friendly.\n\n#Chart using the standard colour brewer palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer()\n\n\n\n\n\n\n\n\nChange the palette used with the palette argument:\n\n#Chart using the Dark2 palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nYou can see the full range of palettes available with their names here:\n\n\n\n\n\n\n\n\n\nYou can also design your own custom palettes using either named colours or hex codes and pass them to your charts using the scale_*x*_manual functions:\n\n#Chart using a custom defined palette\n\nmy_cols &lt;- c(\"#DAF7A6\", \"#CCDC6D\", \"#FFC300\", \"#FF5733\", \"#C70039\", \"#900C3F\", \"#581845\")\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_manual(values = my_cols)\n\n\n\n\n\n\n\n\n\n5.6.1 Exercise\n −+ 10:00 \n\nChange the colour palette your chart uses to something you like better than the default! Use the scale_colour_brewer or scale_colour_manual to do this.\n\n\n\n3.6.1. Solution\n\n\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point(shape = 3) +\n  scale_colour_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#facets",
    "href": "ggplot.html#facets",
    "title": "5  Basic Plotting",
    "section": "5.7 Facets",
    "text": "5.7 Facets\nFaceting charts in R is a good way to produce multiple identical charts; this feature splits data by a provided variable and plots one value per chart. It is very useful when overlapping data is difficult to read. Using the facet_wrap() function, you can pass any variable to the first argument (prefacing it with ~), as well as specifying the row/column layout of the result\n\n#Chart using the Dark2 palette\n\nggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  #Facet by class\n  facet_wrap(~class)+\n  scale_colour_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#titles",
    "href": "ggplot.html#titles",
    "title": "5  Basic Plotting",
    "section": "5.8 Titles",
    "text": "5.8 Titles\nLabels and titles can be added without changing the axes using the labs command.\n\n#Themes, titles, and multiple plots\nggplot(data = mpg, aes(x = class, y =..prop.., group = 1))+\n  geom_bar()+\n  labs(title = \"Proportion of sample by class\", x = \"Class\", y = \"Proportion\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#adding-themes",
    "href": "ggplot.html#adding-themes",
    "title": "5  Basic Plotting",
    "section": "5.9 Adding themes",
    "text": "5.9 Adding themes\nChanging the theme is a quick and easy way to set many of the visual aspects of your charts, such as the appearance of grid lines, size of text, and position of the legends. You can change the theme to a number of presets:\n\n\nplot &lt;- ggplot(data = mpg, aes(x = displ, y = hwy, colour = class))+\n  geom_point()+\n  scale_colour_brewer(palette = \"Dark2\")\n\n#Applying different themes\n plot+theme_bw() \n\n\n\n\n\n\n\n plot+theme_classic() \n\n\n\n\n\n\n\n plot+theme_minimal()\n\n\n\n\n\n\n\n plot+theme_light()\n\n\n\n\n\n\n\n\n\nYou can also make your own custom themes; plot are made up of four elements element_text, element_line, element_rect, and element_blank. Plots can be modified using these element commands. For example:\n\n#You can also make your own custom themes\n#\nugly.theme &lt;-\n  theme(\n    text = element_text(colour ='orange', face ='bold'),\n    panel.grid.major = element_line(colour = \"violet\", linetype = \"dashed\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = 'black', colour = 'red')\n  )\n\n\n\nplot+ugly.theme\n\n\n\n\n\n\n\n\n\n5.9.1 Exercise\n −+ 10:00 \n\nAdd one of the default themes to your chart to improve its appearance.\nAdd a title and labels to your axes\n\n\n\n3.9.1. Solution\n\n\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +\n  geom_point() +\n  #Add title and labels to axes\n  labs(title = \"Chart\", x = \"Sepal.Length\", y = \"Sepal.Width\") +\n  scale_colour_brewer(palette = \"Dark2\") +\n  #Add in-built R theme\n  theme_bw()",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "ggplot.html#saving-plots",
    "href": "ggplot.html#saving-plots",
    "title": "5  Basic Plotting",
    "section": "5.10 Saving plots",
    "text": "5.10 Saving plots\nMost of the time you will want to create plots directly into an R Markdown output, or a shiny app. However plots can also be saved as image (png) file:\n\n‘Export’ button in RStudio viewer\nggsave(filename = “plotname.png”, plot = myplot) - saves the plot into your current working directory in R Studio. Can then be downloaded from the platform via ‘More’ -&gt; ‘Export…’\n\n\n5.10.1 Saving a plot with today’s date\nSomething that has been raised in this section is: what if I want to add today’s date in the filename when saving a plot? This can be useful for organising and tracking plots over time. To do this, you can use the Sys.Date() function in R, which returns the current date in YYYY-MM-DD format.\nFor example, if you wanted to save a plot with today’s date included in the filename, you could use:\n\nggsave(filename = paste0(\"plot_\", Sys.Date(), \".png\"), plot = myplot)\n\nThis will save the plot myplot as a PNG file with a filename that includes the current date, like plot_2025-03-07.png, where Sys.Date() provides today’s date in the format YYYY-MM-DD. It’s a great way to ensure your plot files are uniquely named based on the date.\n\nThis is a good opportunity to take a 10-minute break away from the computer to refresh your mind, stretch, and reset before continuing onto Chapter 4 and 5.",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Basic Plotting</span>"
    ]
  },
  {
    "objectID": "data_manipulation.html#application-for-knbs",
    "href": "data_manipulation.html#application-for-knbs",
    "title": "1  Data Manipulation",
    "section": "1.8 Application for KNBS",
    "text": "1.8 Application for KNBS\n −+ 45:00 \nIn this application, we will practice some of the coding skills learned in the training so far. Whereas the examples used in the textbook use toy datasets which are already clean and well-formatted, the datasets used in our applications may require some initial cleaning prior to analysis. This is likely closer to tasks you might encounter in your everyday work at KNBS.\nIn this particular application, we would like you to produce 3 simple pieces of analysis using Kenya’s 2019 census data: the breakdown of Kenya by religious belief, the share of people who are migrants in each county, and the average working hours for men and women in rural vs. urban areas.\nThere are two stages to this task. The first is to read in the data and prepare it to be analysed. This will involve reading the data in properly, fixing any column name issues, reducing the size of the dataset if it is too large, and finally dealing with any missing values, or NAs, that are found in the data.\nThe second is to perform the analysis on your dataset. This will involve creating new columns,\nThe 2019 Census data you need is found at this link\n\nRead in the dataset and use janitor to clean any column names\nWe need columns relating to migration, age, etc. Select the columns we will need - look at survey metadata\nLet’s rename some columns to make them easier to understand\nFor our religion question, let’s examine the data in this column\nHow should we approach the summary? How do we want to treat NAs, or DK\n\n\n\n\nSolution\n\n\n\nlibrary(tidyverse)\n\n# Example 1\n\n\n\n\n# Example 1\n\n## Reading and cleaning names\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\n## Creating a unique identifier, to allow us to drop columns\ncensus &lt;-  census |&gt; \n  mutate(id = paste(sublocation_code, ea, ea_type, strnumber, hhnumber, line_number, sep = \"_\"),\n         hhid = paste(sublocation_code, ea, ea_type, hhnumber, sep = \"_\"))\n\nlength(unique(census$id))\n## Dropping columns that we don't need, ro red\n\nreligion &lt;- census |&gt; \n  select(sublocation_code, ea, hhnumber, line_number, p17) |&gt; \n  rename(religion = p17) |&gt; \n\nsum(is.na(census$p19))\n\nmigration &lt;- census |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19) |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmig_nairobi &lt;- migration |&gt;\n  group_by(subcounty_code) |&gt;\n  summarise(migration_prop = mean(migrant))\n\n\nworking_hours &lt;- census |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(hours_worked_if_work = p52,\n         age = p12,\n         sex = p11) |&gt;\n  filter(age &gt;= 18) |&gt;\n  mutate(\n    rural = if_else(ea_type == 1, 1, 0),\n    female = if_else(sex == 2, 1, 0),\n    hours_worked = replace_na(hours_worked_if_work, 0)\n  )\n\n\nworking_hours_mean_adult &lt;- working_hours |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )\n\nworking_hours_mean_adult |&gt;\n  ggplot(aes(y = hours_worked,  x = as.factor(female), fill = as.factor(rural))) +\n  geom_bar(stat = \"identity\", position = \"dodge\")",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "mapping.html",
    "href": "mapping.html",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "Aims",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#aims",
    "href": "mapping.html#aims",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "Know how to load spatial data into R using the sf library.\nBe familiar with using GSS codes to join statistics to geographies.\nUnderstand how spatial objects can be manipulated using dplyr.\nUnderstand how to use spatial joins.\nBe aware of map projections and Coordinate Reference Systems (CRS) and be able to modify them.\nKnow how to make static and interactive maps in ggplot2.\nBe able to export your maps and shapefiles.",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#gis-and-r",
    "href": "mapping.html#gis-and-r",
    "title": "Introduction to GIS in R",
    "section": "GIS and R",
    "text": "GIS and R\nR is commonly used for statistical analysis and programming, however it also has a range of geospatial libraries developed by a community of researchers and programmers. In the last few years, working with spatial data became much easier in R, with the development of the sf package. sf keeps all the spatial information for each observation in a geometry column which means that we can treat it like a normal data frame and also perform spatial operations on the data.\n\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 35.81531 ymin: -3.246071 xmax: 40.08169 ymax: 4.061832\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 18\n  name   name_en amenity man_made shop  tourism opening_ho beds  rooms addr_full\n  &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n1 The S… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n2 Masji… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n3 Hotel… &lt;NA&gt;    pub     &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n4 Rosog… &lt;NA&gt;    clinic  &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n5 AIC R… &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n6 FGCK … &lt;NA&gt;    place_… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;       &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     \n# ℹ 8 more variables: addr_house &lt;chr&gt;, addr_stree &lt;chr&gt;, addr_city &lt;chr&gt;,\n#   source &lt;chr&gt;, name_sw &lt;chr&gt;, osm_id &lt;dbl&gt;, osm_type &lt;chr&gt;,\n#   geometry &lt;POINT [°]&gt;",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#working-with-spatial-data-in-r",
    "href": "mapping.html#working-with-spatial-data-in-r",
    "title": "Introduction to GIS in R",
    "section": "Working with spatial data in R",
    "text": "Working with spatial data in R\n\nOpen Street Map Data\nThroughout this tutorial you will be using data about Kenyan rainfall - Kenyan Rainfall Data. It covers total rainfall at the subcounty level in the last 5 years, from 2021-01-01. The data is updated weekly and includes a range of variables capturing amount of rain.\nWe want to visualise, and better understand how much rain has fallen in the last 5 years at the sub-county and county level, and what the distribution is at the MSOA level of geography. To achieve this we will have to import spatial data, manipulate it, create summary statistics, and then plot it.",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#loading-spatial-and-non-spatial-data",
    "href": "mapping.html#loading-spatial-and-non-spatial-data",
    "title": "Introduction to GIS in R",
    "section": "Loading spatial and non-spatial data",
    "text": "Loading spatial and non-spatial data\nRainfall data has been tidied up and saved as a Comma Separated Value file (.csv). We can use read_csv to open it in R.\n\nExercise\n −+ 10:00 \n\nCreate a new object called poi by using read_csv(). Load data located in “https://data.humdata.org/dataset/ken-rainfall-subnational”.\nUse glimpse() or head() to view health structure.\n\n\n\n5.2.2. Solution\n\n\n\n\n\n5.2.2. Solution\n\n\n\npoi &lt;- readr::read_csv(\"/Users/christophersteinberg/Documents/GitHub/intro_R-main/data/poi.csv\")\n\nRows: 34182 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): name, amenity\ndbl (2): easting, northing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoi\n\n# A tibble: 34,182 × 4\n   name                                easting northing amenity         \n   &lt;chr&gt;                                 &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;           \n 1 The Salvation Army_Machakos citadel    37.3   -1.53  place_of_worship\n 2 Masjid Umar bin Khattab                40.1   -3.25  place_of_worship\n 3 Hotel and Pub                          35.9    4.06  pub             \n 4 Rosoga Dispensary                      35.9    0.103 clinic          \n 5 AIC Rosoga Church                      35.9    0.102 place_of_worship\n 6 FGCK Kimngorom                         35.8    0.165 place_of_worship\n 7 Schools of Excellence Primary          36.6   -0.661 school          \n 8 Greenview Secondary School             36.6   -0.869 school          \n 9 Karati Secondary School                36.6   -0.733 school          \n10 Sint Paul Secondary School             36.6   -0.701 school          \n# ℹ 34,172 more rows\n\n\n\n\npoi is currently just a data frame - it has not got an explicit geometry column which links observations to their geographic location. It does however contain several columns which can be used to convert it into a spatial data format.\nWard_code column references the GSS codes of wards within which the observations fall. GSS codes can be used to join lfb data to boundaries from the Open Geography Portal. One issue with this particular column is that it does not indicate the currency of GSS codes. Wards are subject to frequent change, and as such it is best practice to be clear about the dates of any boundaries used by stating the exact code used, e.g. wd19cd. Because LFB data does not include this information we have no guarantee that the boundaries and GSS codes we join will match.\nFortunately we have also been provided with columns recording the easting, and northing of each incident. We can use those to convert lfb into an sf object. To achieve this we will use the st_as_sf() function which takes the following arguments:\n\nnew_object &lt;- st_as_sf(x = input_data_frame, coords = c(\"x_coordinate_column\", \"y_coordinate_column\"), crs = 27700)\n\n\n\nExercise\n −+ 10:00 \n\nCreate a new object called poi_sf by converting poi using the st_as_sf() function.\nUse glimpse() or head() to view poi_sf structure.\n\n\n\n5.2.2. Solution\n\n\n\npoi_sf &lt;- sf::st_as_sf(x = poi, coords = c(\"easting\", \"northing\"), crs = 4326)\nhead(poi_sf)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 35.81531 ymin: -3.246071 xmax: 40.08169 ymax: 4.061832\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 3\n  name                                amenity                      geometry\n  &lt;chr&gt;                               &lt;chr&gt;                     &lt;POINT [°]&gt;\n1 The Salvation Army_Machakos citadel place_of_worship (37.26376 -1.534213)\n2 Masjid Umar bin Khattab             place_of_worship (40.08169 -3.246071)\n3 Hotel and Pub                       pub               (35.88242 4.061832)\n4 Rosoga Dispensary                   clinic            (35.8573 0.1026951)\n5 AIC Rosoga Church                   place_of_worship (35.85683 0.1024965)\n6 FGCK Kimngorom                      place_of_worship (35.81531 0.1654002)\n\n\n\n\nst_as_sf() converted the easting and northing columns to simple feature geometries and created a new column called geometry which holds spatial information for each row. Now that poi is a spatial object we can plot it using the ggplot2 package. For now we will use the geom_sf() function which creates a quick map, using ggplot2's default settings. geom_sf() only needs to be supplied with a simple feature object and is very useful for quickly inspecting your data. To quickly plot multiple layers on the same map use geom_sf() + geom_sf().\n\n\nExercise\n −+ 03:00 \n\nPlot poi_sf using the geom_sf() function.\n\n\n\n5.2.2. Solution\n\n\n\nggplot2::ggplot(poi_sf) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nWe can also create interactive maps using plotly or leaflet. For plotly we ca write the map using ggplot2, before executing ggplotly().\n\n\nExercise\n −+ 03:00 \n\nMake an interactive map of poi_sf using the ggplotly() function.\n\n\n\n5.2.2. Solution\n\n\n\nggplot_graph &lt;- ggplot2::ggplot(poi_sf) + \n  ggplot2::geom_sf() \n\nplotly::ggplotly(ggplot_graph)",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "mapping.html#filtering-by-administration-code",
    "href": "mapping.html#filtering-by-administration-code",
    "title": "Introduction to GIS in R",
    "section": "Filtering by Administration Code",
    "text": "Filtering by Administration Code\nThis dataset covers all of Kenya at subnational administrative boundaries. Let’s look at solely locations in Nairobi to narrow down our exploration more. To remove all points outside of Nairobi we will have to import a shapefile with the right county level and then use it to spatially filter our poi_df data.\nSo far we have created our own sf objects by adding a geometry column. The kenya data set is already a spatial one and as such we can use the st_read() function from the sf package to import it. st_read isextremely versatile and able to import most spatial data formats into R. The only argument that needs to be supplied to st_read is the fullpath to the UTLA file\n\nExercise\n −+ 10:00 \n\nUse st_read() to load the kenya sub-county boundaries you downloaded at the beginning of the tutorial, askenya_2017.\nUTLA path - https://data.humdata.org/dataset/cod-ab-ken\nMake a static map of the object you have just created using geom_sf().\n\n\n\n5.2.2. Solution\n\n\n\nkenya_2017 &lt;- sf::st_read(\"/Users/christophersteinberg/Documents/GitHub/admn2/ken_admbnda_adm2_iebc_20191031.shp\", quiet = TRUE)\nggplot2::ggplot(kenya_2017) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nSub-county level boundaries have loaded correctly but they currently cover all of Kenya. Because simple feature objects are data frames with a geometry column attached, any operations that we would perform on a normal data frame can also be performed on an object of class sf. We will use dplyr::filter and stringr::str_detect() to only keep UTLAs where their ADMN_PCODE code starts with “KE47”. “KE47” denotes that an sub-county is part of Nairobi county.\n\n\nExercise\n −+ 10:00 \n\nInspect kenya_2017 using head() or glimpse(), and identify which column holds the GSS codes - it should end in “cd”.\nCreate a new object called nairobi_subcounty. Use dplyr::filter alongside stringr::str_detect() to only keep observations which have a Administrative code starting with “KE47”.\nPlot nairobi_subcounty to see if the results look correct.\n\n\n\n5.2.2. Solution\n\n\n\nhead(kenya_2017)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 34.04426 ymin: -1.016204 xmax: 36.24609 ymax: 0.5589468\nGeodetic CRS:  WGS 84\n  Shape_Leng Shape_Area      ADM2_EN ADM2_PCODE ADM2_REF ADM2ALT1EN ADM2ALT2EN\n1  1.7469864 0.04082931     Ainabkoi   KE027144     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n2  0.9173066 0.01995665      Ainamoi   KE035190     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n3  1.4026374 0.03799999        Aldai   KE029152     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n4  1.0813543 0.04935731 Alego Usonga   KE041234     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n5  0.7439150 0.02136547       Awendo   KE044254     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n6  0.7970553 0.02423416       Bahati   KE032174     &lt;NA&gt;       &lt;NA&gt;       &lt;NA&gt;\n      ADM1_EN ADM1_PCODE ADM0_EN ADM0_PCODE       date    validOn    ValidTo\n1 Uasin Gishu      KE027   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n2     Kericho      KE035   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n3       Nandi      KE029   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n4       Siaya      KE041   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n5      Migori      KE044   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n6      Nakuru      KE032   Kenya         KE 2017-11-03 2019-10-31 -001-11-30\n                        geometry\n1 MULTIPOLYGON (((35.35933 0....\n2 MULTIPOLYGON (((35.26262 -0...\n3 MULTIPOLYGON (((34.93989 0....\n4 MULTIPOLYGON (((34.20727 0....\n5 MULTIPOLYGON (((34.54577 -0...\n6 MULTIPOLYGON (((36.20478 -0...\n\n\n\nnairobi_subcounty &lt;- dplyr::filter(kenya_2017, stringr::str_detect(ADM2_PCODE, \"KE047\"))\nggplot2::ggplot(nairobi_subcounty) + \n  ggplot2::geom_sf() \n\n\n\n\n\n\n\n\n\n\nFinally, for the next step, we only need the outer boundary of Nairobi - all the internal subcounty boundaries have to be removed and only the outer edges kept. sf has a function exactly for this purpose called st_union(). It only takes one argument, which is the sf object we want to merge.\n\n\nExercise\n −+ 10:00 \n\nCreate a new object called nairobi_boundary using the st_union function.\nPlot it to check the results.\n\n\n\n5.2.2. Solution\n\n\n\nnairobi_boundary &lt;- sf::st_union(nairobi_subcounty)\nggplot2::ggplot(nairobi_boundary) + \n  ggplot2::geom_sf()",
    "crumbs": [
      "Introduction to GIS in R"
    ]
  },
  {
    "objectID": "curriculum.html",
    "href": "curriculum.html",
    "title": "1  KNBS R Programming Curriculum",
    "section": "",
    "text": "1.1 Curriculum Overview\nThis comprehensive curriculum is designed to take KNBS employees from novice R users to advanced practitioners, equipping them with the skills necessary to perform sophisticated data analysis and programming tasks. The curriculum is structured into three distinct levels - Beginner, Intermediate, and Advanced - each building upon the knowledge gained in the previous level.\nThe journey begins with the foundational “Introduction to R” course, which provides non-R users with the essential skills to perform basic work tasks in R. This is followed by a series of courses that broaden the base knowledge of R and introduce complementary languages and tools such as SQL, Bash, and Git. These courses are designed to give analysts a well-rounded skill set that is crucial for most data analysis tasks in R.\nAs learners progress, the Intermediate level courses delve into more specific and advanced topics. These courses introduce various analytical methods and best practices in coding for automation. They are designed to enhance the analysts’ capabilities in handling complex data tasks and improving code efficiency and reproducibility.\nThe Advanced level courses introduce cutting-edge techniques and tools in data science and software development. These courses cover topics such as machine learning, natural language processing, big data processing with Sparklyr, and modern software development practices like continuous integration.\ngantt\n    title KNBS R Curriculum\n    dateFormat  X\n    axisFormat %H\n    \n    section Beginner\n    Introduction to R           :a1, 0, 16h\n    Introduction to Data Visualisation :a2, after a1 start, 16h\n    Optional Modules            :a3, after a2, 4h\n    Introduction to GIS in R    :a4, after a2, 3h\n    R Control Flow Loops and Functions :a5, after a2, 3h\n\n    section Intermediate Level\n    Optional Modules            :b1, after a5, 25h\n    More GIS in R               :b3, after a5, 3h\n    Introduction to Git         :b4, after a5, 4h\n    Statistics in R             :b5, after a5, 16h\n    \n    section Advanced\n    Introduction to NLP in R    :c1, after b5, 16h\n    Introduction to Machine Learning in R :c2, after b5, 24h\n    Additional Analysis Modules: c3, after c2, 38h\n    Packaging and Documentation :c4, after b5, 4h\n    Additional Developer Modules: c5, after c4, 6h",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#beginner-level",
    "href": "curriculum.html#beginner-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.1 Beginner Level",
    "text": "2.1 Beginner Level\n\n2.1.1 Introduction to R\nThis redesigned course focuses on applying skills and building confidence in R programming. Participants will learn about data types, importing data, and working with DataFrames through hands-on exercises. The course is structured with a week between sessions to allow for practice and assimilation of learning, making it ideal for beginners to develop independence and resilience in their R programming journey.\n\nData Types\nImporting Data\nDataFrames, Manipulation, and Cleaning\n\nCourse Length: 2 Days\nPrerequisites: None\n\n\n2.1.2 Introduction to Data Visualisation\nData Visualisation is the art of displaying data in a clear and understandable way that allows for maximum impact. This comprehensive course covers both theoretical and practical aspects of data visualization in R. Participants will learn best practices for presenting data clearly and professionally. The course then transitions into practical application, teaching how to produce production-ready visualizations using R. By the end, students will have a solid foundation in creating impactful and consistent data visualizations.\nCourse Length: 2 Days\nPrerequisites: Introduction to R\n\n\n2.1.3 Introduction to GIS in R\nThis course introduces the fundamentals of working with geospatial data in R. Participants will learn to load spatial data, manipulate spatial objects, and create both static and interactive maps. The course covers important concepts such as GSS codes, spatial joins, and coordinate reference systems. By the end, students will be able to perform basic spatial analysis and create visually appealing maps using R.\nCourse Length: 2-3 hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.4 R Control Flow Loops and Functions\nThis course focuses on more advanced programming concepts in R, specifically loops, control flow, and function writing. Participants will learn how to use these tools to reduce code repetition, improve readability, and follow good coding practices. The course includes practical examples and exercises to reinforce learning, and provides guidance on areas for additional study to further enhance programming skills.\nCourse Length: 3 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5 Optional Modules\n\n2.1.5.1 Introduction to RAP\nThis course introduces the concepts, motivation, and techniques for creating a Reproducible Analytical Pipeline (RAP). Participants will learn about the benefits of RAP and how to overcome common barriers in implementation. The course is tailored for government analysts and managers of analysis, providing a foundation for enhancing reproducibility and efficiency in analytical workflows.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.2 Introduction to RMarkdown\nThis course provides a comprehensive introduction to RMarkdown, a powerful tool for creating dynamic, reproducible documents in R. Participants will learn how to set up new documents, create various types of content, and output to different formats including HTML, PDF, and Word. The course covers text formatting, tables, images, and RMarkdown-specific features, equipping students with the skills to create professional, data-driven reports.\nCourse Length: 1-2 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.3 Best Practice in Programming - Clean Code\nThis workshop emphasizes best practices in programming for improved code readability and collaboration. Participants will learn general principles that enhance coding skills and make code more accessible to others. The course covers techniques for writing clean, maintainable code and encourages the development of good coding habits. By the end, students will have a solid foundation in creating code that is easy to read, understand, and modify.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#intermediate-level",
    "href": "curriculum.html#intermediate-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.2 Intermediate Level",
    "text": "2.2 Intermediate Level\n\n2.2.1 Introduction to Git\nThis course provides a comprehensive understanding of the Git version control system. Participants will gain hands-on experience working both locally and collaboratively with Git. The course covers the fundamentals of Git, its practical applications, and the benefits of using it for individual and team projects. By the end, students will be equipped with the skills to effectively use Git for version control and collaboration in their development workflows.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.2 Statistics in R\nThis comprehensive course covers both statistical theory and its practical application in R. Participants will learn about exploratory data analysis, various statistical tests, linear regression, model adequacy and selection, and generalized linear models. The course provides a balanced mix of theoretical understanding and hands-on coding experience. By the end, students will be equipped to perform advanced statistical analyses and interpret results using R.\nCourse Length: 16 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.3 More GIS in R\nBuilding on the introductory GIS course, this advanced session covers more complex geospatial operations and analysis techniques in R. Participants will learn about buffers, intersections, area summary statistics, and network analysis. The course also focuses on troubleshooting common error messages and improving the accuracy of spatial analyses. By the end, students will be able to conduct and present comprehensive spatial analyses using R.\nCourse Length: 2-3 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4 Optional Modules\n\n2.2.4.1 Foundations of SQL\nThis course provides a comprehensive introduction to SQL, covering syntax applicable to various database systems. Using an online platform with SQLite, participants will learn through hands-on exercises. The course covers basic SQL queries, table manipulation, joining tables, and database alterations. By the end, students will have a solid foundation in SQL, enabling them to work with databases effectively in their data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.2 Command Line Basics\nThis course introduces the powerful world of command line interfaces for both UNIX and Windows systems. Participants will learn basic commands and understand how to navigate file systems and execute operations using the command line. The course aims to make participants comfortable using this essential tool in their work, enhancing their ability to interact with computers efficiently and perform advanced operations.\nCourse Length: 2 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.3 Reproducible Reporting with RMarkdown\nThis course delves into advanced features of RMarkdown for creating reproducible reports. Participants will learn to embed executable code and data into reports, work with YAML headers and theme options, and use parameters for dynamic reporting. The course also covers markdown syntax, code chunks, and chunk options. By the end, students will be able to create professional, reproducible reports that seamlessly integrate code, data, and narrative.\nCourse Length: 2 Hours\nPrerequisites: Introduction to RMarkdown, Introduction to RAP\n\n\n2.2.4.4 Modular Programming in R\nThis course focuses on the principles of modular design in R programming. Participants will learn the importance of well-structured, reproducible code and how to implement modular design principles. The course covers techniques for converting code into functions and modules, improving code organization and reusability. By the end, students will be able to write more efficient, maintainable, and scalable R code.\nCourse Length: 4-5 hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.5 Hypothesis Testing in R\nThis intermediate course focuses on advanced concepts in hypothesis testing using R. Participants will learn to define and calculate Type I and Type II errors, determine effect sizes, and calculate statistical power. The course also covers sample size calculations for various statistical tests. By the end, students will have a deeper understanding of the nuances of hypothesis testing and be able to design more robust statistical analyses in R.\nCourse Length: 4-6 Hours\nPrerequisites: Statistics in R\n\n\n2.2.4.6 Dates and Times in R\nThis course provides a comprehensive overview of handling date and time data in R. Participants will learn how to create, convert, and manipulate date-time objects, understanding the underlying storage mechanisms. The course covers practical operations such as extracting parts of dates, performing date-time arithmetic, and working with time zones. By the end, students will be proficient in managing and analyzing time-based data in R.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "curriculum.html#advanced-level",
    "href": "curriculum.html#advanced-level",
    "title": "1  KNBS R Programming Curriculum",
    "section": "2.3 Advanced Level",
    "text": "2.3 Advanced Level\n\n2.3.1 Introduction to NLP in R\nThis course provides a comprehensive introduction to Natural Language Processing (NLP) using R. Participants will learn fundamental NLP concepts and techniques, including text cleaning, exploratory analysis, and feature engineering. The course covers practical applications such as sentiment analysis on real datasets. By the end, students will have the skills to preprocess text data, extract meaningful features, and perform basic NLP tasks in R.\nCourse Length: 2 days\nPrerequisites: Statistics in R\n\n\n2.3.2 Introduction to Machine Learning in R\nThis course introduces the fundamentals of machine learning using R’s state-of-the-art “mlr3” package. Participants will learn about classification, regression, and cluster analysis experiments. The course covers the entire machine learning workflow, from data preparation to model evaluation and interpretation. By the end, students will have hands-on experience implementing various machine learning algorithms and understand how to apply them to real-world problems.\nCourse Length: 3 days\nPrerequisites: Statistics in R\n\n\n2.3.3 Packaging and Documentation\nThis course guides participants through the process of building and sharing R packages. Students will learn how to create custom functions, organize them into a package structure, and document their code effectively. The course covers best practices in package development, including version control and collaboration. By the end, participants will be able to create their own R packages, enhancing code reusability and facilitating collaboration with other R users.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions, Best Practice in Programming - Clean Code, Introduction to Git\n\n\n2.3.4 Additional Analysis Modules\n\n2.3.4.1 Quality Assurance of Predictive Modelling\nThis course explores critical quality issues in statistical modeling and machine learning for prediction. Participants will learn best practices for model design, validation, and usage across various industries. The course covers topics such as model interpretability, bias detection, and robustness testing. By the end, students will be equipped with the knowledge to ensure the reliability and effectiveness of their predictive models.\nCourse Length: 6-8 hours\nPrerequisites: Statistics in R\n\n\n2.3.4.2 Introduction to Sparklyr\nThis course introduces Sparklyr, the R interface to Apache Spark for big data processing. Participants will learn how to handle and analyze massive datasets that exceed the capabilities of traditional R. The course covers data manipulation, querying, and processing techniques specific to Sparklyr. By the end, students will be able to leverage the power of distributed computing for their large-scale data analysis projects in R.\nCourse Length: 2 days\nPrerequisites: Introduction to Machine Learning in R\n\n\n2.3.4.3 Introduction to Bayesian Data Analysis\nThis course provides a foundational understanding of Bayesian Data Analysis and its implementation in R. Participants will explore the principles of Bayesian statistics, including prior and posterior distributions, and Markov Chain Monte Carlo (MCMC) methods. The course includes hands-on examples of Bayesian analysis in R, covering model fitting and interpretation. By the end, students will be able to apply Bayesian techniques to their own data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: Statistics in R\n\n\n\n2.3.5 Additional Developer Modules\n\n2.3.5.1 Introduction to Unit Testing\nThis course focuses on the principles and implementation of unit testing in R for ensuring code quality. Participants will learn how to design, create, and execute tests for their R code. The course covers both the theoretical aspects of software development and practical implementation in R. By the end, students will be able to implement robust testing strategies in their R projects, improving code reliability and maintainability.\nCourse Length: 4 Hours\nPrerequisites: Packaging and Documentation\n\n\n2.3.5.2 Introduction to Continuous Integration\nThis course introduces modern software development approaches, focusing on Continuous Integration (CI) concepts. Participants will learn about DevOps and CI/CD philosophies and how they contribute to efficient software development. The course covers tools and practices that automate testing and deployment processes. While not hands-on, this course provides essential background knowledge for advanced technical courses on building modern development infrastructures. Course Length: 2 Hours\nPrerequisites: Packaging and Documentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>KNBS R Programming Curriculum</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Session aims\nIn this course, you will explore the versatility of R, a powerful language for statistical computing and graphics. You will discover the benefits of using R and get started with the basics, and fain confidence with the user-friendly R Studio interface and learn fundamental R concepts. You will also dive into the Tidyverse, a collection of packages for data storage, visualisation and manipulation. This course offers a solid foundation to kickstart your journey with R!\nThis book is designed to accompany the Introduction to R training that is starting at KNBS. To complete this course, you will need to have R Studio installed on your computer.\nIf you’re running through this book solo, it is recommended to run through it in order and try out all the of the exercises as you go through. Each exercise has a Solution dropdown, which allows you to view prompts to help with the question and see the answers.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#session-aims",
    "href": "intro.html#session-aims",
    "title": "Introduction to R",
    "section": "",
    "text": "navigate the R and R Studio environment\nunderstand and use the common R functions for data manipulation\nunderstand the basics of data visualisation using the ggplot2 package\nunderstand the term tidy data and why it is important for writing efficient code",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#what-is-r",
    "href": "intro.html#what-is-r",
    "title": "Introduction to R",
    "section": "What is R?",
    "text": "What is R?\nR is an open-source programming language and software environment, designed primarily for statistical computing. It has a long history - it is based on the S language, which was developed in 1976 in Bell Labs, where the UNIX operating system and the C and C++ languages were developed. The R language itself was developed in the 1990s, with the first stable version release in 2000.\nR has grown rapidly in popularity particularly in the last five years, due to the increased interest in the data science field. It is now a key tool used by analysts in governments globally.\nSome of the advantages:\n\nIt is popular - there is a large, active and rapidly growing community of R programmers, which has resulted in a plethora of resources and extensions.\nIt is powerful - the history as a statistical language means it is well suited for data analysis and manipulation.\nIt is extensible - there are a vast array of packages that can be added to extend the functionality of R, produced by statisticians and programmers around the world. These can range from obscure statistical techniques to tools for making interactive charts.\nIt’s free and open source - a large part of its popularity can be owed to its low cost, particularly relative to proprietary software such as SAS or STATA.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#introducing-rstudio",
    "href": "intro.html#introducing-rstudio",
    "title": "Introduction to R",
    "section": "Introducing RStudio",
    "text": "Introducing RStudio\nRStudio is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with. RStudio has four main panels;\n\nScript Editor (top left) - used to write and save your code, which is only run when you explicitly tell RStudio to do so.\nConsole (bottom left) - all code is run through the console, even the code you write in the script editor is sent to the console to be run. It’s perfect for quickly viewing data structures and help for functions but should not be used to write code you want to save (that should be done in the script editor).\nEnvironment (top right) - all data, objects and functions that you have read in/created will appear here.\nFiles/Plots/Help (bottom right) - this pane groups a few miscellaneous areas of RStudio.\n\nFiles acts like the windows folder to navigate between files and folders.\nPlots shows any graphs that you generate.\nPackages let’s you install and manage packages currently in use.\nHelp provides information about packages or functions, including how to use them.\nViewer is essentially RStudio’s built-in browser, which can be used for web app development.\n\n\nYou may have noticed that your Script Editor is bigger than the Console or your Environment has suddenly disappeared. In RStudio, you can adjust the size of different panes by clicking and dragging the dividers between them. If you want to maximize a specific pane, such as the Script Editor, use the shortcut Ctrl + Shift + 1 (Windows/Linux) or Cmd + Shift + 1 (Mac) to focus on it. To restore the default layout, press Ctrl + Shift + 0 (Windows/Linux) or Cmd + Shift + 0 (Mac). You can also use the View menu to toggle different panes on and off, ensuring your workspace suits your needs.\nIf you find the text difficult to read or prefer a different appearance, you can customise the theme, font, and text size in RStudio. Go to Tools &gt; Global Options &gt; Appearance, where you can choose from different editor themes (e.g., light or dark mode), adjust the font type, and increase or decrease the text size for better readability. These changes can help make coding more comfortable, especially during long sessions.\n\nRecommended Changes\nWhile not necessary, certain changes are almost always recommended for visibility reasons. These include:\n\nChoosing a different theme, as Textmate can be hard on the eyes. This can be done in Tools &gt; Global Options &gt; Appearance &gt; Editor theme:.\nHighlight R function calls. This makes functions look a different colour than normal text, which can make reading your code much easier. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Highlight R function calls.\nUse Rainbow Parenthesis. This makes each pair of () in a line a different colour, which can help you catch if you’re missing one and it’s breaking your code. This can be done in Tools &gt; Global Options &gt; Code &gt; Display &gt; Use rainbow parenthesis.",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#basic-syntax",
    "href": "intro.html#basic-syntax",
    "title": "Introduction to R",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nExercise\n\n\n\n−+\n03:00\n\n\n\nAs a quick exercise, try out some arithmetic in your console:\n\n25 * 15\n(45 + 3) ^ 2\n78 / 4\n\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n\nSolution\n25 * 15\n\n\n[1] 375\n\n\nSolution\n(45 + 3) ^ 2\n\n\n[1] 2304\n\n\nSolution\n78 / 4\n\n\n[1] 19.5\n\n\n\n\nThe assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\nSolution\nx1 &lt;- 14\nx1\n\n\n[1] 14\n\n\n\n\nSolution\nx2 &lt;- x1 + 7\nx2\n\n\n[1] 21\n\n\n\n\nSolution\nx3 &lt;- x2 / 3\nx3\n\n\n[1] 7\n\n\n\n\nCombining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n\n[1] 1 2 3\n\n\nSolution\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSolution\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x121bae720&gt;\n&lt;environment: namespace:lubridate&gt;\n\n\nSolution\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#functions",
    "href": "intro.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\nExercise\n\n\n\n−+\n05:00\n\n\n\n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n\n[1] 36",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "Introduction to R",
    "section": "Packages",
    "text": "Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "intro.html#the-tidyverse",
    "href": "intro.html#the-tidyverse",
    "title": "Introduction to R",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\nExercise\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\nlibrary(tidyverse)",
    "crumbs": [
      "Introduction to R"
    ]
  },
  {
    "objectID": "application_1.html",
    "href": "application_1.html",
    "title": "5  Application 1",
    "section": "",
    "text": "5.1 A real world example\nIn this application, we will practice some of the coding skills learned in the training so far. Whereas the examples used in the textbook use toy datasets which are already clean and well-formatted, the datasets used in our applications may require some initial cleaning prior to analysis. This is likely closer to tasks you might encounter in your everyday work at KNBS.\nIn this particular application, we would like you to produce 3 simple pieces of analysis using Kenya’s 2019 census data: 1. Breakdown of Kenya by religious belief 2. Share of people who are migrants in each county 3. Average working hours for men and women in rural vs. urban areas.\nThere are two stages to this task. The first is to read in the data and prepare it to be analysed. This will involve reading the data in properly, fixing any column name issues, reducing the size of the dataset if it is too large, and finally dealing with any missing values, or NAs, that are found in the data.\nThe second is to perform the analysis on your dataset. This will involve creating new columns, filtering out certain data, and creating summaries.",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "application_1.html#application-for-knbs",
    "href": "application_1.html#application-for-knbs",
    "title": "3  KNBS Application",
    "section": "",
    "text": "Read in the dataset and use janitor to clean any column names\nWe need columns relating to migration, age, etc. Select the columns we will need - look at survey metadata\nLet’s rename some columns to make them easier to understand\nFor our religion question, let’s examine the data in this column\nHow should we approach the summary? How do we want to treat NAs, or DK\nRead the dataset using read_csv and clean the column names using janitor::clean_names.\nSelect the columns required for the religion analysis (e.g., p17 for religion) using select.\nRename the selected religion column to be more descriptive (e.g., religion) using rename.\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(count = n()).\nAddress how to handle any NA or “Don’t Know” responses during this summary.\nSelect the columns required for the migration analysis (e.g., county, p19 for birthplace) using select. Rename the birthplace column (e.g., birthplace) using rename.\nCreate a new binary column named migrant using mutate and if_else to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).\nSelect the columns required for the working hours analysis (e.g., ea_type for rural/urban, p11 for sex, p12 for age, p52 for hours worked) using select.\nRename the selected columns for clarity (e.g., age, sex, hours_worked_if_work) using rename. Filter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and if_else.\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNBS Application</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum-overview",
    "href": "index.html#curriculum-overview",
    "title": "KNBS: Introduction to R",
    "section": "1.1 Curriculum Overview",
    "text": "1.1 Curriculum Overview\nThe journey begins with the foundational “Introduction to R” course, which provides non-R users with the essential skills to perform basic work tasks in R. This is followed by a series of courses that broaden the base knowledge of R and introduce complementary languages and tools such as SQL, Bash, and Git. These courses are designed to give analysts a well-rounded skill set that is crucial for most data analysis tasks in R.\nAs learners progress, the Intermediate level courses delve into more specific and advanced topics. These courses introduce various analytical methods and best practices in coding for automation. They are designed to enhance the analysts’ capabilities in handling complex data tasks and improving code efficiency and reproducibility.\nThe Advanced level courses introduce cutting-edge techniques and tools in data science and software development. These courses cover topics such as machine learning, natural language processing, big data processing with Sparklyr, and modern software development practices like continuous integration.\n\n\n\n\n\ngantt\n    title KNBS R Curriculum\n    dateFormat  X\n    axisFormat %H\n    \n    section Beginner\n    Introduction to R           :a1, 0, 16h\n    Introduction to Data Visualisation :a2, after a1 start, 16h\n    Optional Modules            :a3, after a2, 4h\n    Introduction to GIS in R    :a4, after a2, 3h\n    R Control Flow Loops and Functions :a5, after a2, 3h\n\n    section Intermediate Level\n    Optional Modules            :b1, after a5, 25h\n    More GIS in R               :b3, after a5, 3h\n    Introduction to Git         :b4, after a5, 4h\n    Statistics in R             :b5, after a5, 16h\n    \n    section Advanced\n    Introduction to NLP in R    :c1, after b5, 16h\n    Introduction to Machine Learning in R :c2, after b5, 24h\n    Additional Analysis Modules: c3, after c2, 38h\n    Packaging and Documentation :c4, after b5, 4h\n    Additional Developer Modules: c5, after c4, 6h",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#beginner-level",
    "href": "index.html#beginner-level",
    "title": "KNBS: Introduction to R",
    "section": "2.1 Beginner Level",
    "text": "2.1 Beginner Level\n\n2.1.1 Introduction to R\nThis redesigned course focuses on applying skills and building confidence in R programming. Participants will learn about data types, importing data, and working with DataFrames through hands-on exercises. The course is structured with a week between sessions to allow for practice and assimilation of learning, making it ideal for beginners to develop independence and resilience in their R programming journey.\n\nData Types\nImporting Data\nDataFrames, Manipulation, and Cleaning\n\nCourse Length: 2 Days\nPrerequisites: None\n\n\n2.1.2 Introduction to Data Visualisation\nData Visualisation is the art of displaying data in a clear and understandable way that allows for maximum impact. This comprehensive course covers both theoretical and practical aspects of data visualization in R. Participants will learn best practices for presenting data clearly and professionally. The course then transitions into practical application, teaching how to produce production-ready visualizations using R. By the end, students will have a solid foundation in creating impactful and consistent data visualizations.\nCourse Length: 2 Days\nPrerequisites: Introduction to R\n\n\n2.1.3 Introduction to GIS in R\nThis course introduces the fundamentals of working with geospatial data in R. Participants will learn to load spatial data, manipulate spatial objects, and create both static and interactive maps. The course covers important concepts such as spatial joins, and coordinate reference systems. By the end, students will be able to perform basic spatial analysis and create visually appealing maps using R.\nCourse Length: 2-3 hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.4 R Control Flow Loops and Functions\nThis course focuses on more advanced programming concepts in R, specifically loops, control flow, and function writing. Participants will learn how to use these tools to reduce code repetition, improve readability, and follow good coding practices. The course includes practical examples and exercises to reinforce learning, and provides guidance on areas for additional study to further enhance programming skills.\nCourse Length: 3 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5 Optional Modules\n\n2.1.5.1 Introduction to RAP\nThis course introduces the concepts, motivation, and techniques for creating a Reproducible Analytical Pipeline (RAP). Participants will learn about the benefits of RAP and how to overcome common barriers in implementation. The course is tailored for government analysts and managers of analysis, providing a foundation for enhancing reproducibility and efficiency in analytical workflows.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.2 Introduction to RMarkdown\nThis course provides a comprehensive introduction to RMarkdown, a powerful tool for creating dynamic, reproducible documents in R. Participants will learn how to set up new documents, create various types of content, and output to different formats including HTML, PDF, and Word. The course covers text formatting, tables, images, and RMarkdown-specific features, equipping students with the skills to create professional, data-driven reports.\nCourse Length: 1-2 Hours\nPrerequisites: Introduction to Data Visualisation\n\n\n2.1.5.3 Best Practice in Programming - Clean Code\nThis workshop emphasizes best practices in programming for improved code readability and collaboration. Participants will learn general principles that enhance coding skills and make code more accessible to others. The course covers techniques for writing clean, maintainable code and encourages the development of good coding habits. By the end, students will have a solid foundation in creating code that is easy to read, understand, and modify.\nCourse Length: 1 Hour\nPrerequisites: Introduction to Data Visualisation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#intermediate-level",
    "href": "index.html#intermediate-level",
    "title": "KNBS: Introduction to R",
    "section": "2.2 Intermediate Level",
    "text": "2.2 Intermediate Level\n\n2.2.1 Introduction to Git\nThis course provides a comprehensive understanding of the Git version control system. Participants will gain hands-on experience working both locally and collaboratively with Git. The course covers the fundamentals of Git, its practical applications, and the benefits of using it for individual and team projects. By the end, students will be equipped with the skills to effectively use Git for version control and collaboration in their development workflows.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.2 Statistics in R\nThis comprehensive course covers both statistical theory and its practical application in R. Participants will learn about exploratory data analysis, various statistical tests, linear regression, model adequacy and selection, and generalized linear models. The course provides a balanced mix of theoretical understanding and hands-on coding experience. By the end, students will be equipped to perform advanced statistical analyses and interpret results using R.\nCourse Length: 16 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.3 More GIS in R\nBuilding on the introductory GIS course, this advanced session covers more complex geospatial operations and analysis techniques in R. Participants will learn about buffers, intersections, area summary statistics, and network analysis. The course also focuses on troubleshooting common error messages and improving the accuracy of spatial analyses. By the end, students will be able to conduct and present comprehensive spatial analyses using R.\nCourse Length: 2-3 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4 Optional Modules\n\n2.2.4.1 Foundations of SQL\nThis course provides a comprehensive introduction to SQL, covering syntax applicable to various database systems. Using an online platform with SQLite, participants will learn through hands-on exercises. The course covers basic SQL queries, table manipulation, joining tables, and database alterations. By the end, students will have a solid foundation in SQL, enabling them to work with databases effectively in their data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.2 Command Line Basics\nThis course introduces the powerful world of command line interfaces for both UNIX and Windows systems. Participants will learn basic commands and understand how to navigate file systems and execute operations using the command line. The course aims to make participants comfortable using this essential tool in their work, enhancing their ability to interact with computers efficiently and perform advanced operations.\nCourse Length: 2 Hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.3 Reproducible Reporting with RMarkdown\nThis course delves into advanced features of RMarkdown for creating reproducible reports. Participants will learn to embed executable code and data into reports, work with YAML headers and theme options, and use parameters for dynamic reporting. The course also covers markdown syntax, code chunks, and chunk options. By the end, students will be able to create professional, reproducible reports that seamlessly integrate code, data, and narrative.\nCourse Length: 2 Hours\nPrerequisites: Introduction to RMarkdown, Introduction to RAP\n\n\n2.2.4.4 Modular Programming in R\nThis course focuses on the principles of modular design in R programming. Participants will learn the importance of well-structured, reproducible code and how to implement modular design principles. The course covers techniques for converting code into functions and modules, improving code organization and reusability. By the end, students will be able to write more efficient, maintainable, and scalable R code.\nCourse Length: 4-5 hours\nPrerequisites: R Control Flow Loops and Functions\n\n\n2.2.4.5 Hypothesis Testing in R\nThis intermediate course focuses on advanced concepts in hypothesis testing using R. Participants will learn to define and calculate Type I and Type II errors, determine effect sizes, and calculate statistical power. The course also covers sample size calculations for various statistical tests. By the end, students will have a deeper understanding of the nuances of hypothesis testing and be able to design more robust statistical analyses in R.\nCourse Length: 4-6 Hours\nPrerequisites: Statistics in R\n\n\n2.2.4.6 Dates and Times in R\nThis course provides a comprehensive overview of handling date and time data in R. Participants will learn how to create, convert, and manipulate date-time objects, understanding the underlying storage mechanisms. The course covers practical operations such as extracting parts of dates, performing date-time arithmetic, and working with time zones. By the end, students will be proficient in managing and analyzing time-based data in R.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "index.html#advanced-level",
    "href": "index.html#advanced-level",
    "title": "KNBS: Introduction to R",
    "section": "2.3 Advanced Level",
    "text": "2.3 Advanced Level\n\n2.3.1 Introduction to NLP in R\nThis course provides a comprehensive introduction to Natural Language Processing (NLP) using R. Participants will learn fundamental NLP concepts and techniques, including text cleaning, exploratory analysis, and feature engineering. The course covers practical applications such as sentiment analysis on real datasets. By the end, students will have the skills to preprocess text data, extract meaningful features, and perform basic NLP tasks in R.\nCourse Length: 2 days\nPrerequisites: Statistics in R\n\n\n2.3.2 Introduction to Machine Learning in R\nThis course introduces the fundamentals of machine learning using R’s state-of-the-art “mlr3” package. Participants will learn about classification, regression, and cluster analysis experiments. The course covers the entire machine learning workflow, from data preparation to model evaluation and interpretation. By the end, students will have hands-on experience implementing various machine learning algorithms and understand how to apply them to real-world problems.\nCourse Length: 3 days\nPrerequisites: Statistics in R\n\n\n2.3.3 Packaging and Documentation\nThis course guides participants through the process of building and sharing R packages. Students will learn how to create custom functions, organize them into a package structure, and document their code effectively. The course covers best practices in package development, including version control and collaboration. By the end, participants will be able to create their own R packages, enhancing code reusability and facilitating collaboration with other R users.\nCourse Length: 4 Hours\nPrerequisites: R Control Flow Loops and Functions, Best Practice in Programming - Clean Code, Introduction to Git\n\n\n2.3.4 Additional Analysis Modules\n\n2.3.4.1 Quality Assurance of Predictive Modelling\nThis course explores critical quality issues in statistical modeling and machine learning for prediction. Participants will learn best practices for model design, validation, and usage across various industries. The course covers topics such as model interpretability, bias detection, and robustness testing. By the end, students will be equipped with the knowledge to ensure the reliability and effectiveness of their predictive models.\nCourse Length: 6-8 hours\nPrerequisites: Statistics in R\n\n\n2.3.4.2 Introduction to Sparklyr\nThis course introduces Sparklyr, the R interface to Apache Spark for big data processing. Participants will learn how to handle and analyze massive datasets that exceed the capabilities of traditional R. The course covers data manipulation, querying, and processing techniques specific to Sparklyr. By the end, students will be able to leverage the power of distributed computing for their large-scale data analysis projects in R.\nCourse Length: 2 days\nPrerequisites: Introduction to Machine Learning in R\n\n\n2.3.4.3 Introduction to Bayesian Data Analysis\nThis course provides a foundational understanding of Bayesian Data Analysis and its implementation in R. Participants will explore the principles of Bayesian statistics, including prior and posterior distributions, and Markov Chain Monte Carlo (MCMC) methods. The course includes hands-on examples of Bayesian analysis in R, covering model fitting and interpretation. By the end, students will be able to apply Bayesian techniques to their own data analysis projects.\nCourse Length: 6 Hours\nPrerequisites: Statistics in R\n\n\n\n2.3.5 Additional Developer Modules\n\n2.3.5.1 Introduction to Unit Testing\nThis course focuses on the principles and implementation of unit testing in R for ensuring code quality. Participants will learn how to design, create, and execute tests for their R code. The course covers both the theoretical aspects of software development and practical implementation in R. By the end, students will be able to implement robust testing strategies in their R projects, improving code reliability and maintainability.\nCourse Length: 4 Hours\nPrerequisites: Packaging and Documentation\n\n\n2.3.5.2 Introduction to Continuous Integration\nThis course introduces modern software development approaches, focusing on Continuous Integration (CI) concepts. Participants will learn about DevOps and CI/CD philosophies and how they contribute to efficient software development. The course covers tools and practices that automate testing and deployment processes. While not hands-on, this course provides essential background knowledge for advanced technical courses on building modern development infrastructures. Course Length: 2 Hours\nPrerequisites: Packaging and Documentation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Curriculum</span>"
    ]
  },
  {
    "objectID": "importing_data.html#column-names",
    "href": "importing_data.html#column-names",
    "title": "3  Importing Data",
    "section": "3.5 Column Names",
    "text": "3.5 Column Names\nIn the previous section we looked at reading data into R and also inspecting it.\nIn this section we are going to look at how to manipulate it.\nIn the previous session we stated that every column in a data frame is a variable and it is good practice to not have spaces within variable names, as spaces makes it harder for us to call on the variables when we need to use them.\nWhen you enter data in Excel, you most often don’t think too much about what you call each column. After all, you just label them once and as long as they are meaningful to you, what does it matter if the column name is a long combination of CAPITALLETTERS, lowercaseletters, and numbers?\nWhen you are working with variables in R though, you need to type the name of each variable, every time you want to work with it. So, it makes sense to make your column names as simple, but meaningful as possible.\n\nIdeally, they should also be consistently formatted.\n\nFor example if we wanted to pick the name of Passenger column from the Titanic dataset.\n\nTitanic$name Of Passenger\n\nTo get around this we enclose name of passenger with back ticks like the code below - this is the key above the tab key on the left hand side of your keyboard.\n\n# Selecting data using the $ symbol\n# note this now works because of the back ticks\nTitanic$`name Of Passenger`\n\nIf your column names have spaces and you don’t get rid of them, you must use backticks.\nHowever its good practise to remove spaces and symbols.\nWe can see the column names by using the names() function to access the name attribute of the data.\n\n# Getting the column names using the names function\n\nnames(Titanic)\n\nNULL\n\n\nAs we can see our column names have spaces and some start with capital letters and some with small letters, we can clean the names using the either the janitor package or gsub.\n\n3.5.1 Cleaning Column Names\n\njanitor Package\n\nThe janitor package offers many functions used to manipulate data, for example removing empty rows and columns, finding duplicates within a data frame. In this session we will use the library to to clean our data set names.\nWe can clean the names of our dataset with the janitor::clean_names() function as shown below.\nWe are overwriting the original Titanic data frame with a version with the column names cleaned.\n\n# Cleaning the column names using the janitor\n# package and the clean_names() function.\n# This will put all names in lower case letters and \n# replace blank spaces with underscores.\n\nTitanic &lt;- janitor::clean_names(Titanic)\n\n# Getting the column names of the dataset\n\nnames(Titanic)\n\nNULL\n\n\nclean_names() removes spaces, symbols, changes characters to lower case and makes all columns start with letters.\nThis is the default setting, there are many other options such as snake, lower_camel and all_caps. These can be put inside the clean_names() function as shown below:\n\n# Specifying the case within the clean_names function\n\njanitor::clean_names(Titanic, case = \"snake\")\n\n, , Age = child, Survived = no\n\n      Sex\nClass  male female\n  x1st    0      0\n  x2nd    0      0\n  x3rd   35     17\n  crew    0      0\n\n, , Age = adult, Survived = no\n\n      Sex\nClass  male female\n  x1st  118      4\n  x2nd  154     13\n  x3rd  387     89\n  crew  670      3\n\n, , Age = child, Survived = yes\n\n      Sex\nClass  male female\n  x1st    5      1\n  x2nd   11     13\n  x3rd   13     14\n  crew    0      0\n\n, , Age = adult, Survived = yes\n\n      Sex\nClass  male female\n  x1st   57    140\n  x2nd   14     80\n  x3rd   75     76\n  crew  192     20",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "application_1.html#the-2019-census-data-you-need-is-found-at-this-link",
    "href": "application_1.html#the-2019-census-data-you-need-is-found-at-this-link",
    "title": "3  KNBS Application",
    "section": "4.1 The 2019 Census data you need is found at this link",
    "text": "4.1 The 2019 Census data you need is found at this link",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>KNBS Application</span>"
    ]
  },
  {
    "objectID": "application_1.html#task",
    "href": "application_1.html#task",
    "title": "5  Application 1",
    "section": "7.1 Task",
    "text": "7.1 Task\n\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(total = n()).",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "application_1.html#task-1",
    "href": "application_1.html#task-1",
    "title": "5  Application 1",
    "section": "7.2 Task",
    "text": "7.2 Task\n\nCreate a new binary column named migrant using mutate and case_when to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "application_1.html#task-2",
    "href": "application_1.html#task-2",
    "title": "5  Application 1",
    "section": "7.3 Task",
    "text": "7.3 Task\n\nFilter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and case_when\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).\n\n\n\nSolution\nlibrary(tidyverse)\n\n\n# start\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\ncensus_small &lt;- census |&gt;\n  select(county, subcounty_code, ea_type, p11, p12, p17, p19, p52)\n\n# religion\nreligion &lt;- census |&gt;\n  select(p17) |&gt;\n  rename(religion = p17)\n\nreligion_without_dk &lt;- religion |&gt;\n  filter(religion != 99)\n\nreligion_by_county &lt;- religion_without_dk |&gt;\n  group_by(county, religion) |&gt;\n  summarise(total = n())\n\n# migration\nmigration &lt;- census_small |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19)\n\nmigration_col &lt;- migration |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmigration_prop &lt;- migration_col |&gt;\n  summarise(migration_prop = mean(migrant))\n\n# working_hours\n\nworking_hours &lt;- census_small |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(age = p12,\n         sex = p11,\n         hours_worked_if_work = p52)\n\nworking_hours_18 &lt;- working_hours |&gt;\n  filter(age &gt;= 18)\n\nworking_hours_gender_rural &lt;- working_hours_18 |&gt;\n  mutate(rural = if_else(ea_type == 1, 1, 0),\n         female = if_else(sex == 2, 1, 0))\n\nworking_hours_no_nas &lt;-  working_hours_gender_rural |&gt;\n  mutate(hours_worked = if_else(is.na(hours_worked_if_work), 0, hours_worked_if_work))\n\n\nworking_hours_mean_adult &lt;- working_hours_no_nas |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Basics",
    "section": "",
    "text": "2.1 Basic Syntax\n−+\n03:00\nAs a quick exercise, try out some arithmetic in your console:\nNow open a new script (File -&gt; New File -&gt; R Script) and save it as Intro.R",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#basic-syntax",
    "href": "basics.html#basic-syntax",
    "title": "2  Basics",
    "section": "",
    "text": "25 \\* 15\n(45 + 3) \\^ 2\n78 / 4\n\n\n\nRepeat the above exercises. What happens when you hit enter? Try using Ctrl + Enter\n\n\n2.1.1 The assignment operator\nR uses the assignment operator &lt;- to assign values or data frames to objects. The object name goes on the left, with the object value on the right. For example, x &lt;- 5 assigns the value 5 to the object x. You can quickly type the assignment operator in RStudio by pressing Alt + - (Windows) or Option + - (Mac).\nOther programming languages tend to use =. The equals sign is used in R but for other purposes, as you’ll find out later. Note: = will actually work for assignment in R but it is not convention.\n\n\n\n−+\n05:00\n\n\n\n\nCreate an object x1 with a value of 14\nCreate an object x2 with a value of x1 + 7\nCheck the value of x2 by looking in the environment pane\nCreate an object x3 equal to x2 divided by 3.\n\n\n\n2.1.2 Combining using c()\nSo how do you assign more than one number to an object? Typing x &lt;- 1,2,3 will throw an error. The way to do it is to combine the values into a vector before assigning. For example, x &lt;- c(1, 2, 3).\nNote: all elements of a vector must be of the same type; either numeric, character, or logical. Vector types are important, but they aren’t interesting, which is why they aren’t covered on this course. We advise you to read about vectors in your own time.\n\n\n\n−+\n05:00\n\n\n\n\nUse the combine function to create a vector with values 1, 2 and 3.\nWhat happens when you write 1:10 inside c()?\nWhat happens if you try to create a vector containing a number such as 2019 and the word “year”?\n\n\n\nSolution\n#1. combine c() to create vector with values 1,2,3\nx &lt;- c(1, 2, 3)\nx\n\n\n[1] 1 2 3\n\n\nSolution\n#2. combine c() with 1:10\nx &lt;- c(1:10)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSolution\n#3. Incorrect code: will throw an error\nx &lt;- c(2019, year)\nx\n\n\n[[1]]\n[1] 2019\n\n[[2]]\nfunction (x) \n{\n    UseMethod(\"year\")\n}\n&lt;bytecode: 0x107b84370&gt;\n&lt;environment: namespace:lubridate&gt;\n\n\nSolution\n#3. Correct code\nx &lt;- c(2019, \"year\")\nx\n\n\n[1] \"2019\" \"year\"",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#functions",
    "href": "basics.html#functions",
    "title": "2  Basics",
    "section": "2.2 Functions",
    "text": "2.2 Functions\nFunctions are one of the most important aspects of any programming language. Functions are essentially just R scripts that other R users have created. You could write a whole project without using any functions, but why would we when others have done the hard work for us? To demonstrate how using functions can save us time let’s look at an example.\nImagine you had the following data for test scores of students and you wanted to find the mean score:\n\ntest_scores &lt;- c(70, 68, 56, 88, 42, 55)\n\nWe could extract each individual score from the data frame, add them together and then divide them by the number of elements:\n\n(test_scores[1] + test_scores[2] + test_scores[3] + test_scores[4] + test_scores[5] + test_scores[6]) / 6\n\n[1] 63.16667\n\n\nThis gives us the mean score of 63.2. But that’s pretty tedious, especially if our data set was of any significant size. To overcome this we can use a function called mean(). To read about a function in R type help(\"function_name\") or ?function_name in the console. By reading the help file we see that mean() requires an R object of numerical values. So we can pass our test_scores data as the argument:\n\nmean(test_scores)\n\n[1] 63.16667\n\n\nNot only does this save us time, it makes the code far more readable. While the two approaches above return the same answer, the use of the function makes our intention immediately clear. It’s important to remember it’s not just you that will be using and reading your code.\nThe values you passed to the mean function are known as arguments. Most functions require one or more arguments in order to work, and details of these can be seen by checking the help file.\nRunning ?mean shows us that the function mean has three arguments; x, trim and na.rm. You can pass these arguments to a function either by position or name. If you name the arguments in the function, R will use the values for the arguments they’ve been assigned to, e.g.:\n\nmean(x = c(1, 2, 3),\n     trim = 0,\n     na.rm = FALSE)\n\n[1] 2\n\n\nIf you don’t provide names for the arguments, R will just assign them in order, with the first value going to the first argument, etc:\n\nmean(c(1, 2, 3), #These are used for the first argument, x\n     0, #This is used for the second argument, trim\n     FALSE) #This is used for the third argument, na.rm\n\n[1] 2\n\n\nIt is good practice to use names to assign any arguments after the first one or two, to avoid confusion and mistakes!\nYou will notice that the first time we called the mean function, we didn’t have to specify values for either trim or na.rm. if you check the help file, you’ll notice that trim and na.rm have default values:\n\nmean(x, trim = 0, na.rm = FALSE)\n\nWhen arguments have default values like this, they will use these if you don’t provide an alternative. There is no default value for x, so if you don’t provide a value for x the function will return an error.\n\n\n\n−+\n05:00\n\n\n\n\nLook at the help for the sum() function. What does it do?\nHow many arguments does the sum() function have? How many of these have default values?\nTry summing up the values 1 to 8 using this function.\n\n\n\nSolution\n#1. using sum() function\n?sum()\n\n#2.sum() has two arguments: a numeric value or logical vector and 'na.rm'\n# whether missing values (NA) should be removed (TRUE or FALSE)\n# by default, NA values are ignored (i.e. na.rm = TRUE)\n\n#3. summing values 1 to 8 using sum()\nsum(1:8, na.rm = TRUE)\n\n\n[1] 36",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#packages",
    "href": "basics.html#packages",
    "title": "2  Basics",
    "section": "2.3 Packages",
    "text": "2.3 Packages\nBeing open-source means R has an extensive community of users that are building and improving packages for others. Base R covers a lot of useful functions but there’s lots it doesn’t, that’s when we want to install packages. Each package contains a number of functions, once we install a package we have access to every one of it’s functions.\nPackages need to be both installed and loaded before they can be used. You only need to install a package the first time you use it, but you will need to load it every time you want to use it.\nStart by opening RStudio, which is an integrated development environment (IDE) for R. You don’t have to use an IDE but it’s strongly advised as it provides a user-friendly interface to work with.\nTo install a package locally, run install.packages(\"package_name\"), making sure the package name is wrapped in quotation marks. The code below will install the tidyverse package, which is actually a collection of data manipulation and presentation packages.\n\ninstall.packages(\"tidyverse\")\n\nOnce installed, you can load the packages using the library() function. Unlike installing packages, you don’t need to wrap package names in quotation marks inside a library call.\n\nlibrary(tidyverse)\n\nTo know more about a package, it is always useful to read the associated documentation. You can do this by adding a ? in front of the name of any package or function, and running this in the console\n\n?tidyverse\n\n?select",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "basics.html#the-tidyverse",
    "href": "basics.html#the-tidyverse",
    "title": "2  Basics",
    "section": "2.4 The Tidyverse",
    "text": "2.4 The Tidyverse\nWhile base R has a wide range of functions for data manipulation and visualisation, most analytical code will make use of the tidyverse. This is a specific group of packages which are designed for use in the reading, processing and visualisation of data, and aim to be easy to use for beginner coders and clear to read and write. It is recommended to use the tidyverse packages wherever possible to make code consistent.\nThis training course will therefore make extensive use of tidyverse packages including dplyr, ggplot2 and tidyr.\nThe following exercise should be completed by those who are running through the course solo.\n\nInstall the tidyverse package in your Console (do you remember where this is?!)\nLoad the tidyverse library at the top of your Intro.R script.\n\n\n\nSolution\nlibrary(tidyverse)",
    "crumbs": [
      "Introduction to R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basics</span>"
    ]
  },
  {
    "objectID": "application_1.html#setup",
    "href": "application_1.html#setup",
    "title": "5  Application 1",
    "section": "5.2 Setup",
    "text": "5.2 Setup\n\nRead the dataset using read_csv and clean the column names using janitor::clean_names. The 2019 Census data you need is found at this link. The survey information is here\nSelect the columns required for the three pieces of analysis (e.g., p17 for religion) using select. Rename the selected columns to be more descriptive (e.g., religion) using rename.\nExamine your data. For each task, are there any NA’s or other strange values in your columns? Think about how to handle any NA’s and use mutate and case_when, or filter to address them.\n\n\n5.2.1 Breakdown of Kenya by religious belief\n\nAnalyze the religious breakdown: Group by the religion column using group_by and count the number of individuals in each category using summarise(total = n()).\n\n\n\n5.2.2 Share of people who are migrants in each county\n\nCreate a new binary column named migrant using mutate and case_when to indicate if an individual’s birthplace county is different from their current county.\nCalculate the share of migrants in each county: Group by county using group_by and calculate the mean of the migrant column using summarise(migration_prop = mean(migrant)).\n\n\n\n5.2.3 Average working hours for men and women in rural vs. urban areas.\n\nFilter the data to include only the working-age population or adults (e.g., age &gt;= 18) using filter.\nCreate binary indicator columns for rural (based on ea_type) and female (based on sex) using mutate and case_when\nCreate an hours_worked column where missing values (NA) in the original hours worked column (hours_worked_if_work) are replaced with 0, using mutate and replace_na.\nCalculate the average working hours: Group by the rural and female columns using group_by and calculate the mean of hours_worked and hours_worked_if_work using summarise(mean()). Remember to handle potential NAs in the mean calculation (e.g., using na.rm = TRUE).\n\n\n\nSolution\nlibrary(tidyverse)\n\n\n# start\ncensus &lt;- read_csv(\"../intro_R-main/data/census.csv\") |&gt;\n  janitor::clean_names()\n\ncensus_small &lt;- census |&gt;\n  select(county, subcounty_code, ea_type, p11, p12, p17, p19, p52)\n\n# religion\nreligion &lt;- census |&gt;\n  select(p17) |&gt;\n  rename(religion = p17)\n\nreligion_without_dk &lt;- religion |&gt;\n  filter(religion != 99)\n\nreligion_by_county &lt;- religion_without_dk |&gt;\n  group_by(county, religion) |&gt;\n  summarise(total = n())\n\n# migration\nmigration &lt;- census_small |&gt;\n  select(county, subcounty_code, p19) |&gt;\n  rename(birthplace = p19)\n\nmigration_col &lt;- migration |&gt;\n  mutate(migrant = if_else(birthplace == county, 0, 1))\n\nmigration_prop &lt;- migration_col |&gt;\n  summarise(migration_prop = mean(migrant))\n\n# working_hours\n\nworking_hours &lt;- census_small |&gt;\n  select(ea_type, p11, p52, p12) |&gt;\n  rename(age = p12,\n         sex = p11,\n         hours_worked_if_work = p52)\n\nworking_hours_18 &lt;- working_hours |&gt;\n  filter(age &gt;= 18)\n\nworking_hours_gender_rural &lt;- working_hours_18 |&gt;\n  mutate(rural = if_else(ea_type == 1, 1, 0),\n         female = if_else(sex == 2, 1, 0))\n\nworking_hours_no_nas &lt;-  working_hours_gender_rural |&gt;\n  mutate(hours_worked = if_else(is.na(hours_worked_if_work), 0, hours_worked_if_work))\n\n\nworking_hours_mean_adult &lt;- working_hours_no_nas |&gt;\n  group_by(rural, female) |&gt;\n  summarise(\n    hours_worked = mean(hours_worked, na.rm = TRUE),\n    hours_worked_if_work = mean(hours_worked_if_work, na.rm = TRUE)\n  )",
    "crumbs": [
      "KNBS Applications",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Application 1</span>"
    ]
  }
]